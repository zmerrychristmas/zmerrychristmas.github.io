[ { "title": "AWS Api gateway, Lambda, Serverless, and Services part I", "url": "/posts/aws-api-gateway-lambda-serverless-and-services/", "categories": "Fullstack, Architect, AWS", "tags": "aws, sqs, sns, serverless, service, sts", "date": "2022-12-05 00:00:00 +0700", "snippet": "AWS API Gateway AWS Lambda + API Gateway: No infrastructure to manage Support for the WebSocket Protocol Handle API versioning (v1, v2…) Handle different environments (dev, test, prod…) Handle...", "content": "AWS API Gateway AWS Lambda + API Gateway: No infrastructure to manage Support for the WebSocket Protocol Handle API versioning (v1, v2…) Handle different environments (dev, test, prod…) Handle security (Authentication and Authorization) Create API keys, handle request throttling Swagger / Open API import to quickly define APIs Transform and validate requests and responses Generate SDK and API specifications Cache API responsesAPI Gateway – Integrations High Level Lambda Function Invoke Lambda function Easy way to expose REST API backed by AWS Lambda HTTP Expose HTTP endpoint in the backend Example: internal HTTP API on premise, Application Load Balancer… Why? Add rate limiting, caching, user authentications, API keys, etc… AWS Service Expose any AWS API through the API Gateway? Example: start an AWS Step Function workflow, post a message to SQS Why? Add authentication, deploy publicly, rate control… API Gateway - Endpoint Types Edge-Optimized (default): For global clients Requests are routed through the CloudFront Edge locations (improves latency) The API Gateway still lives in nly one region Regional: For client within the same region Cloud manually combine with CloudFront (more control over the caching strategies and the distribution) Private: Can only be accessed from your VPC using an interface VPC endpint (ENI) Use a resource policy to define access API Gateway – Security: IAM Permissions Create an IAM policy authorization and attach to User / Role API Gateway verifies IAM permissions passed by the calling application Good to provide access within your own infrastructure Leverages “Sig v4” capability where IAM credential are in headersAPI Gateway – Security: Lambda Authorizer (formerly Custom Authorizers) Uses AWS Lambda to validate the token in header being passed Option to cache result of authentication Help to use Oauth / SAML / 3rd party type of authentication Lambda must return an IAM policy for the userAPI Gateway – Security: Cognito User Pools Cognito fully manages user lifecycle API gateway verifies identity automatically from AWS Cognito No custom implementation required Cognito only helps with authentication, not authorizationAPI Gateway – Security – Summary IAM Greate for users/ roles already within your AWS account Handle authentication + authorization Leverages Sig v4 Custom Authorizer: Greate for 3rd party tokens Very flexible in terms of what IAM policy is returned Handle Authentication + Authorization Pay per Lambda invocation Cognito User Pool you manage your user pool (can be backed by Facebookm Google login etc…) No need to write any custom code Must implement authorization in the backend AWS Integration &amp; MessaginngSQS, SNS &amp; Kinesis When we start deploying multiple applications, they will inevitably need to communicate with one another There are two patterns of application communication Synchromous communication (app to app) Asynchronous / Evenbased (app to queue to app) Introdution Synchronnous between applications can be problematic if there are sudden spikes of traffic What if you need to suddenly encode 1000 videos but usually it’s 10? in that case, it better to decouple your applications, SQS: queue model SNS: pub/sub model Kinesis: real-time streaming model These services can scale independently from our application!Amazon SQS, What’s a queue?SQS - Standard queue Oldest offering (over 10 years old) Fully managed service, used to decouple applications Attributes: unlimited throughput, unlimited number of messages in queue Default retention of message: 4 days, maxximum of 14 days Low latenccy (&lt; 10ms on publish and receive) Limitation of 256kb per message sent Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering)SQS - product messages Produced to SQS using the SDK (SendMessage API) The message is persited in SQS until consumer deletes it Message retention: default 4 days, upto 14days Example: order id customer id any attributes you want SQS standard: unlimited throughputSQS - Consuming messages Consumes (running on EC2 instances, Servers, or AWS lambda)… Poll SQS for messages (receive up to 10 messages at a time) Process the messages (example: insert the message inyo an RDS database) Delete the messages using thhe deleteMessage APISQS - Mutiple EC2 Innstances Consumers Consumers receive and process messages in parallel At least once delivery Best effort message ordering Consumers delete messages after processig them We can scale consumers horzontally to improve throughput of processingSQS with auto scaling group (ASG)SQS to decouple between appliccation tiersAmazon SQS - Security Encryption In-flight encryption using HTTPS API At rest encryption using KMS keys Client side encrytion if the client wants to perform encryptioon/decrytion itself Access Controls: IAM policies too regulate access to SQS API SQS Access Policies (similiar to S3 bucket policies) Useful for cross account access to SQS queues Useful for allowinng other services (SNS, S3…) to write to an SQS queue SQS Queue Access Policy Cross Acccount Access Publish S3 Event Notifications to SQS QueueSQS Message Visiblity Timeout After a message is polled by a consumer, it becomes invisible to other consumers By default, the “message visibility timeout” is 30 seconds That means the message has 30 seconnds to be processed After the message visibility timeout is over, the message is “visible” in SQS If a message is not processed within the visibility timeout, it will be processed twice A consumer could call the ChangelMessageVisibility API tto get more time If visibility timeout is high (hours), and consumer crashes, re-processing will take ttime If visiblity ttimeouut is too low (seconds), we may get duplicatesAmazon SQS - Dead Letter Queue If a consumer fails to process a message within the visibility timeout… the message goes back to the queue! We can set a threshold of how manny times a message can go back to the queue After tthe mmaximumreceivves tthreshold is exceeded, the message goes intto a dead lettter queue (DLQ) Userful for debugginng! Make sure to process the messages in the DLQ before they expire: Good to sett a rettenttion oof 14days in the DLQ SQS DLQ - Redrive to Source Feature to help consume messages in the DLQ to understand what is wrong with them. When our code is fixed, we can redrive the messages from the DLQ backk into the source queue (or any otther queue) inn batches without writing custtom codeAmazon SQS - Delay Queue Delay a message (consummers don’t see it immediately) up to 15 minuttes Defaultt is 0 secconnds (message is available rightt away) Can set a defaultt at queue level Can override tthe defaultt o sennd using the DelaySecccond parametersAmazon SQS - Long Polling When a consumer requests messages from the queue, it can optioally “wait” for messages to arrive if tthere are nonne in the queue This is called long pollinng LongPolling dereases the nuumber of API calls made tto SQS while increasign tthe efficiency and reduinng lattenccy of your applicationn The wait time can be between 1 se tto 20 sec (20 sec preferable) Lonng polling is preferable to short polling long polling level using waittimeseconndsAmazon SQS – Dead Letter Queue If a consumer fails to process a message within theVisibility Timeout… We can set a threshold of how many times a message can go back to the queue After the maximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ) Useful for debugging! Make sure to process the message in the DLQ before they expire: Good to set a retention of 14days in the DLQ SQS DLQ – Redrive to Source Redrive to Source Feature to help consume messages in the DLQ to understand what is wrong with them When our code is fixed, we can redrive the messages from the DLQ back into the source queue (or any other queue) in batches without writing custom codeAmazon SQS – Delay Queue Delay a message up to 15minutes Default is 0 seconds Can set a default at queue level Can override the default on send using the delayseconds parameterAmazon SQS - Long Polling When a consumer requests messages from the queue, it can optionally “wait” for messages to arrive if there are none in the queue This is called long polling Longpolling decrease the number of API calls mafe to SQS while increasing the The wait time can be between 1 sc to 20 sec Long polling is preferable to short polling Long polling can be enabled at the queue level or at the API level using WaitTimeSecondsSQS – Request-Response Systems To implement this pattern: use the SQS Temporary Queue Client It leverages virtual queues instead of creating / deleting SQS queues (cost-effective)Amazon SQS – FIFO Queue FIFO = First In First Out (ordering of messages in the queue) Limited throughput: 300 msg/s without batching, 3000 msg/s with Exactly-once send capability (by removing duplicates) Messages are processed in order by the consumerKinesis Overview Makes it easy to collect, process, and analyze streaming data in real-time Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data… Kinesis Data Streams: capture, process, and store data streams Kinesis Data Firehose: load data streams into AWS data stores Kinesis Data Analytics: analyze data streams with SQL or Apache Flink Kinesis Video Streams: capture, process, and store video streamsKinesis Data Streams Retention between 1 day to 365 days Ability to reprocess (replay) data Once data is inserted in Kinesis, it can’t be deleted (immutability) Data that shares the same partition goes to the same shard (ordering) Producers: AWS SDK, Kinesis Producer Library(KPL), Kinesis Agent Consumers: Write your own: Kinesis Client Library (KCL), AWS SDK Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics Kinesis Data Streams – Capacity Modes Provisioned mode: You choose the number of shards provisioned, scale manually or using API Each shard get 1MB/s in (or 1000 records per second) Each shard get 2MB/s out (classic or enhanced fan-out consumer) You pay per shard provisioned per hour On-demand mode: No need to provision or manage the capacity Default capacity provisioned (4 MB/s in or 4000 records per second) Scales automatically based on observed throughput peak during the last 30days Pay per stream per hour &amp; data in/out per GB Kinesis Data Firehose Fully managed service, no administration, automatic scaling, serverless AWS: Redshift / Amazon S3 / ElasticSearch 3rd party partner: Splunk / MongoDB / DataDog / NewRelic / … Custom: send to any HTTP endpoint Pay for data going through Firehose Near Real Time 60 seconds latency minimum for non full batches Or minimum 32 MB of data at a time Supports many data formats, conversions, transformations, compression Supports custom data transformations using AWS Lambda Can send failed or all data to a backup S3 bucketKinesis Data Streams vs FirehoseKinesis Data Streams Streaming service for ingest at scale Write custom code (producer / consumer) Realtime (~200ms) Manage scaling (shard splitting / merging) Data storage for 1 to 365 days Suports replay capabilityKinesis Data Firehose Load streaming data into S3 / Redshift /ES / 3 rd party / custom HTTP Fully managed Near real-time (buffer time min. 60 sec) Automatic scaling No data storage Doesn’t support replay capabilityKinesis Data Analytics (SQL application) Perform real-time analytics on Kinesis Streams using SQL Fully managed, no servers to provision Automatic scaling Real-time analytics Pay for actual consumption rate Can create streams out of the real-time queries Use cases: Time-series analytics Real-time dashboards Real-time metrics Ordering data into Kinesis Imagine you have 100 trucks (truck_1, truck_2,.., truck_100) on the road sending their GPS positions regularly into AWS You want to consume the data in order for each truck, so that you can track their movment accurately How should you send that data into Kinesis? Answer: send using a “Partition Key” value of the “truck_id” The same key will always go to the same shardOrdering data into SQS For SQS standard, there is no ordering For SQS FIFO, if you don’t use a Group ID, messages are consumed in the order they are sent, with only one consumer You want to scale the number of consumers, but you want messages to be “grouped” when they are related to each other Then you use a Group ID (similar to Partition Key in Kinesis)Kinesis vs SQS ordering Let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO Kinesis Data Streams: (500 consumer) On average you’ll have 20 trucks per shard Trucks will have their data ordered within each shard The maximum amount of consumers in parallel we can have is 5 Can receive up to 5 MB/s of data SQS FIFO You only have one SQS FIFO queue You will have 100 Group ID You can have up to 100 Consumers (due to the 100 Group ID) You have up to 300 messages per second (or 3000 if using batching) Amazon SNS What if you want to send one message to many receivers? The “event producer” only sends message to one SNS topic As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications Each subscriber to the topic will get all the messages (note: new feature to filter messages) Up to 12,500,000 subscriptions per topic 100,000 topics limitSNS integrates with a lot of AWS services Many AWS services can send data directly to SNS for notificationsAmazon SNS – How to publish Topic Publish (using the SDK) Create a topic Create a subscription (or many) Publish to the topic Direct Publish (for mobile apps SDK) Create a platform application Create a platform endpoint Publish to the platform endpoint Works with Google GCM, Apple APNS, Amazon ADM… Amazon SNS – Security Encryption: In-flight encryption using HTTPS API At-rest encryption using KMS keys Client-side encryption if the client wants to perform encryption/decryption itself Access Controls: IAM policies to regulate access to the SNS API SNS Access Policies (similar to S3 bucket policies) Useful for cross-account access to SNS topics Useful for allowing other services ( S3…) to write to an SNS topic SNS + SQS: Fan Out Push once in SNS, receive in all SQS queues that are subscribers Fully decoupled, no data loss SQS allows for: data persistence, delayed processing and retries of work Ability to add more SQS subscribers over time Make sure your SQS queue accesss policy allows for SNS to writeApplication: S3 Events to multiple queues For the same combination of: event type (e.g. object create) and prefix (e.g. images/) you can only have one S3 Event rule If you want to send the same S3 event to many SQS queues, use fan-outApplication: SNS to Amazon S3 through Kinesis Data Firehose SNS can send to Kinesis and therefore we can have the following solutions architecture:Amazon SNS – FIFO Topic FIFO = First In First Out (ordering of messages in the topic) Similar features as SQS FIFO: Ordering by message group ID (all messages in the same group are ordred) Deduplication using a Deduplication ID or Content Based Deduplication Can only have SQS FIFO queues as subscribers Limited throughput (same throughput as SQS FIFO)SNS FIFO + SQS FIFO: Fan Out In case you need fan out + ordering + deduplicationSNS – Message Filtering JSON policy used to filter messages sent to SNS topic’s subscriptions If a subscription doesn’t have a filter policy, it receives every messageSQS vs SNS vs KinesisSQS Consumer “pull data” Data is deleted after being consumed Can have as many workers (consumers) as we want No need to provision throughput Ordering guarantes only on FIFO queues Individual message delay capabilitySNS Push data to many subscribers Up to 12500000 subcribers Data is not persisted (lost if not delivered) Pub/sub Up to 100000 topics No need to provision throughput Integrates with SQS for fan-out architecture pattern FIFO capability for SQS FIFO Kinesis Standard: Pull data 2Mb per shard Enhanced-fan out: push data 2MB per shard per consumer Possibility to replay data Meant for real-time big data, analytics and ETL Ordering at the shard level Data expires aftr X days Provisioned mode or on-demand capacity modeAmazon MQ SQS, SNS are “cloud-native” service, and they’re using proprietary protocols from AWS Traditional applications running from on-premises may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ Amazon MQ = managed Apache ActiveMQ Amazon MQ doesn’t “scale” as much as SQS / SNS Amazon MQ runs on a dedicated machine, can run in HA with failover Amazon MQ has both queue feature (~SQS) and topic features (~SNS)Amazon MQ – High AvailabilitySERVERLESS AND APPLICATION SERVICES Tiered Architecture When we start deploying multiple applications, they will inevitably need to communicate with one another. There are two patterns of application communication. Evolving with queues Event-driven architecture Amazon SQS – Standard Queue Oldest offering (over 10 years old) Fully managed service, used to decouple applications Attributes: Unlimited throughput, unlimited number of messages in queue Default retention of messages: 4 days, maximum of 14 days Low latency (&lt;10 ms on publish and receive) Limitation of 256KB per message sent Can have duplicate messages (at least once delivery, occasionally) Can have out of order messages (best effort ordering)SQS – Producing Messages Produced to SQS using the SDK (SendMessage API) The message is persisted in SQS until a consumer deletes it Message retention: default 4 days, up to 14 days Example: send an order to be processed Order id Customer id Any attributes you want SQS standard: unlimited throughputSQS – Consuming Messages Consumers (running on EC2 instances, servers, or AWS Lambda)… Poll SQS for messages (receive up to 10 messages at a time) Process the messages (example: insert the message into an RDS database) Delete the messages using the DeleteMessage APILambda: function as a service- You are billed for the duration that a function runs- The environment has a direct memory (indirect CPU) allocation- Deployment a package with 50MB zipped and 250MB unzipped- 512 MB storage available as /tmp- Serverless application (S3, API Gateway, Lambda)- File Processing (S3, S3 event, lamba)- Database Triggers (DynamoDB, Streams, Lambda)- Serverless CROn (EventBrige/CWEvents + Lambda)- Realtime Stream Data Processing (kinesis + Lambda)Lambda in depth:- Public Lambda:![](https://data.terabox.com/thumbnail/9fb8e7f5d4deb514c210c436daa33b68?fid=4401547290288-250528-754626766169061&amp;rt=pr&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-AyXAee6ddgI%2bVvNASA%2fyuIDC100%3d&amp;expires=8h&amp;chkbd=0&amp;chkv=0&amp;dp-logid=125173180413113238&amp;dp-callid=0&amp;time=1667199600&amp;size=c1920_u1080&amp;quality=90&amp;vuk=4401547290288&amp;ft=image&amp;autopolicy=1)- Private Lambda:![](https://data.terabox.com/thumbnail/33d179bf393395941a01511d2a744ce3?fid=4401547290288-250528-1097958099823614&amp;rt=pr&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-45Ti2FPZBOQcIaJFI2fcCqPuJCU%3d&amp;expires=8h&amp;chkbd=0&amp;chkv=0&amp;dp-logid=125190523939933141&amp;dp-callid=0&amp;time=1667199600&amp;size=c1920_u1080&amp;quality=90&amp;vuk=4401547290288&amp;ft=image&amp;autopolicy=1) - Lambda run in VPC obey all VPC networking rules- Lambda security: - Lambda execution roles are IAM roles attached to lambda functions which control the permissions the lambda function receives ... - Lambda resource policy controls what services and accounts can invoke lambda functions- Lambda Logging: - Lambda uses Cloudwatch, cloudwatch logs &amp; x-ray - Logs from lambda executions - cloudwatchlogs - Metrics - invocation success/failure, retries, latency ... stored in cloudwatch - lambda can be integrated with x-ray for distributed tracing - Cloudwatch logs requires permissions via execution roleLambda in depth: Invocation Invocation: Synchronous cli / api, client communicates with APIGW , proxied to lambda function Asynchronous (typically used when AWS services invoke lambda functions) S3 isn’t waitting for any kind of response. The event is generated and S3 stops tracking If processing of the event fails, lambda will retry between 0 and 2 time (configurable). Lambda handles the retry logic The lambda function needs to be idempotent reprocessing a result should have the same end state Kinesis data stream: producers(telemetry) An execution context is the environment a lambda function run in. A cold start is a full creation and configuration including function code dowload.Cloudwatch events and eventbridge If X happen, or at Y times .. do Z Eventbrigde is cloudwatch api v2 a default event bus for account cloudwatch event has only one bus eventbrigde have additional bus rules match event route the events to +1 targets as LambdaCloudWatch Events and EventBridge have visibility over events generated by supported AWS services within an account.They can monitor the default account event bus - and pattern match events flowing through and deliver these events to multiple targets.They are also the source of scheduled events which can perform certain actions at certain times of day, days of the week, or multiple combinations of both - using the Unix CRON time expression format.Both services are one way how event driven architectures can be implemented within AWS.SQS – Multiple EC2 Instances Consumers Consumers receive and process messages in parallel At least once delivery Best-effort message ordering Consumers delete messages after processing them We can scale consumers horizontally to improve throughput of processing" }, { "title": "AWS concept and classic Solutions Architecture", "url": "/posts/aws-concept-and-classic-solutions-architecture/", "categories": "", "tags": "", "date": "2022-12-04 00:00:00 +0700", "snippet": "AWS Concept for solution architectAWS History 2002 Internally launched 2003 Amazon infrastructure is one of their core strength. Idea to market 2004 Launched publicly with SQS 2006 Re-launched ...", "content": "AWS Concept for solution architectAWS History 2002 Internally launched 2003 Amazon infrastructure is one of their core strength. Idea to market 2004 Launched publicly with SQS 2006 Re-launched publicly with SQS, S3 &amp; EC2 2007 Launched in EuropeAWS Cloud Number Facts In 2019, AWS had $35.02 billion in annual revenueAWS Cloud Use Cases AWS enables you to build sophisticated, scalable applications Applicable to a diverse set of industries Use cases include - Enterprise IT, Backup &amp; Storage, Big Data analytics - Website hosting, Mobile &amp; Social Apps - GamingAWS Global Infrastructure AWS Region AWS Availability Zones AWS Data centers AWS Edge Locations / Points of presenceAWS Regions AWS has regions all around the world Names can be us-east-1, eu-west-3… A region is a cluster of data centers Most AWS seervices are region-scopedHow to choose an AWS Region Compliance with data goverance and legal requirements data never leaves a region without your explicit permission Proximity to customers: reduced latency Avaiable services with in a Region new services and new features aren’t avaiable in every Region Pricing pricing varies region to region and is transparent in the sevice pricing pageAWS Availability Zones Each region has many availability zones (usually 3, min is 2, max is 6). ap-southeast-2a ap-southeast-2b ap-southeast-2c Each availability zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity They’re separate from each other, so that they’re isolated from disasters. They’re connected with high bandwidth ultra-low latency networkingAWS Points of Presence Amazon has 216 points of presence (205 edge locations &amp;   Regional caches) in 84 cities across 42 countries Content is delivered to end users with low latencyTour of the AWS console AWS hash Global Services Identty and Access Management(IAM) Route 53(DNS service) CloudFront (Content Delivery Network) WAF (Web application firewall) Most AWS services are region-scoped Amazone EC2 (Infrastructure as a Service) Elastic Beanstalk Lambda (Function as a Service) Rekognition (Software as a Service) Region TableDNSSEC Domain Name System Security ExtensionsDNSSEC strengthens authentication in DNS using digital signatures based on public key cryptography. With DNSSEC , it’s not DNS queries and responses themselves that are cryptographically signed, but rather DNS data itself is signed by the owner of the data.AWS OrganizationsIt’s architecture and some of the benefits for businesses managing larger numbers of AWS Accounts. The GENERAL account will become the MASTER account for the organisation We will invite the PRODUCTION account as a MEMBER account and create the DEVELOPMENT account as a MEMBER account. Finally - we will create an OrganizationAccountAccessRole in the production account, and use this role to switch between accounts. Create AWS Role Create Aws Account name Email inviteService Control Policies (SCP)I introduce service control policies - a feature of AWS Organizations which allow restrictions to be placed on MEMBER accounts in the form of boundaries.SCPs can be applied to the organization, to OU’s or to individual accounts.Member accounts can be affected, the MANAGEMENT account cannot.SCPs DON’T GIVE permission - they just control what an account CAN and CANNOT grant via identity policies.ex: apply an SCP to the PRODUCTION account to test their capabilities.Security token service(STS)AWS provides AWS Security Token Service (AWS STS) as a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users). This guide describes the AWS STS API.By default, AWS Security Token Service (AWS STS) is available as a global service, and all AWS STS requests go to a single endpoint at https://sts.amazonaws.com. sts:AssumeRoleIAM Role Temporary Security credentialsYou can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permissions to do so. security-credentials example: Revoking Temporary Credentials by curl permission revoke Policies and permissions in IAM Most policies are stored in AWS as JSON documents. AWS supports six types of policies: identity-based policies, resource-based policies, permissions boundaries, Organizations SCPs, ACLs, and session policies. Version – Specify the version of the policy language that you want to use. As a best practice, use the latest 2012-10-17 version. Statement – Use this main policy element as a container for the following elements. You can include more than one statement in a policy. Sid (Optional) – Include an optional statement ID to differentiate between your statements. Effect – Use Allow or Deny to indicate whether the policy allows or denies access. Principal (Required in only some circumstances) – If you create a resource-based policy, you must indicate the account, user, role, or federated user to which you would like to allow or deny access. If you are creating an IAM permissions policy to attach to a user or role, you cannot include this element. The principal is implied as that user or role. Action – Include a list of actions that the policy allows or denies. Resource (Required in only some circumstances) – If you create an IAM permissions policy, you must specify a list of resources to which the actions apply. If you create a resource-based policy, this element is optional. If you do not include this element, then the resource to which the action applies is the resource to which the policy is attached. Condition (Optional) – Specify the circumstances under which the policy grants permission. example: experience a few ways to access S3 using cross-account access example: access S3 using bucket policyService linked roleA service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role. A service might automatically create or delete the role. It might allow you to create, modify, or delete the role as part of a wizard or process in the service. Or it might require that you use IAM to create or delete the role. Resource Access ManagerResource Access Manager (RAM) allows AWS resources to be shared between AWS Accounts.It can be used to support certain common architectures such as a Shared Services VPC.Service endpoints and quotas To connect programmatically to an AWS service, you use an endpoint. Service quotas, also referred to as limits, are the maximum number of service resources or operations for your AWS account. Example: Cloudwatch A quota request template aws service-quotas list-service-quotas \\ --service-code cloudformationlist-aws-default-service-quotas --service-code &lt;value&gt; [--cli-input-json | --cli-input-yaml] [--starting-token &lt;value&gt;] [--page-size &lt;value&gt;] [--max-items &lt;value&gt;] [--generate-cli-skeleton &lt;value&gt;] [--debug] [--endpoint-url &lt;value&gt;] [--no-verify-ssl] [--no-paginate] [--output &lt;value&gt;] [--query &lt;value&gt;] [--profile &lt;value&gt;] [--region &lt;value&gt;] [--version &lt;value&gt;] [--color &lt;value&gt;] [--no-sign-request] [--ca-bundle &lt;value&gt;] [--cli-read-timeout &lt;value&gt;] [--cli-connect-timeout &lt;value&gt;] [--cli-binary-format &lt;value&gt;] [--no-cli-pager] [--cli-auto-prompt] [--no-cli-auto-prompt]request-service-quota-increase --service-code &lt;value&gt; --quota-code &lt;value&gt; --desired-value &lt;value&gt; [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton &lt;value&gt;] [--debug] [--endpoint-url &lt;value&gt;] [--no-verify-ssl] [--no-paginate] [--output &lt;value&gt;] [--query &lt;value&gt;] [--profile &lt;value&gt;] [--region &lt;value&gt;] [--version &lt;value&gt;] [--color &lt;value&gt;] [--no-sign-request] [--ca-bundle &lt;value&gt;] [--cli-read-timeout &lt;value&gt;] [--cli-connect-timeout &lt;value&gt;] [--cli-binary-format &lt;value&gt;] [--no-cli-pager] [--cli-auto-prompt] [--no-cli-auto-prompt]{ \"Quotas\": [ { \"ServiceCode\": \"xray\", \"ServiceName\": \"AWS X-Ray\", \"QuotaArn\": \"arn:aws:servicequotas:us-west-2::xray/L-C6B6F05D\", \"QuotaCode\": \"L-C6B6F05D\", \"QuotaName\": \"Indexed annotations per trace\", \"Value\": 50.0, \"Unit\": \"None\", \"Adjustable\": false, \"GlobalQuota\": false }, { \"ServiceCode\": \"xray\", \"ServiceName\": \"AWS X-Ray\", \"QuotaArn\": \"arn:aws:servicequotas:us-west-2::xray/L-D781C0FD\", \"QuotaCode\": \"L-D781C0FD\", \"QuotaName\": \"Segment document size\", \"Value\": 64.0, \"Unit\": \"Kilobytes\", \"Adjustable\": false, \"GlobalQuota\": false }, { \"ServiceCode\": \"xray\", \"ServiceName\": \"AWS X-Ray\", \"QuotaArn\": \"arn:aws:servicequotas:us-west-2::xray/L-998BFF16\", \"QuotaCode\": \"L-998BFF16\", \"QuotaName\": \"Trace and service graph retention in days\", \"Value\": 30.0, \"Unit\": \"None\", \"Adjustable\": false, \"GlobalQuota\": false } ]}Amazon Resource Names (ARNs)Questions1. What functionality does STS provide?it generates short term credentials which can be used to interact with AWS resources.2. Which of the following is NOT an example of a ‘real’ identity which can be referenced by ARNs in resource policies?IAM Groups3. How are role sessions revoked (Choose one)A inline policy is added to the role with an explicit deny for role assumptions before .. NOW4. If an SCP on the AWS account allows S3, a managed policy attached to your identity allows S3 and an inline policy denys S3.. what is your effective access (Choose one)Denied5.An SCP on account B denies S3. A resource policy on the bucket in account B allows account A. An identity policy on Bob in Account A allows access to S3. What is the effective access when Bob accesses the bucket in account B (choose one)HashSSL/TLSDigital SignalAWS Public vs Private ServicesAWS Default Virtual Private Cloud (VPC)A default VPC is created once per region when an AWS account is first created.There can only be one default VPC per region, and they can be deleted and recreated from the console UI .They always have the same IP range and same ‘1 subnet per AZ’ architecture.This lesson details and demos the functionality of a default VPC.Elastic Compute Cloud (EC2) Basics IAAS: virtual services Private service by default - uses VPC networkingIAM SectionIAM: Users &amp; Groups IAM = Identity and Access Management, Global service Root account created by default, shouldn’t be used or shared Users are people within your organization, and can be grouped Groups only contain users, not other groups Users don’t have to belong to a group, and user can belong to multiple groupsIAM: Permissions Users or Groups can be assigned JSON documents called policies These policies define the permissions of the users In AWS you apply the least privilege principle don’t give more permission than a user needs{\"Version\": \"2012-10-17\",\"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"elasticloadbalancing:Describe*\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:Describe*\" ], \"Resource\": \"*\" } ]}IAM Policies inheritanceIAM Policies Structure Json structure version: policy language version, alway include “2012-10-17” ID: an identifier for the policy Statement: one or more individual statements Sid Effect Principal Action Resource Condition IAM – Password Policy Strong passwords: higher security for your account In AWS, you can setup a password policy Set a minimum password length Require specific character types includng uppercase letter lowercase letters numbers non-alphanumeric characters Allow all IAM users to change their own passwords Require users to change their password after some time Prevent password re-use Multi Factor Authentication - MFA Users have access to your account and can possibly change configurations or delete resources in your AWS account You want to protect your Root Accounts and IAM users MFA = password you know + security device you own Main benefit of MFA: If a password is stolen or hacked the password is not compromised MFA virtual app or USB hardware Factor Hardware Key Fob MFA Device, Hardware Key Fob MFA DeviceHow can users access AWS To access AWS, you have three options AWS Management Console AWS Command line interface AWS software developer kit Access keys are generated through the AWS Console, password don’t share Users manage their own access keys Access Key ID Secret Access Key Example Access key ID: AKIASK4E37PV4983d6C Secret Access Key: AZPN3zojWozWCndIjhB0Unh8239a1bzbzO5fqqkZq What’s the AWS CLI? A tool that enables you to interact with AWS services using commands inyour command-line shell Direct access to the public APIs of AWS services You can develop scripts to manage your resources It’s open-source https://github.com/aws/aws-cli Alternative to using AWS Management ConsoleWhat’s the AWS SDK? AWS Software Development Kit (AWS SDK) Language-specific APIs (set of libraries) Enables you to access and manage AWS servicesprogrammatically Embedded within your application Supports SDKs (JavaScript, Python, PHP, .NET, Ruby, Java, Go, Node.js,C++) Mobile SDKs (Android, iOS, …) IoT Device SDKs (Embedded C, Arduino, …) Example: AWS CLI is built on AWS SDK for PythonIAM Roles for Services Some AWS service will need to perform actions on your behalf To do so, we will assign permissions to AWS services with IAM Roles Common roles: EC2 Instance Roles Lamba function roles Roles for CloudFormation IAM Security Tools IAM Credentials Report: A report that list all your account’s users and the status of their various credentials IAM Access Advisor (user-level): Access advisor shows the service permissions granted to a user and when those. You can use this information to revise your policies.IAM Guidelines &amp; Best Practices Don’t use the root account except for AWS account setup One physical user = One AWS user Assign user to groups and assign permissions to groups Create a strong password policy User and enforce the use of Multi Factor Authentication(MFA) Create and use Roles for giving permissions to AWS services User Access keys for programmactic access Audit permissions of your account with the IAM credentials report Never share IAM users &amp; Access KeysIAM Section – Summary Users: mapped to a physical user, has a password for AWS console Groups: contains users only Policies: JSON document that outlines permissions for users or groups Roles: for EC2 instances or AWS services Security: MFA + Password policy Acess keys: access AWS using the CLI or SDK Audit: IAM credential reports &amp; IAM Access AdvisorEC2 BasicsAmazon EC2 EC2 is one of the most popular of AWS offering EC2 = Elastic Compute Cloud = IAAS Renting virtual machines EC2 Storing data on virtual drives EBS Distributing load acroos machines ELB Scaling the services using an auto-scaling group ASG Knowing EC2 is fundamental to understand how the Cloud worksEC2 sizing &amp; configuration options Operating System (OS): Linux, Windows, MAC OS How much compute power and core: CPU How much random-access memory: RAM How much storage space: Network-attached: EBS &amp; EFS Harkware: EC2 Instance Store Network card speed of the card, public ip address Firewall rules: security group Boostrap script: EC2 User DataEC2 User Data It is possible to bootstrap our instances using an EC2 User data script bootstrapping means launching commands when a machine starts That script is only run once at the instace first start EC2 user data is used to automate boot tasks such as Install updates Install software Download common files from the internet Anything you can think of The EC2 User data script runs with the root user Example: Web server is launched using EC2 user dataEC2 Instance Types - Overview References to EC2 instances example: m5.2xlarge m: instance class 5: generate 2xlarge: size within the instance class EC2 Instance Types – General Purpose Great for a diversity of workloads such as web servers or code repositories Balance between: Compute Memory Networking Ex: Mac, t4g, t3, t3a, t2, M6g, M6I, M6a, M5, M5a, M5n, M5zn, M4, A1EC2 Instance Types – Compute Optimized Great for compute intensive tasks that require high performance processors: Batch processing workload Media transcoding High performance web servers High performance computing HPC Scientific modeling &amp; machine learning Dedicated gaming servers Ex: C7g, C6g, C6gn, C6I, C6a, Hpc6a, C5, C5a, C4EC2 Instance Types - Memory Optimized Fast performance for workloads that process large data set in memory Use cases: High performance, relational/non-relational databases Distributed web scale cache store In-memory database optimized for BI Applications performing real-time processing of big unstructured data Ex: R6a, R6g, R6I. R5, R5a, R5b, R5n, R4, X2gd, X2ldn, X2ledn, X2lezn, X1, High Memory, z1dEC2 Instance Types – Storage Optimized Great for storage-intensive tasks that require high, sequential read and writeaccess to large data sets on local storage Use cases: High frequency online transaction processing (OLTP) systems Relational &amp; NoSQL databases Cache for in-memory databases (for example, Redis) Data warehousing applications Distributed file systems Ex: Im4gn, Is4gen, I4i, I3, I3en, D2, D3, D3en, H1EC2 Instance Types – Accelerated Computing Hardware accelerators, co-processors, perform functions, floating point number calculations, graphics processing, data pattern matching Use case: Machine learning, high performace computing, computanional fluid dynamics Ex: P4, P3, P2, DL1, Trn1, Irn1, Inf1, G5, G5g, G4gn, G4ad, G3, F1, VT1Introduction to Security Groups Security groups are the fundamental of network security in AWS They control how traffic is allowed into or out of our EC2 instances Security groups only contain allow rules Security groups rules can reference by IP or by security groupSecurity Groups Deeper Dive Security groups are acting as a “firewall” on EC2 instances They regulate: Access to Ports Authorised IP ranges: IPv4 and IPv6 Control of inbound network (from other to the instance) Control of outbound network (from the instance to other)Security Groups Good to know Can be attaced to multiple instances Locked down to a region / VPC combination Does live “outside” the EC2 - if traffic is blocked the EC2 instance won’t see it It’s good to maintain one separate security group for SSH access If your application is not accessible (time out), then it’s a security group issue If your application gives a “connection refused” error, then it’s an application error or it’s not lauched All inbound traffic is blocked by default All outbout traffic is authorised by defaultClassic Ports to know 22 = SSH 21 = FTP 22 = SFTP 80 = HTTP 443 = HTTPS 3389 = RDP Ec2 instance connect is popular tool for ssh, ssh commancd is avaiable for all opeartion but withou windows version less than 10, so on you need PuttyEC2 Instance Connect Connect to your EC2 instance within your browser No need to use your key file that was downloaded The “magic” is that a temporary key is uploaded onto EC2 by AWS Work only out of the box Amazon Linux 2 Need to make sure the port 22 is still opened!EC2 Instances Purchasing Options On-demand Instances: short workload, predictable pricing, pay by second Reserved (1 &amp; 3 years) Reserved instances: long workloads Convertible Reserved Instances: long workloads with flexible instances Savings plans (1 &amp; 3 years): Commitment to an amount of usage, long workload Spot Instances: short workloads, cheap, can lose instances (less reliable) Dedicated Hosts: Book an entire physical server, control instance placement Dedicated Instances: No other customers will share your hardware Capacity Reservations: Reserve capacity in a specific AZ for any durationEC2 On Demand Pay for what you use Linux or WIndows: Bulling per second, after the first minute All other operating systems: billing per hour Has the highest cost but no upfront payment No long-term commitment Recommended for short-term and un-interupted workloads where you can’t predict how the application will behaveEC2 Reserved Instances Up to 72% discounr compared to On-demand You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS) Reservation Period - 1 year(+discount) or 3 years (+++discount) Payment Options - No upfront(+), Partial upfront(++), All upfront(+++) Reserved Instance’s Scope - Regional or Zonal (reserve capacity in an AZ) Recommended for steady-state usage applications (think database) You can buy and sell in the Reserved Instance marketplace Convertible Reserved Instance Can change the EC2 instance type, instance family, OS, scope and tenacy Up to 66% discountEC2 Savings Plans Get a discount based on long-term usage (up to 72% - same as RIs) Commit to certain type of usage ($10/hour for 1 or 3 years) Usage beyond EC2 savings plans is billed at the On-Demand price Locked to a specific instance family &amp; AWS region (e.g, M5 in us-east-1) Flexible across: Instance Size OS Tenancy (Host, Dedicated, Default)EC2 Spot Instances Can get a discount of up to 90% compared to On-demand Instances that you can “lose” at any point of time if your max price is less than the current spot price The Most cost-efficient instances in AWS Useful for workloads that are resilient to failure Batch jobs Data analysis Image processing Any distributed workloads Workloads with a flexible start and end time Not suitable for critical jobs or databasesEC2 dedicated hosts A physical server with EC2 instance capacity fully dedicated to your use Allows you address compliance requirements and use your existing server bound software licenses (per-socket, per-core, pe—VM software licenses) Purchasing Options: On-demand: pay per second for active Dedicated Host. Reserved: 1 or 3 years (No Upfront, Partial Upfront, All Upfront) The most expensive option Useful for software that have complicated licensing model: BYOL Bring your own license Or for companies that have strong regulatory or compliance needs.EC2 Dedicated Instances Instances run on hardware that’s dedicated to you May share hardware with other instances in same account No control over instance placement (Can move hardware after Stop/Start)EC2 Capacity Reservations Reserve On-demand instance capacity in a specific AZ for any duration You always have access to EC2 capacity when you need it No time commitment, no billing discount Combine with Regional reserved instances and savings plans to benefit from billing discounts You’re charged at On-Demand rate whether you run instances or not Suitable for short-term, uninterrupted workloads that needs to be in a specific AZWhich purchasing option is right for me? On demand: coming and staying in resort whenever we like, we pay the full price Reserved: like planing ahead and if we plan to stay for a long time, we may get a good discount. Saving Plans: pay a cerain amount per hour for certain period and stay in any room type Spot Instance: The hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms. You can get kicked out at any time Dedicated Hosts: We book an entire building of the resort Capacity Reservations: you book a room for a period with full price even you don’t stay in itPrice Comparison Example – m4.large – us-east-1EC2 Spot Instance Requests Can get a discount of up to 90% compared to On-demand Define max spot price and get the instance while current spot price &lt; max The hourly spot price varies based on offer and capacity If the current spot price &gt; your max price you can choose to stop or terminate your instance with a 2 minutes grace period Other strategy: Spot Block “block” spot instance during a specified time frame (1 to 6 hours) without interuptions In rare situations, the instance may be reclaimed Used for batch jobs, data analysis, or workloads that are resilient to failures. Not greate for critical jobs or databasesHow to terminate Spot Instances?Spot Fleets Spot Fleets = set of Spot instances + (optional) On-Demand instances The spot fleet will try to meet the target capacity with price constraints Define possible launch pools: instance type (m5.large), OS, Avaiability Zone Can ave multiple launch pools, so that the fleet can choose Spot Fleet stops launching instances when reaching capacity or max cost Strategies to allocate Spot Instances lowestPrice: from the pool with the lowest price (cost optimizations, short workload) diversified: distributed across all pool (great for availability, long workloads) capacityOptimized: pool with the optimal capacity for the number of instances. EC2 Section – Summary EC2 Instance: AMI (OS) + Instance Size (CPU + RAM) + Storage +security groups + EC2 User Data Security Groups: Firewall attached to the EC2 instance EC2 User Data: Script launched at the first start of an instance SSH: start a terminal into our EC2 instances EC2 Instance Role: link to IAM roles Purchasing Options: On-Demand, Spot, Reversed (Standard + Convertible + Scheduled), Dedicated Host, Dedicated InstanceEC2 – Associate Private vs Public IP (IPv4) Networking has two sorts of IPs. IPv4 and IPv6: Ipv4: 1.160.10.240 IPv6: 3ffe:1900:4545:3:200:f8ff:fe21:67cf In this course, we will only be using IPv4. Ipv4 is still the most common IPv6 is newer and solves problems for the internet of Thing (IoT) IPv4 allow for 3.7 bilion different address in the public space IPv4: [0-255].[0-255].[0-255].[0-255].Private vs Public IP (IPv4) Public IP public IP means the machine can be identified on the internet Must be unique across the whole web Can be geo-located easily Private IP: Private Ip means the machine can only be identified on a private network only The IP must be unique across the private network But two different private networks (two companies) can have the same IPs Machine connect to WWW using a NAT + internet gateway Only a specified range of IPs can be used as private IP Elastic IPs When you stop and then start an EC2 instance, it can change its public IP. If you need to have a fixed public IP for your instance, you need an Elastic IP An Elastic IP is a public IPv4 IP you own as long as you dont delete it You can attach it to one instance at a timeElastic IP With an Elastic IP address, you can mask the failure of an instance or software by rapidy remapping the address to another instace in your account. You can only have 5 Elastic IP in your account Overall, try to avoid using Elastic IP: They often reflect poor architectural decisions Instead, use a random public IP and register a DNS name to it Or as we’ll see later, use a load balancer and dont use a public IP In AWS EC2 – Hands On By default, your EC2 machine comes with: A private IP for the internal AWS Network A public IP, for the WWW When we are doing SSH into our EC2 machines: We can’t use a private IP, because we are not in the same network We can only use the public IP If your machine is stopped and then started the public IP can changePlacement Groups Cluster Pros: Greate network (10 Gbs bandwith between instances with Enhanced Networking enabled) Cons: If the rack fails, all instances fail at the same time Use case: Big Data job that needs to complete fast Application that needs extremely low latency and high network throughput Placement Groups Spread Pros: Can span across avaiability zones (AZ) Reduced risk is simultaneous failure EC2 Instances are on different phusical hardware Cons: Limited to 7 instances per AZ per placement group Use case: Application that needs to maximize high availability Critical Applications where each instance must be isolated from failure from each other Placements Groups Partition Up to 7 partions per AZ Can span across multiple AZs in the same region Up to 100s of EC2 instances The instances in a partion do not share racks with the instances in the other partitions A partition failure can affect many EC2 but won’t affect other partitions EC2 instances get access to the partition information as metadata Use cases: HDFS, HBase, Cassandra, KafkaElastic Network Interfaces (ENI) Logical component in a VPC that represents a virtual network card The ENI can have the following attributes: primary private IPv4, one or more secondary IPv4 One Elastic IP (IPv4) per private IPv4 One public IPv4 One or more security groups A MAC address You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover Bound to a specific availability zone (AZ)Ec2 Hibernate We know we can stop, termiate instances stop - the data on disk (EBS) is kept intact in the next start terminate - any EBS volumes (root) also set-up to be destroyed is lost On start, the following happens: First start: the OS boots &amp; the EC2 User Data script is run Following starts the OS boots up Then your application starts, caches get warmed up, and that can take time! EC2 Hibernate Introducing Ec2 Hibernate The in-memory (RAM) state is preserved The instance boot is much faster! Under the hood: the RAM state is written to a file in the root EBS volume The root EBS volume must be encrypted Long-running processing Saving the RAM state Services that take time to initializeEC2 Hibernate – Good to know - Accelerate EC2 Supported Instance Families: C3, C4, C5, I3, M3, M4, R3, R4, T2, T3, … Instance RAM size: must be less than 150 GB Instance Size: not supported for bare metal instances AMI: Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS &amp; Windows… Root volume: must be EBS, encrypted, not instance store and large Available for On-Demand, Reserved and Spot instances An instance can NOT be hibernated more than 60 daysEC2 Nitro Underlying platform for the next generation of EC2 instances New virualization technology Allows for better performance Better networking options (enhanced networking, HPC, IPv6) Higher speed EBS (Nitro is necessary for 64.000 EBS IOPS - max 32000 on non-Nitro) Better underlying security Instance types example: Virtualized: AI, C5, C5a, C5ad, C5d, C5n, C6g, C6gn, D3, D3en, G4, I3en, Infi, M5a, M5ad, M5d, M5dn, M5n, …. Bare metal: a l.metal, c5.metal, c5n.metal, c6g.metal. c6gd.metal… EC2 – Understanding vCPU Multiple threads can run on one CPU (multithreading) Each thread is represented as a vitual CPU (vCPU) Example: m5.2xlarge 4 CPU 2 threads per CPU =&gt; 8 vCPU in total EC2 – Optimizing CPU options: Reservations EC2 instances come with a combination of RAM and vCPU But in some cases, you may want to chage the vCPU options: of CPU cores: you can decrease it (helpful if you need high RAM and low number of CPU) - to decrease licensing costs of threads per core: disable multithreading to have I thread per CPU - helpful for high performance computing (HPC) workloads Only specified during instance launchEC2 – Capacity Reservations: Reservations Capacity reservations ensure you have EC2 capacity when needed Manual or planned end date for the reservation No need for 1 or 3 year commitment Capacity access is immediate, you get billed as soon as it starts Specify: The Availability Zone in which to reserve the capacity The number of instances for which to reserve capacity The instance attributes, including the instance type tenancy, and platform/OS Combine with REserved Instances and Savings Plans to do cost savingEC2 Instance Storage SectionWhat’s an EBS Volume? An EBS (Elastic Block Store) Volume is a network drive you can attack to your instances while they run It allows your instances to persist data, even after their termination They can only be mounted to one instance at a time (at the CCP level) They are bound to a specific availability zone Analogy Think of them as a “network USB stick” Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per monthEBS Volume It’s a network drive It uses the network to communicate the instance, which means there might be a bit of latency It can be detached from an EC2 instance and attached to another one quickly It’s locked to an Availability Zone An EBS volume in us-east-1 a cannot be attached to us-east-1 b To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GBs, and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time EBS – Delete on Termination attribute Controls the EBS behaviour when an EC2 instance terminates By default, the root EBS volume is deleted (attribute enabled) By default, any other attached EBS volume is not deleted (attribute disabled) This can be controlled by thw AWS console / AWS CLI Use case: preserve root volume when instance is terminatedEBS Snapshots Make a backup (snapshot) of your EBS volume at a point in time Not necessary to detach volume to do snapshot, but recommended Can copy snapshots across AZ or RegionEBS Snapshots Features EBS Snapshot Archive Move a snapshot to an “archive tier” that is 75% cheaper Takes within 24 to 72 hours for restoring the archive Recycle Bin for EBS Snapshots Setup rules to retain deleted snapshots so you can recover them after an accidental deletion Specify retention (from 1 day to 1 year) AMI Overview AMI = Amazone Machine Image AMI are a customization of an EC2 instance You add your own software, configuration, operating system, monitoring… Faster boot / configuration time because all your software is pre-packaged AMI are built for a specfic region (and can be copied across regions) You can launch EC2 instances from: A Public AMI: AWS provided Your own AMI: you make and maintain them yourself An AWS Marketplace AMI: an AMI someone else made (and potentially sells) AMI Process (from an EC2 instance) Start an EC2 instance and customize it Stop the instance (for data integrity) Build an AMI - this will also create EBS snapshots Launch instances from other AMIsEC2 Instance Store EBS volumes are network drives with good but “limited” performance If you need a high performance hardware disk, use EC2 Instance Store Better I/O performance EC2 Instance Store lose their storage if they’re stopped (ephemeral) Good for buffer / cache / scratch data / temporary content Risk of data loss if hardware fails Backups and replication are your responsibilityEBS Volume Types EBS Volumes come in 6 types gp2 / gp3 (SSD): General purpose SSD volumne that balances price and performance for a wide variety of workloads io1 / io2 (SSD) Highest-performance SSD volume for mission critical low latency or high throughput workloads stl (HDD): Low cost HDD volume designed for frequency accessed, throughput-intensive workloads scl (HDD): Lowest cost HDD volume designed for less frequently accessed workloads EBS Volumes are characterized in Size Throughput IOPS (I/O Ops Per Sec) When in doubt always consult the AWS documentation Only gp2/gp3 and io1/io2 can be used as boot volumesGeneral Purpose SSD Cost effective storage, low-latency System boot volumes, Virtual desktops, Development and test environments 1 GiB - 16 TiB gp3: Baseline of 3,000 IOPS and throughput of 125 MiB/s Can increase IOPS up to 16,000 and throughput up to 1000 MiB/s independently gp2: Small gp2 volumes can burst IOPS to 3,000 Size of the volume and IOPS are linked, max IOPS is 16,000 3 IOPS per GB, means at 5,334 GB we are at the max IOPS Provisioned IOPS (PIOPS) SSD Critical business applications with sustained IOPS performance Or applications that need more than 16000 IOPS Great for databases workloads (sensitive to storate perf and consistency) io1/io2 (4Gib - 16TB) Max PIOPS: 64,000 for Nitro EC2 instances &amp; 32,000 for other Can increase PIOPS independently from storage size io2 have more durability and more IOPS per GiB (at the same price as io1) io2 Block Express (4 GiB – 64 TiB): Sub-millisecond latency Max PIOPS: 256,000 with an IOPS:GiB ratio of 1,000:1 Supports EBS Multi-attachHard Disk Drives (HDD) Cannot be a boot volume 125 GB to 16 TB Throughput Opitimed HDD (stl) Big Data, Data warehouse, Log processing Max throughput 500 Mib/s - max IOPS 500 Cold HDD (scl): For data that is infrequently accessed Scenarios where lowest cost is important Max throughput 250 MiB/s - max IOPS 250 EBS Multi-Attach – io1/io2 family Attach the same EBS volume to multiple EC2 instances in the same AZ Each instance has full read &amp; write permissions to the volume Use case: Archieve higher application availability in cluster Linux application Application must manage concurent write operations Must use a file system that’s cluster-aware (not XFS, EX4, etc…)EBS Encryption When you create an encrypted EBS volume, you get the following Data at rest is encrypted inside the volume All the data in flight moving between the instance and the volume is encrypted All snapshots are encrypted All volumes created from the snapshot Encryption and decryption are handled transparently (you have nothing to do) Encryption has a minimal impact on latency EBS encryption leverages keys from KMS (AES-256) Copying an unencrypted snapshot allows encryption Snapshots of encrypted volumes are encryptedEncryption: encrypt an unencrypted EBS volume Create an EBS snapshot of the volume Encrypt the EBS snapshot ( using copy ) Create new ebs volume from the snapshot (the volume will also be encrypted) Now you can attach the encrypted volume to the original instanceAmazon EFS – Elastic File System Managed NFS (network file system) that can be mounted on many EC2 EFS works with EC2 instances in multi-AZ Highly available, scalable, expensive (3x gp2), pay per useAmazon EFS – Elastic File System Use cases: content management, web serving, data sharing, Wordpress Use NFSv4.1 protocol Uses security group to control access to EFS Compatible with Linux bases AMI (not windows) Encryption at rest using KMS POSIX file system (~Linux) that has a standard file API File system scales automatically, pay per use, no capacity planning!EFS – Performance &amp; Storage Classes EFS Scale 1000s of concurrent NFS clients, 10 GB+/s throughput Grow to Petabyte-scale network file system, automatically Perfornmance mode (set at EFS creation time) General purpose(default): latency sensitive use cases (web serve, CMS, etc…) Max I/O - higher latency, throughput, highly parallel (big data, media processing) Throughput mode Bursting (1TB = 50Mib/s + brust of up to 100MiB/s) Provisioned: set your throughput regardless of storage size, ex: 1Gib/s for | TB storage EFS – Storage Classes Storage Tiers (lifecycle management features - move file after N days) Standard: for frequently accessed files Infrequent access (EFS-IA): cost to retrieve files, lower price to store. Enable EFS-IA with a lifecycle policy Availability and durability Regional: Multi-AZ, great for prod One Zone: One AZ, greate for dev, backup enabled by default, compatible with IA (EFS one Zone-IA) Over 90% in cost savingsEBS vs EFS – Elastic Block Storage EBS volumes… Can be attached to only one instance at a time Are locked at the Availability Zone (AZ) level GP2: IO increases if the disk size increases IO1: Can increase IO independently To migrate an EBS volume across AZ Take a snapshot Restore the snapshot to another AZ EBS backups use IO and you shouldn’t run them while your application is handling a lot of traffic Root EBS Volumes of instances get terminated by default if the EC2 instance get terminated (you can disable that) EBS vs EFS – Elastic File System Mounting 100s of instances across AZ EFS share website files (WordPress) Only for Linux Instances (POSIX) EFS has a higher price point than EBS Can leverage EFS-IA for cost savings Availability Zone 1 Availability Zone 2 Remember: EFS vs EBS vs Instance StoreAWS Fundamentals – Part II Load Balancing, Auto Scaling Groups and EBS VolumesScalability &amp; High Availability Scalability means that an application / system can handle greater loads by adapting There are two kinds of scalability Vertical scalability Horizontal scalability Scalability is linked but different to high availability Let’s deep dive into the distinction, using a call center as an exampleVertical saclability Vertically scalability means increasing the size of the instance For example, you application runs on a t2.micro Scaling that application vertically means running it on a t2.large Vertical scalability is very common for non distributed systems, such as a database. RDS, ElastiCache are service that can scale vertically There’s uusually a limit to how much you can vertically scale (hardware limit)Horizontal Scalability Horizontal scalability means increasing the number of instances / systems for your application Horizontal scaling implies distributed systems. This is very common for web applications / modern applications It’s easy to horizontally scale thanks the cloud offerings such as Amazon EC2High Avaiability High availability usually goes hand in hand with horizontal scaling High Avaiability means running your application / system in at least 2 data centers(==AZ) The goal of high availability can be passive (for RDS Multi AZ for example) The high availability can active (for horizontal scaling)High Availability &amp; Scalability For EC2 Vertical Scaling: increase instance size (= scale up / down) From: t2.nano - 0.5G of RAM, 1 vCPU To: u-12tb1.metal – 12.3 TB of RAM, 448 vCPUs Horizontal scaling: Increase number of instances (scale out / in) Auto Scaling Group Load Balancer High Availability: Run instances for the same application across multi AZ Auto Scaling group multi AZ Load Balancer multi AZ What is load balancing? Load Balances are servers that forward traffic to multiple servers downstreamWhy use a load balancer? Spread load across multiple downstream instances Expose a single point of access DNS to your application Seamlessly handle failures of downstream instances Do regular health checks to your instances Provide SSL termination HTTPS for your websites Enforce stickiness with cookies High availability across zones Separate public traffic from private trafficWhy use an Elastic Load Balancer? An Elastic Load Balancer is a managed load balancer AWS gurantees that it will be working AWS takes care of upgrades, maintenace, high availability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offering / services EC2, EC2 auto scaling groups, amazone ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator Why use an Elastic Load Balancer? An Elastic load balancer is a managed load balancer AWS guarantees that it will be wroking AWS takes care of upgrades, maintaince, high vailability AWS provides only a few configuration knobs It costs less to setup your own load balancer but it will be a lot more effort on your end It is integrated with many AWS offering / services EC2, EC2 Auto scaling groups, amazon ECS AWS Certificate Manager (ACM), CloudWatch Route 53, AWS WAF, AWS Global Accelerator Health Checks Health checks are crucial for load balancers They enable the load balancer to know if instances it forwards traffic to are available to reply to requests The health check is done on a prt and a route (/health is common) If the response is not 200 (OK), then the instance is unhealthyTypes of load balancer on AWS AWS has 4 kind of managed load balancers Classic Load Balancer (v1 - old generation) - 2009 - CLB HTTP, HTTPS, TCP, SSL (secure TCP) Application load balancer (v2-new generation) - 2016 - ALB HTTP, HTTPS, Web Socket Network load balancer (v2 - new generation) - 2017 - NLB TCP, TLS (secure TCP), UDP Gateway load balancer - 202 -GWLB Operates at layer 3 (Network layer) - IP Protocol Overall, It is recommended to use the newer generation load balancers as they provide more features Some load balancers can setup as internal or external ELBsClassic Load Balancers (v1) Supports TCP (layer 4), HTTP &amp; HTTPS (layer 7) Health checks are TCP or HTTP based Fixed hostname XXX.region.elb.amazoneaws.comApplication Load Balancer (v2) Application load balancers is layer 7 (HTTP) Load balancing to multiple applications on the same machine Support for HTTP/2 and websocket Support redirects (from HTTP to HTTPS for example)Application load balancer (v2) Routing tables to different target groups Routing based on path in URL based on Hostname in URL based on query string headers ALB are a great fit for micro services &amp; container-based application Has a port mapping feature to redirect to a dynamic port in ECS In comparison, we’d need multiple classic load balancer per applicationApplication Load Balancer (v2) Target Groups EC2 instances (can be managed by an Auto Scaling Group) - HTTP ECS tasks (managed by ECS itself) - HTTP Lamba functions - HTTP request is transalted into a JSON event IP Addresses - must be private IPs ALB can route to multiple target groups Health checks are at the target group level Application Load Balancer (v2) Good to Know Fixed hostname (XXX.region.elb.amazoneaws.com) The application servers dont see the IP of the client directly The true IP of the client is inserted in the header X-FORWARDED-For We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)Network Load Balancer (v2) Network load balancers (layer 4) allow to: Forward TCP &amp; UDP traffic to your instances Handle milions of request per seconds Less latency ~ 100ms (vs 400 ms for ALB) NLB has one static Ip per AZ, and supports assigning Elastic IP(helpful for whitelisting specific IP) NLB are used for extreme performance, TCP or UDP traffic Not included in the AWS free tierNetwork load balancer (v2) TCP (layer 4) based trafficNetwork Load Balancer – Target Groups EC2 instance IP Address - must be private IPs Application load balancerGateway Load Balancer Deploy, scale, and manage a fleet of 3 party network virtual appliances in AWS Example: Firewalls, Instrusion detection and prevention systems, deep packet inspection systems, payload manipulation, … Operates at layer 3 (Network layer) Ip packets Combines the following functions: Transparent Network Gateway: single entry/exit for all traffic Load Balancer: distributes traffic to your virtual appliances Uses the Geneve protocol on port 6081GAteway load balancer - Target Groups EC2 instances IP Addresses - must be private IPsSticky Sessions (Session Affinity) It is possible to implement stickiness so that the same client is always redirected to the same instances behind a load balancer This works for classic load balancers &amp; application load balancers The “cookie” used for stickiness has an expiration date you control Use case: make sure the use doesn’t lost his session data Enabling stickiness may bring imbalance to the load over the backend EC2 instancesSticky Sessions - Cookie Names Application-based Cookies Custom cookie Can include any custom attributes required by the application Cookie name must be specified individually for each target group Don’t use AWSALB.AWSALBTB (reserved for use by the ELB) Application cookie Generated by the load balancer Cookie name is AWSALBAPP Duration based cookies Cookie generated by the load balancer Cookie name is AWSALB for ALB, AWSELB for CLB Cross-Zone Load Balancing With Cross-Zone Load Balancing: Each load balancer instance distributes evently across all registered instances in all AZ Without Cross zone load balancing: Request are distributed in the instances of the node of the elastic load balancerCross-Zone Load Balancing Application load balancer Always on (can’t be disabled) No charges for inter AZ data Network Load Balancer Disabled by default you pay charge($) for inter AZ data if enabled Classic Load Balancer Disabled by default No charges for inter AZ data if enabled High Availability (HA)aims to ensure an agreed level of operational performance ussually uptime, for a higher than normal period. 99.9%(Three 9’s) = 8.77 hours p/year downtime 99.999%(Five 9’s) = 5.26 minutes p/year downtime aims zero downtime Minimise any outagesFault-Tolerance (FT) is the property that enables a system to continue operating properly in the event of the failure of some (one or more faults within) of its components Systems design with plan fault tolerance Operate through faultsDisater recovery(DR) a set of polocies, tools and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a natural or human-inclued disaster Pre planning ——————–&gt; DR Process Used when these don’t work # Classic Solutions Architecture Section Introduction These solutions architectures are the best part of this course Let’s understand how all the technologies we’ve seen work together This is a section you need to be 100% comfortable with We’ll see the progression of a Solution’s architect mindset through many sample case studies: WhatIsTheTime.Com MyClothes.Com MyWordPress.Com Instantiating applications quickly Beanstalk Stateless Web App: WhatIsTheTime.com WhatIsTheTime.com allows people to know what time it is We don’t need a database We want to start small and can accept downtime We want to fully scale vertically and horizontally, no downtime Let’s go through the Solutions Architect journey for this app Let’s see how we can proceed! Stateless web app: What time is it? Starting simple Stateless web app: What time is it? Scaling verticallyStateless web app: What time is it? Scaling horizontallyStateless web app: What time is it? Scaling horizontallyStateless web app: What time is it? Scaling horizontally, adding and removing instancesStateless web app: What time is it? Scaling horizontally, with a load balancerStateless web app: What time is it? Scaling horizontally, with an auto-scaling groupStateless web app: What time is it? Making our app multi-AZMinimum 2 AZ =&gt; Let’s reserve capacityIn this lecture we’ve discussed… PUblic vs Private IP and EC2 instances Elastic IP vs Route 53 vs Load Balancers Route 53 TTL, A records and Alias Records Maintaining EC2 instances manually vs Auto Scaling Groups Multi AZ to survive disasters ELB health Checks Security GRoup Rules Reservation of capacity for costing saving when possible We’re considering 5 pillars for a well architected application: costs, performance, reliability, security, operational excellenceStateful Web App: MyClothes.com MyClothes.com allows people to buy clothes online. There’s a shopping cart Our website is having hundreds of users at the same time We need to scale, maintain horizontal scalability and keep our web application as stateless as possible Users should not lose their shopping cart Users should have their details (address, etc) in a database Let’s see how we can proceed!Stateful Web App: MyClothes.comStateful Web App: MyClothes.com Introduce Stickiness (Session Affinity)Stateful Web App: MyClothes.com Introduce User CookiesStateful Web App: MyClothes.com Storing User Data in a databaseStateful Web App: MyClothes.com Scaling ReadsStateful Web App: MyClothes.com Scaling Reads (Alternative) – Write ThroughStateful Web App: MyClothes.com Multi AZ – Survive disastersStateful Web App: MyClothes.com Security GroupsIn this lecture we’ve discussed… 3-tier architectures for web applications ELB sticky sessions Web clients for storing cookies and making our web app sateless ElasticCache For storing sessions (alternative: dynamoDB) For caching data from RDS Multi AZ RDS For storing user data Read replicas for caling reads Multi AZ for disaster recovery Tigh Security with security groups referencing each otherStateful Web App: MyWordPress.com We are trying to create a fully scalable WordPress website We want that website to access and correctly display picture uploads Our user data, and the blog content should be stored in a MySQL database Let’s see how we can achieve this!Stateful Web App: MyWordPress.com RDS layerStateful Web App: MyWordPress.com Scaling with Aurora: Multi AZ &amp; Read ReplicasStateful Web App: MyWordPress.com Storing images with EBSStateful Web App: MyWordPress.com Storing images with EBSStateful Web App: MyWordPress.com Storing images with EFSIn this lecture we’ve discussed… Aurora Database to have easy Multi-AZ and Read-Replicas Storing data in EBS (single instance application) Vs Storing data in EFS (distributed application)Instantiating Applications quickly When launching a full stack (EC2, EBS, RDS), it can take time to: Install applications Insert initial (or recovery) data Configure everything Launch the application We can take advantage of the cloud to speed that up!Instantiating Applications quickly EC2 Instances: Use a Golden AMI: Install your applications, OS dependencies etc.. beforehand and launch your EC2 instance from the Golden AMI Bootstrap using User Data: For dynamic configuration, use User Data scripts Hybrid: mix Gloden AMI and user Data (Elastic Beanstalk) RDS Databases: Restore from a snapshot the database will have schemas and data ready! EBS Volumes: Restore from a snapshot: the disk will already be formatted and have data! Typical architecture: Web App 3-tierDeveloper problems on AWS Manging infrastructure Deploying Code Configuring all the databases, load balancers, etc Scaling concerns Most web apps have the same architecture (ALB + ASG) All the developers want is for their code to run! Possibly, consistency across different applications and environmentsElastic Beanstalk - Overview Elastic Beantalk is a developer centric view of delpoying an application on AWS It uses all the component’s we’ve seen before: EC2, ASG, ELB, RDS, … Managed service: Automatically handles capacity provisoning, load balancing, scaling, application health monitoring, instance configuration, … Just the application code is the responsibility of the developer We still have full control over the configuration Beanstalk is free but you pay for the underlying instancesElastic Beanstalk – Components Application: collection of ElasticBeantalk components (environments, versions, configurations, …) Application version: an iteration of your application code Environment Collection of AWS resources running an application version (only one application version at a time) Tiers: Web Server Environment Tier &amp; Workerd Environment Tier you can create multiple environment (dev, test, prod) Elastic Beanstalk – Supported Platforms Go Java SE Java with Tomcat .NET Core on Linux .NET on Windows Server Node.js PHP Python Ruby Packer Builder Single Container Docker Multi-container Docker Preconfigured Docker If not supported, you can writeyour custom platform (advanced)Web Server Tier vs. Worker Tier" }, { "title": "AWS Advance: Aurora", "url": "/posts/aws-advance-elasticache-rds-and-aurora-part-3/", "categories": "Fullstack, Architect, AWS", "tags": "aurora, database", "date": "2022-12-04 00:00:00 +0700", "snippet": "Amazon Aurora Aurora is a proprietary technology from AWS (not open sourced) Postgress and MYSQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or ...", "content": "Amazon Aurora Aurora is a proprietary technology from AWS (not open sourced) Postgress and MYSQL are both supported as Aurora DB (that means your drivers will work as if Aurora was a Postgres or MYSQL database) Aurora is “AWS cloud optimized” and claims5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS Aurora storage automatically grows in increements of 10GB, up to 128 T B. Aurora have 15 replicas while MYSQL has 5, and the replication process is faster (sub 10ms replica lag) Failover in Aurora is instantaneous. It’s HA (high Avaiability) native. Aurora costs more than RDS** (20% more)** - but is more efficientAurora High Availability and read scaling 6 copies of your data across 3 AZ: 4 copies out of 6 needed for writes 3 copies out of 6 need for reads Self healing with peer to peer replication Storage is striped across 100s of volumes One Aurora instance takes write (master) Automated failover for master in less than 30 seconds Master + up to 15 Aurora read replicas serve reads Support for Cross Region Replication Shared storage Volume Replication + self healing + auto expandingAurora DB Cluster Write endpoint Pointing to the master Reader Endpoint Connection Load Balancing Features of Aurora Automatic Failover Backup annd Recovery Isolation and security Industry compliance Push-button scaling Automated Patching with Zero Downtime Advanced Monitoring Routine Maintenance Backtrack: restore data at anypoint of time without using backupsAurora Security Similar to RDS because uses the same engies Encryption at rest using KMS Automated backups, snapshots and replicas are also encrypted Encryption in flight using SSL (same proccess as MYSQL or Postgres) Possibility to autheticate usig IAM token (same method as RDS) You are responsible for protecting the instance with security groups You can’t SSHAurora Repliccas - Auto Scaling Write endpoint Many request to Reader endpoint and edpoint extended above replicas auto sccaling policies and shared storage volumeAurora - Custom Endpoints Define a subset of Aurora Instannces as a Custom endpoint Example: Run analytical queries on specific replicas The reader Endpoint is generally not used after defining custom endpointsAurora Serverless Automated database instantiation and auto scaling based on actual usage Good for infrequent, intermittent or unpredictable workloads No capacity plannig needed Pay per second, can be most cost-effective Proxy fleet is managed by AuroraAurora Multi-Master In case you want immediate failover for write node (HA) Every node does R/W - vs promoting a RR as the new masterGlobal Aurora Aurora cross region read replicas Usefule for disaster recovery Simple to put in place Aurora Global Database (recommended): 1 primary region (read / write) Up to 5 secodary (read only) regions, replication lag is less than 1 second Up to 16 read repliccas per secondary region Helps for decreasing latency Promotig another region (for disater recovery) has an RTO of &lt; 1 minute Aurora Machine Learning Enables you to add ML-based predictions to your applications via SQL Simple, optimized and secure integration between Aurora and AWS ML services Supported services Amazon SageMaker (use with any ML model) Amazon Comprehend (for sentiment analysis) You don’t need to have ML experience Use cases: fraud detection, ads targeting, sentiment analysis, product recommendationsAmazon ElastiCache Overview The same way RDS is to get managed Relational Databases… ElastiCache is to get managed Redis or Memcached Caches are in-memory databases with really high performance, low latency Helps reduce load off of databases for read intensive workloads Helps make your application stateless AWS takes care of OS maintanance / patching, optimizations, setup, configuration, monitoring, failure recovery and backups Using ElastiCache involves heavy application code changesElastiCache Solution Architecture - DB Cache Applications queries Elasticacche, if not available, get from RDS and store in ElastiCache Helps relieve load in RDS Cache must have an invalidation strategy to make sure only the most current data is used in there.ElastiCache Solution Architecture - User Session Store User logs into any of the application The application writes the session data into ElastiCache The user hits another instance of our application The instance retrieves the data ans the user is already logged in The user retrieve session from ElastiCacheElasticache: Redis vs Memcached Redis: Multi AZ with Auto Failover Read replicas to scale read and have availability Data durability using AOF persistence Backup and restore features Memcached Multi-node for pariniong of data (sharding) No high availability (replicationn) Non persistent No Backup and restore Multi threaded architedture ElastiCache - Cache Security All caches in ElastiCache Do not support IAM authentication IAM policies on ElastiCache are only used for AWS API-level security Redis AUTH You can set a “password/token” when you ccreate a Redis cluster This is an extra level of security for your cache (on top of security groups) Support SSL in flight encryptionn Memcached Supports SASL-based authentication (advanced) Patterns for ElastiCache Lazy Loading: all the read data is cached, data can become state in cache Write Through: Adds or update data in the cache when written to a DB (no scale data) Session Store: store temporary session data in a cache (using TTL features) Qoute: There are only two hard things in Computer Science: cache invalidation and naming thingsElastiCache - Redis Use Case Gaming leadeerboards are coomputationally complex Redis Sorted sets guarantee both uniqueness and element ordering Each time a new elemennt added, it’s ranked in real time, then added in correct order" }, { "title": "AWS Advance: RDS", "url": "/posts/aws-advance-alb-clb-rds-and-aurora-part-2/", "categories": "Fullstack, Architect, AWS", "tags": "aws, RDS, Encryption, DisaterRecovery, Security, Encapsolution", "date": "2022-12-04 00:00:00 +0700", "snippet": "RDSAWS RDS Overview RDS stannds for Relatioal Database service It’s a managed DB service for DB use SQL as a query language. It allow you to create databases in the cloud that are managed by AWS...", "content": "RDSAWS RDS Overview RDS stannds for Relatioal Database service It’s a managed DB service for DB use SQL as a query language. It allow you to create databases in the cloud that are managed by AWS Postgres MYSQL MariaDB Oracle Microsoft SQL Server Aurora(AWS Proprietary database) Advantage over using RDS versus deploying DB on EC2 RDS is a managed service: Automated provisioning, OS patching Conitnous backups and restore to specific timestamp (Point in Time Restore)! Monitoring dashboard Read replicas for improved read performance Maintennace windows for upgrade Sacling capacity (vertical and horizotal) Storage backed by EBS (gp2 or io1) But you can’t SSH into your instancesRDS backups Backups are automatically enabled in RDS Automated backups: Daily full backup of the database (durinng the maintenancce window) Transaction logs are baccked-up by RDS every 5 minutes ability to restore to any point in time (from oldest to 5 minutes ago) 7 days retention (can be increased to 35 days) DB snapshots: Manually triggered by the user Retention of backup for as long as you want RDS Storage Auto Scaling Helps you increase storage on your RDS DB instannce dynamically When RDS detects you are running out of free database storage, it scales automatically Avoid manually scaling your database storage You have to set Maximum Storage Threshold (maximum limit for DB storage) Automatically modify storage if Free storage is less than 10% of alloccated storage Low storage lasts at least 5 minutes 6 hours have passed sincce last modification RDS Read Replicas for read scalability Up to 5 read replicas Within AZ, Cross AZ or Cross Region Replication is ASYNC, so reads are eventually consistent Replicas can be promoted to their own DB Applications must update the connection string to leverage read replicasRDS Read replicas - Use Cases You have a production database that is taking on normal load You want to run a reporting application is unfacted Read replicas are used for SELECT(=read) only kind of statements (not INSERT, UPDATE, DELETE)RDS Read Replicas - Network Cost In AWS there’s a networ cost when data goes from one AZ to another For RDS read replicas within the same region, you don’t pay that feeRDS Multi AZ (Disater Recovery) Sync replication One DNS name - automactic app failover too standby Increase availability Failover in case of loss AZ, loss of networkk, instance or storage failure No manual intervention in apps Not used for scaling Multi AZ replication is free Note: The Read Replicas bet setup as Multi AZ for** Disater Recovery**RDS - From Single AZ to Multi AZ Zero downtime operation (no need to stop the DB) Just click on “modify” for the database The following happens internally: A snapshot is taken A new DB is retored from the snapshot in a new AZ Syncchronization is establishhed between the two databases RDS Security - Encryption At rest ecryption Possibility to encrypt the master &amp; read replicas with AWS KMS - AES-256 encryption Enccryption has to be defined at launch time If the master is not encrypted the read replicas cannot be encrypted Transparent Data Encryption (TDE) available for Oracle and SQL Server-** In-flight encryption** SSL certificates to encrypt data to RDS in flight Provide SSL options with trust certificate when connecting to database To enforce SSL: PostgreSQL: rds.force_ssl=1 i QWS RDS console MYSQL: Within the DB Grant Usage On . To ‘mycluster’@’%’ Require SSL; RDS Encryption Operations Encrypting RDS backups Snapshots of un-crypted RDS databases are un-crypted Sapshots of enncrypted RDS databases are encrypted Can copy a snapshot into an encrypted one To encrypt an un-encrypted RDS database Create a snapshot of the un-encrypted database Copy the snapshot and enable encryption for the snapshot Restore the database from the encrypted snapshot Migrate applications to the new database, and delete the old database RDS Security - Network &amp; IAM Network Security RDS databases are usually deployed within a private subnet, not in a public one RDS security works by leveraging seccurity groups (the same concept as for EC2 instances) it controls which ip / security group can comminicate with RDS Access Management IAM policies help control who can manage AWS RDS (through the RDS API) Tranditionnal Username annd Password can be used to login into the database IAM-based authentication can be used to login into RDS MYSQL &amp; PostgreSQL RDS - IAM Authentication IAM database authentication works with MYSQL and PostgreSQL You don’t need a password, just an authentication token obtained through IAM &amp; RDS API calls Auth token has a llifetime of 15 minutes Benefits: Networkk in/out must be encrypted using SSL IAM to cenntrally manage users instead of DB Cann leverage IAM roles and EC2 instance profiles for easy integration RDS Security - Summary Encryptionn at rest is done only when you first create the DB instance or: unencrypted DB =&gt; snapshot =&gt; ccopy snapshot as ecrypted =&gt; create DB from sapshot Your resposibility: Checck the ports / IP / security group inbound rules in DB’s SG In-database user creation and permission or manage through IAM Creating a database with or without public access Ensure parameter groups or DB is configured to only allow SSL connections AWS responsibility No SSH access No manual DB patching No manual OS patching No way to audit the underlying instance " }, { "title": "AWS Certified Solutions Architect - Associate (SAA-C03) Exam Guide", "url": "/posts/aws-certified-solutions-architect-associate-saa-c03-exam-guide/", "categories": "Fullstack, Architect, AWS", "tags": "aws", "date": "2022-12-03 00:00:00 +0700", "snippet": "Content outlineThis exam guide includes weightings, test domains, and task statements for the exam. It is not acomprehensive listing of the content on the exam. However, additional context for each...", "content": "Content outlineThis exam guide includes weightings, test domains, and task statements for the exam. It is not acomprehensive listing of the content on the exam. However, additional context for each of the taskstatements is available to help guide your preparation for the exam. The following table lists the maincontent domains and their weightings. The table precedes the complete exam content outline, whichincludes the additional context. The percentage in each domain represents only scored content. Domain % exam Domain 1: Design Secure Architectures 30% Domain 2: Design Resilient Architectures 26% Domain 3: Design High-Performing Architectures 24% Domain 4: Design Cost-Optimized Architectures 20% Total 100% Domain 1: Design Secure ArchitecturesTask Statement 1: Design secure access to AWS resources.knowledge of: Access controls and management across multiple accounts AWS federated access and identity services (for example, AWS Identity and Access: IAM, AWS SSO AWS global infrastructure (Availbility zone and Access Managment , AWS Region) AWS Security best practicle The AWS shared responsibility modelSkill in: Applying AWS security best practices to IAM users and root users (for example mutil-factor authentication [MFA]) Designing a flexible authoriation model that includes IAM users, groups, roles, and policies Designing a role based access control strategy (for example AWS Security Token Service [AWS STS], role switching, cross account access) Designing a security strategy for multiple AWS accounts (for example, AWS Control Tower, service control policies [SCPs]) Determining the appropriate use of resource policies for AWS services Determining when to federate a directory service with IAM rolesTask Statement 2: Design secure workloads and applications.Knowledge of: Application configuration and credentials security AWS service endpoints Control ports, protocol, and network traffic on AWS Secure appplication access Security services with appropriate use cases (for example Amaon Cognito, Amazon GuardDuty, Amazon Macie) Threat vector external to AWS (for example DDos, SQL Injection)Skill in: Designing VPC architectures with security components (for example, security groups, route table, network ACLs, NAT gateways) Determining network segmentation stategies (for example, using public subnets and private subnets) Intergrating AWS services to secure applications (for example, AWS Shield, AWS WAF, AWS SSO, AWS Secrets Manager) Securing external network connections to and from the AWS Cloud (for example VPN, AWS Direct Connect)Task Statement 3: Determine appropriate data security controlsKnowledge of: Data access and governance Data recovery Data retention and classification Encryption and appropriate key managementSkills In: Aligning AWS technologies to meet compliance requirements Encription data at rest (AWS Key Management Service [AWS KMS]) Encrypting data in transit (AWS certificate manager ACM using TLS) Implementing acces policies for encryption keys Implementing policies for data access, life cycle and protection Rotating encryption key and renewing certificatesDomain 2:Design Resilient Architecturestask statement 1: Determine approriate data security controlsknowledge of: API creation and management (API Gateway, REST API) AWS managed services with appropriate use cases (AWS Transfer family, Amazon simple queue services, secret manager) Caching strategies Design principles for microservices (stateless workloads compared with stateful workloads) Event-driven architectures Horizontal scaling and vertical scaling How to appropriately use edge accelerators ( content delivery network CDN) How to migrate application into container Load balancing concept (application load balancer) Multi-tier architectures Queuing and messaging concepts (publish/subcrile) Serverless technologies and patterns (for example, AWS Fargate, AWS Lambda) Storage types with associated characteristics (for example, object, file, block) The orchestration of containers (for example, Amazon Elastic Container Service [Amazon ECS],Amazon Elastic Kubernetes Service [Amazon EKS]) When to use read replicas Workflow orchestration (for example, AWS Step Functions)Skills in: Designing event-driven, microservice, and/or multi-tier architectures based on requirements Determining scaling strategies for components used in an architecture design Determining the AWS services required to achieve loose coupling based on requirements Determining when to use containers Determining when to use serverless technologies and patterns Recommending appropriate compute, storage, networking, and database technologies basedon requirements Using purpose-built AWS services for workloadsTask Statement 2: Design highly available and/or fault-tolerant architectures.Knowledge of: AWS global infrastructure (for example, Avaialbility ones, AWS Regions, Amazon Route 53) AWS managed services with approiate use cases (Amazon comprehend, Amazone Polly) Basic networking concepts (for example, route tables) Disaster recovery strategies(DR),*backup and restore, pilot light, warm standby, active - active failover, recovery point objective [RPO], recovery time objective [RTO]) * Distributed design pattern Failover strategies Immutable infrastructure load balancing concept (Application load balancer) Proxy concept (Amazon RDS Proxy) Service quotas and throttling (how to configure the service qoutas for a workload in a standby environment) Storage options and characteristics (for example, durability, replication) Workload visibility (for example, AWS X-Ray)Skills in: Determining automation strategies to ensure infrastructure integrity Determining the AWS services required to provide a highly available and/or fault-tolerant Determining the AWS services required to provide a highly available and/or fault-tolerant Identifying metrics based on business requirements ti deliver a high available solution Implementing designs to mitigate single point of failure Implementing strategies to ensure the durability and availability of data (for example, backups) Selecting an appropriate** DR strategy** to meet business requirements Using AWS services that improve the reliability of legacy applications and applications not builtfor the cloud (for example, when application changes are not possible) Using purpose-built AWS services for workloadsDomain 3:Design high performing architecturesTask Statement 1: Determine high-performing and/or scalable storage solutionsKnowledge of: Hybrid storage solutions to meet business requirements Storage services with appropriate use cases (for example,Amazon S3, Amazon Elastic File System [Amazon EFS], Amazon Elastic Block Store [Amazon EBS]) Storage types with associated characteristics (for example, object, file, block)Skills in: Determining storage services and configurations that meet performance demands Determining storage services that can scale to accommodate future needsTask Statement 2: Design high-performing and elastc compute solutions.Knowledge of: AWS compute services with appropriate use cases (for example, AWS Batch, Amazon EMR, AWS Fargate) Distributed computing concepts supported by AWS global infrastructure and edge services Queuing and messaging concept (for example, publish/subcrible) Scalabilities with appropriate use cases (EC2 Auto Scaling, AWS Auto Scaling) Serverless technology and patterns (Lambda, fargate) The orchestration of containers (for example, Amazon ECS, Amazon EKS)Skill in: Decoupling workloads so that components can scale independently Identifing metrics and conditions to perform scaling actions Selecting the appropriare resource type and size (the amount of Lambda memory) to meet business requirementsTask Statement 3: Determine high-performing database solutions.Knowledge of: AWS global infrastructure (for example, Availability Zones, AWS Regions) Caching strategies and services (for example, Amazon ElastiCache) Data access patterns (for example, read-intensive compared with write-intensive) Database capacity planning (for example, capacity units, instance types, Provisioned IOPS) Database connections and proxies Database engines with appropriate use cases (for example,heterogeneous migrations,homogeneous migrations) Database replication (for example, read replicas) Database types and services (for example, serverless, relational compared with non-relational, in-memory)Skills in: Configuring read replicas to meet business requirements Designing database architectures Determining an appropriate database engine (for example, MySQL compared with PostgreSQL) Determining an appropriate database type (for example,Amazon Aurora, Amazon DynamoDB) Integrating caching to meet business requirementsTask Statement 4: Determine high-performing and/or scalable network architectures.Knowledge of: Data analytics and visualization services with appropriate use cases (for example,Amazon Athena, AWS Lake Formation, Amazon QuickSight) Data ingestion patterns (for example, frequency) Data transfer services with appropriate use cases (for example, AWS DataSync, AWS Storage Gateway) Secure access to ingestion access points Sizes and speeds needed to meet business requirements Streaming data services with appropriate use cases (for example,Amazon Kinesis)Skill In: Building and securing data lakes Designing data streaming architectures Design data transfer solutions Implementting visualization strategies Selecting appropriate compute options for data processing (Amazon EMR) Selecting appropriate configurations for ingestion Transforming data between formats (.csv to .parquet)Domain 4:Design Cost-Optimized ArchitecturesTask Statement 1: Design cost-optimized storage solutions.Knowledge of: Access options (for example, an S3 bucket with Requester Pays object storage) AWS cost management service features (for example, cost allocation tags, multi-account billing) AWS cost management tools with appropriate use cases (for example, AWS Cost Explorer, AWS Budgets, AWS Cost and Usage Report) AWS storage services with appropriate use cases (for example,Amazon FSx, Amazon EFS, Amazon S3, Amazon EBS) Backup strategies Block storage options (for example, hard disk drive [HDD] volume types, solid state drive [SSD] volume types) Data lifecycles Hybrid storage options (for example , DataSync, Transfer Family, Storage Gateway) Storage access patterns Storage tiering (for example, cold tiering for object storage) Storage types with associated characteristics (for example, object, file, block)Skills in: Designing appropriate storage strategies (for example, batch uploads to Amazon S3 compared with individual uploads) Determining the correct storage size for a workload Determining the correct storage size for a workload Determining when storage auto scaling is required Managing S3 object lifecycles Selecting the appropriate backup and/or archival solution Selecting the appropriate service for data migration to storage services Selecting the appropriate storage tier Selecting the correct data lifecycle for storage Selecting the most cost-effective storage service for a workloadTask Statement 2: Design cost-optimized compute solutionsKnowledge of: AWS cost management service features (for example, cost allocation tags, multi-account billing) AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report) AWS global infrastructure (for example, Availability Zones, AWS Regions) AWS purchasing options (for example, Spot Instances, Reserved Instances, Savings Plans) Distributed compute strategies (for example, edge processing) Hybrid compute options (for example, AWS Outposts, AWS Snowball Edge) Instance types, families, and sizes (for example, memory optimized, compute optimized, virtualization) Optimization of compute utilization (for example, containers, serverless computing, microservices) Scaling strategies (for example, auto scaling, hibernation)Skills in: Determining an appropriate load balancing strategy (for example, Application Load Balancer [Layer 7] compared with Network Load Balancer[Layer 4] compared with Gateway Load Balancer) Determining appropriate scaling methods and strategies for elastic workloads (for example, horizontal compared with vertical, EC2 hibernation) Determining cost-effective AWS compute services with appropriate use cases (for example, Lambda, Amazon EC2, Fargate) Determining the required availability for different classes of workloads (for example, production workloads, non-production workloads) Selecting the appropriate instance family for a workload Selecting the appropriate instance size for a workloadTask Statement 3: Design cost-optimized database solutions.Knowledge of: AWS cost management service features (for example, cost allocation tags, multi-account billing) ** AWS cost management tools** with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report) Caching strategies Data retention policies Database capacity planning (capacity units) Database connection and proxies database engines with appropriate use case (heterogeneous migrations, homogeneous migrations) database replication (read replica) database types and services (relation compared with non-relational, Aurora, Dynamo DB)Skills in: Designing appropriate backup and retention policies (for example, snapshot frequency) Determining an appropriate database engine (for example, MySQL compared with PostgreSQL) ** Determining cost-effective AWS database services with appropriate use cases (for example, DynamoDB compared with Amazon RDS, serverless)** Determining cost-effective AWS database types (for example, time series format, columnar format) Migrating database schemas and data to different locations and/or different database enginesTask Statement 4: Design cost-optimized network architectures.Knowledge of: AWS cost management service features (for example, cost allocation tags, multi-account billing) AWS cost management tools with appropriate use cases (for example, Cost Explorer, AWS Budgets, AWS Cost and Usage Report) Load balancing concepts (for example, Application Load Balancer) NAT gateways (for example, NAT instance costs compared with NAT gateway costs) Network connectivity (for example, private lines, dedicated lines, VPNs) Network routing, topology, and peering (for example, AWS Transit Gateway, VPC peering) Network services with appropriate use cases (for example, DNS)Skills in: Configuring appropriate NAT gateway types for a network (for example, a single shared NAT gateway compared with NAT gateways for each Availability Zone) Configuring appropriate network connections (for example, Direct Connect compared with VPN compared with internet) Configuring appropriate network routes to minimize network transfer costs (for example, Region to Region, Availability Zone to Availability Zone, private to public, Global Accelerator, VPC endpoints) Determining strategic needs for content delivery networks (CDNs) and edge caching Reviewing existing workloads for network optimizations Selecting an appropriate throtting strategy Selecting the appropriate bandwidth allocation for a network device (a single VPN compared with multiple VPNs, Direct Connect speed)" }, { "title": "AWS Advance: Auto scaling and auto load balancer", "url": "/posts/aws-advance/", "categories": "Fullstack, AWS, Architect", "tags": "aws\naurora, elasticcache, rds, aurora, redis, memcache, template, asg, acl, NLB", "date": "2022-12-03 00:00:00 +0700", "snippet": "SSL/TLS - Basics An SSL Certificate allow traffic between your clients and your load balancer to be encrypted in transit (i-flight encryption) SSL refers to Secure Soccket Layeer, used to encrypt...", "content": "SSL/TLS - Basics An SSL Certificate allow traffic between your clients and your load balancer to be encrypted in transit (i-flight encryption) SSL refers to Secure Soccket Layeer, used to encrypt conections TLS refers to transport layer security, which is a newer versio Nowadays, TLS certificates are mainly used, but people still refer as SSL Public SSL certificates are issued by Certificate Authorities (CA) Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc… SSL ccertificates have an expiration date (yout set) and must be renewed\\Load Balacer - SSL Certificates The load balacer uses and** X.509 certificate** (SSL/TLS server certificcate) You can manage certificcates using ACM (AWS Certificate Manager) You can create upload your own certificates alternatively HTTPS listener You must specify a default certificate You can add an optional list of certs to support multiple domains Clients can use SNI (Server Name Indication) to specify the host name they reach Ability to specify a security policy to support older versions of SSL / TLS (legacy clients)\\ ## SSL - Server Name idication (SNI) SNI sloves the problem of loadinng multiple SSL certificates onto one web server (to serve multiple websites)-It’s a ‘newer’ protocol, and requires the client to indicate the hostname of the target server in the initial SSL hanshake The server will then find the correct certificccate, or return the default one Note: Only work for ALB &amp; CLB (newer generation), Cloudfront Does not work for CLB ( older gen)\\ Elastic Load Balancers - SSL Certicates Classic Load Balancer (v1) Support only one SSL certificate Must use multiple CLB for multiple host name with multiple SSL certificates Application Load Balanccer (v2) Supports multiple listeners with multiple SSL certificcates Uses serve name indication (SNI) to make it work Network Load Balancer (v2) Supports multiple listeners with multiple SSL certificates Uses Server Name Indication (SNI) to make it work Connection Draining Feature naming Connection Draining - for CLB Deregistration Delay - for ALB &amp; NLB Time to complete “i-fight request” while the instance is de-registerinng or unhealthy Stops sending ew request to EC2 instance which is de-registering Between 1 to 3600 seconds ( default 300 seconds) Can be disabled (set value to 0) Set to a low value if your request are short\\What’s an Auto Scaling Group? In real-life, the load on your websites and application can change In the cloud, you can create and get rid of servers very quickly The goal of an Auto Scalinng Group (ASG) is to: Scale out (add EC2 instances) to match an increased load Scale in (remove EC2 instances) to match a decreased load Ensure we have a minimum and a maximum number of machinnes running Automatically Register new instances to a load balancer\\ Auto Sccaling Group in AWS With Load Balancer A launch configuration AMI + Instance Type EC2 User data EBS Volumes Security Groups SSH key pair Min size / Max size / Initial capacity Network + Subnets informatio Load balancer information Scaling Policies\\Auto Scaling Alarms It is possible to scale an ASG based on* CloudWatch alarms* An Alarm monitors a metric (such as Average CPU) Metrics are computed for the overall ASG instances Based on the alarm We can create scale-out policies (increase the number of instances) We can create scale-in policcies (decrease the number of instances)\\ Auto scaling new rules It is now possible to define “better” auto scaling rules that are directly managed by EC2 Target Average CPU Usage Number of request on ELB per instance Average Network In Average Network Out These rules are are easier to setup and can make more sense\\Auto Scaling Custom Metric We can auto scale based on a custom metric ( number of connect users) Send custom metric from application on EC2 to CloudWatch (PutMetric API) Create CloudWatch alarm to react to low / high values Use the CloudWatch alarm as the scaling policy for ASG\\ ASG Brain Dump Scaling policies can be on CPU, Network… and can even be on custom metrics or based on a schedule (if you know your visitor patterns) ASGs use Launch configurations or Launch configuration / launch template IAM roles attached to an ASG will get assigned to EC2 instances ASG are free. You pay for the underlying resources being launched Having instances under an ASG means that if they get terminated for whatever reason, the ASG will automatically create new onces as a replacement. Extra safety! ASG can terminate instances marked as unhealthy by an ALB(and hence replace them)\\Auto Sccaling Groups - Dynamic Scaling Policies Target Trackkinng Scaling Most simple and easy to set up Example: I want the average ASG CPU to stay at around 40% Simple / Step Scaling When a CloudWatch alarm is triggerd(&gt;70%), then add 2 units When a CloudWatch alarm is triggerd(&lt;30%), then remove 1 Sechedule Actions Anticipate a sccaling based on known usage patterns Example: increase the min capacity to 10 at 5 pm on Fridays\\ Auto Scaling Groups - Predictive scaling Predictive scaling: continously forecast load and schedule scaling ahead analysis historical, generate forecast, schedule\\ Good metrics to scale on CPUUtillizatioon: Average CPUutilization across your instances RequestCountPerTarget: to make sure the number of requests per EC2 instances is stable Average Network In/Out (if you’re application is network bound) Any custom metric (that you push using CloudWatch)\\Auto Scaling Groups - Scaling cooldowns After a scaling activity happes, you are in the cooldown period (300second) During the cooldown period, the ASG will not launch or teminate additional instance (to allow for metrics to stabilize) Advice: Use a ready to use AMI to reduce configuration time in order to be serving request faster and reduce the cooldown period\\ASG for Solutions Architects ASG Default termination policy(simplified version) Find the AZ which has the most number of instaces If there are multiple instances in the AZ to choose from, delete the one with the oldest launch configuration ASG tries the balance the number of instances across AZ by default\\ASG for solutions architects lifecycle hooks By default as soon as an istance is launched in an ASG it’s in service You have the** ability to perform **extra steps before the instance goes i service(Pendinng state) You have the** ability to perform some actions** before the innstannce is terminated\\ASG for solutions architect launch template vs launch configuration Both: ID of the Amazon machie Image (AMI), the instance type, a key security groups, and the other parameter that you use to launch EC2 istances(tags, EC2 user-data) Launch Configuration (Legacy): Must be re-created every time Launch Template (newer) Can have multiple versions Create parameters subsets (partial cconfiguration for re-use annd inheritance) PRovision using both On-Demand and Spot instances Can use T2 unlimited brust features Recomend by AWS going forward " }, { "title": "Node.JS concept", "url": "/posts/nodejs-concept/", "categories": "documents, concept, tutorials, nodejs", "tags": "node.js, javascript, js, node, emitt", "date": "2022-09-02 00:00:00 +0700", "snippet": "NodeJS - ConceptNode.js is an open-source and cross platform javascript runtime environment. It is a popular tool for almost any kind of project!NodeJS is javascript on the server, built from v8 en...", "content": "NodeJS - ConceptNode.js is an open-source and cross platform javascript runtime environment. It is a popular tool for almost any kind of project!NodeJS is javascript on the server, built from v8 engine used to read, parse javascript code and executed necessary action. NodeJs i a non blocking event based IO and run on a single thread process.Event loop is constanly running process that monitors the callback queue and the call stack. The process will continously check the call stack, and if the call stack is empty, push the next function from the callback queue to the stack. If there is nothing in the callback queue, nothing will happen. Runs the V8 javascript engine, the core of Google Chrome, outside of browser. This allows Node.js to be very performant Single thread for every request provides a set of asynchronous I/O primitives using non-blocking paradigms, making blocking behaviour the exception rather than the norm. Reading network, database, file system: resume the operation when response is comeback AdonisJS, Egg.js, Express, Fastify, featherJs, Gastby, hapi, koa, loopback.io, meteor, micro, nestjs, nextjs, NX, Remix, Sapper, socker.io, strapi.const http = require('http')const hostname = '127.0.0.1'const port = 3000const server = http.createServer((req, res) =&gt; { res.statusCode = 200 res.setHeader('Content-Type', 'text/plain') res.end('Hello World\\n')})server.listen(port, hostname, () =&gt; { console.log(`Server running at http://${hostname}:${port}/`)})ProsHandle thousands of concurrent connection with a single server withou introducing the burden of managing thread concurency, which could be a significant source of bugs. unique advantage because milions of frontend developers that write javascript for the browser are now able to write client-server. ECMAScript can be used by specifics NodeJs version. Install NVM is a popular way to run Node.js. It allows you to easily switch the Node.Js version and install new version to try and easily rollback if something breaks. Javascript Topics Lexical Structure Unicode Semicolons White space Case sensitive Comments Literals and Identifiers Reserved words Expressions Name Shorthand operator Meaning Assignment x = f() x = f() Addition assignment x += f() x = x + f() Subtraction assignment x -= f() x = x - f() Multiplication assignment x *= f() x = x * f() Division assignment x /= f() x = x / f() Remainder assignment x %= f() x = x % f() Exponentiation assignment x **= f() x = x ** f() Left shift assignment x &lt;&lt;= f() x = x &lt;&lt; f() Right shift assignment x &gt;&gt;= f() x = x &gt;&gt; f() Unsigned right shift assignment x &gt;&gt;&gt;= f() x = x &gt;&gt;&gt; f() Bitwise AND assignment x &amp;= f() x = x &amp; f() Bitwise XOR assignment x ^= f() x = x ^ f() Bitwise OR assignment x \\|= f() x = x \\| f() Logical AND assignment x &amp;&amp;= f() x &amp;&amp; (x = f()) Logical OR assignment x \\|\\|= f() x \\|\\| (x = f()) Logical nullish assignment x ??= f() x ?? (x = f()) TypesJavaScript Types are Dynamic. String, Numbers, Booleans, Arrays, Objects, Undefined, Typeof ClassesClasses are a template for creating objects. They encapsulate data with code to work on that data. // unnamedlet Rectangle = class {constructor(height, width) { this.height = height; this.width = width;}};console.log(Rectangle.name);// output: \"Rectangle\" Variables4 Ways to Declare a JavaScript Variable: Using var Using let Using const Using nothing FunctionsA JavaScript function is a block of code designed to perform a particular task. thisIn JavaScript, the this keyword refers to an object.Which object depends on how this is being invoked (used or called). Arrow FunctionsAn arrow function expression is a compact alternative to a traditional function expression, but is limited and can’t be used in all situations.```JS// ———————-// Arrow Example// ———————-// A simplistic object with its very own “this”.const obj = { num: 100,};// Setting “num” on window to show how it gets picked up.window.num = 2020; // yikes!// Arrow Functionconst add = (a, b, c) =&gt; this.num + a + b + c;// callconsole.log(add.call(obj, 1, 2, 3)); // result 2026// applyconst arr = [1, 2, 3];console.log(add.apply(obj, arr)); // result 2026// bindconst bound = add.bind(obj);console.log(bound(1, 2, 3)); // result 2026Arrow functions cannot be used as constructors and will throw an error when used with new.Arrow functions do not have a prototype property.The yield keyword may not be used in an arrow function's bodyArrow functions can have either a concise body or the usual block body.```JS// An empty arrow function returns undefinedconst empty = () =&gt; {};(() =&gt; 'foobar')();// Returns \"foobar\"// (this is an Immediately Invoked Function Expression)const simple = (a) =&gt; a &gt; 15 ? 15 : a;simple(16); // 15simple(10); // 10const max = (a, b) =&gt; a &gt; b ? a : b;// Easy array filtering, mapping, etc.const arr = [5, 6, 13, 0, 1, 18, 23];const sum = arr.reduce((a, b) =&gt; a + b);// 66const even = arr.filter((v) =&gt; v % 2 === 0);// [6, 0, 18]const double = arr.map((v) =&gt; v * 2);// [10, 12, 26, 0, 2, 36, 46]// More concise promise chainspromise .then((a) =&gt; { // … }) .then((b) =&gt; { // … });// Parameterless arrow functions that are visually easier to parsesetTimeout(() =&gt; { console.log('I happen sooner'); setTimeout(() =&gt; { // deeper code console.log('I happen later'); }, 1);}, 1); Loops for - loops through a block of code a number of times for/in - loops through the properties of an object for/of - loops through the values of an iterable object while - loops through a block of code while a specified condition is true do/while - also loops through a block of code while a specified condition is true Scopes Block scope Function scope Global scope Arrays const person = [];person[\"firstName\"] = \"John\";person[\"lastName\"] = \"Doe\";person[\"age\"] = 46;person.length; // Will return 0person[0]; // Will return undefined Template LiteralsTemplate literals are literals delimited with blacktick (`) characters, allowing for multi-line string, for string interpolation with embedded expressions, and for special constructs called tagged templates. `string text ${expression} string text` SemicolonsSemicolons are an essential part of javascript code. They are read and used by the compiler to distinguising between separate statements so that statements do not lead into other parts of the code. The good news is that JavaScript includes an automatic semicolon feature. Strict ModeStrict directive was new in ECMA Script version 5.It is not a statement, but a literal expression, ignored by earlier versions of JavaScript.The purpose of \"use strict\" is to indicate that the code should be executed in “strict mode”. \"use strict\";x = 3.14; // This will cause an error because x is not declaredfunction myFunction() {\"use strict\";y = 3.14; // This will cause an error}x = {p1:10, p2:20}; // This will cause an errordelete x; // This will cause an error not allowed: duplicated, deleted, undeclare, octal number, octal escape, Writing to a read-only property, Writing to a get-only property , delete undeleteable properties, cant use eval word, a variable can not be used before it is declared,eval() can not declare a variable using the var keyword, eval() can not declare a variable using the let keyword, the arguments, with cant be used as a variable. ECMAScript 6, 2016, 2017With those concept in mind, you are well on your road to become a proficient javascript developer in both browser and in Node.js.Fundamental part of Node.js: Asynchronous programming and callbacks Timers Promises Async and Await Closures The Event Loop Asynchronous programming and callbacks Asynchronous means that things can happen independently of the main program flow.In the current consumer computers, every program runs for a specific time slot and then it stops its exceution to let another program continue their execution. This thing runs in a cycle so fast that it’s impossible to notice. We thing our computer run many programs simultaneously, but this is an illusion.Programs internally use interrupts, a signal that’s emitted to the processor to gain the attention of the system.JavaScript is synchronous by default and is single threaded. This means that code cannot create new threads and run in parallel.The browser provides a way to do it by providing a set of APIs that can handle this kind of functionality. More recently, Node.js introduced a non-blocking I/O environment to extend this concept to file access, network calls and so on.Callback You define an event handler for the click event. This event handler accpet a function, which will be called when the event is triggered const xhr = new XMLHttpRequest();xhr.onreadystatechange = () =&gt; {if (xhr.readyState === 4) { xhr.status === 200 ? console.log(xhr.responseText) : console.error('error');}};xhr.open('GET', 'https://yoursite.com');xhr.send(); browser event(DOM) access file and make XHR request setTimeOut function Handling errors in callbacks The first parameter in any callback function is the error object: error-first callbacks The problem with callbacks However every callback adds a level of nesting, and when you have lots of callbacks, the code starts to be complicated very quickly```jsfunction doStep1(init) {return init + 1;} function doStep2(init) { return init + 2;}function doStep3(init) { return init + 3;}function doOperation() { let result = 0; result = doStep1(result); result = doStep2(result); result = doStep3(result); console.log(result: ${result});}doOperation();Because we have to call callbacks inside callbacks, we get a deeply nested doOperation() function, which is much harder to read and debug. This is sometimes called \"callback hell\" or the \"pyramid of doom\" (because the indentation looks like a pyramid on its side).For these reasons, most modern asynchronous APIs don't use callbacks. Instead, the foundation of asynchronous programming in JavaScript is the Promise### Timers- SetTimeout : you soecify a callback function to execute later and value expressing how later you want it to run, in milisecons.```JSconst myFunction = (firstParam, secondParam) =&gt; { // do something};// runs after 2 secondssetTimeout(myFunction, 2000, firstParam, secondParam);// I changed my mindclearTimeout(id);Promiselet done = trueconst isItDoneYet = new Promise((resolve, reject) =&gt; { if (done) { const workDone = 'Here is the thing I built' resolve(workDone) } else { const why = 'Still working on something else' reject(why) }})const checkIfItsDone = () =&gt; { isItDoneYet .then(ok =&gt; { console.log(ok) }) .catch(err =&gt; { console.error(err) })}checkIfItsDone()A promise is commonly defined as a proxy for a value that will eventually become avaiable. promise is oneway to deal with asynchronous code.How promise working When a promise called, it will start in a pending state. This means that the calling function continues executing, while the promise is pending until it resolves, giving the calling function whatever data was being requested. The created promise will end in resolved state or rejected state. Calling the respective callback function upon finishing. PromisifyingThis technique is a way to be able to use a classic JavaScript function that takes a callback, and have it return a promise.```jsconst fs = require(‘fs’);const getFile = fileName =&gt; { return new Promise((resolve, reject) =&gt; { fs.readFile(fileName, (err, data) =&gt; { if (err) { reject(err); // calling reject will cause the promise to fail with or without the error passed as an argument return; // and we don’t want to go any further } resolve(data); }); });};getFile(‘/etc/passwd’) .then(data =&gt; console.log(data)) .catch(err =&gt; console.error(err));**Consuming a promise**```jsconst isItDoneYet = new Promise(/* ... as above ... */);// ...const checkIfItsDone = () =&gt; { isItDoneYet .then(ok =&gt; { console.log(ok); }) .catch(err =&gt; { console.error(err); });};Chaining promisesconst status = response =&gt; { if (response.status &gt;= 200 &amp;&amp; response.status &lt; 300) { return Promise.resolve(response); } return Promise.reject(new Error(response.statusText));};const json = response =&gt; response.json();fetch('/todos.json') .then(status) // note that the `status` function is actually **called** here, and that it **returns a promise*** .then(json) // likewise, the only difference here is that the `json` function here returns a promise that resolves with `data` .then(data =&gt; { // ... which is why `data` shows up here as the first parameter to the anonymous function console.log('Request succeeded with JSON response', data); }) .catch(error =&gt; { console.log('Request failed', error); });Cascading Errornew Promise((resolve, reject) =&gt; { throw new Error('Error');}) .catch(err =&gt; { throw new Error('Error'); }) .catch(err =&gt; { console.error(err); });Orchestrating promisePromise have two function synchonized Promise.all()```jsconst f1 = fetch(‘/something.json’);const f2 = fetch(‘/something2.json’);Promise.all([f1, f2]) .then(res =&gt; { console.log(‘Array of results’, res); }) .catch(err =&gt; { console.error(err); });- Promise.race()Promise.race() runs when the first of the promises you pass to it settles- Promise.any()Promise settle when any of the promise you pass to it fulfill or all of promise is get rejected(AggregateError)**Common errors**- Uncaught TypeError: undefined is not a promise- UnhandledPromiseRejectionWarning### ThenablesThe JavaScript ecosystem had made multiple Promise implementations long before it became part of the language. Despite being represented differently internally, at the minimum, all Promise-like objects implement the Thenable interface. A thenable implements the .then() method, which is called with two callbacks: one for when the promise is fulfilled, one for when it's rejected. Promises are thenables as well.```jsconst aThenable = { then(onFulfilled, onRejected) { onFulfilled({ // The thenable is fulfilled with another thenable then(onFulfilled, onRejected) { onFulfilled(42); }, }); },};Promise.resolve(aThenable); // A promise fulfilled with 42 Complex promise```js“use strict”;let promiseCount = 0;function testPromise() { const thisPromiseCount = ++promiseCount; const log = document.getElementById(“log”); // begin log.insertAdjacentHTML(“beforeend”, ${thisPromiseCount}) Started&lt;br&gt;); // We make a new promise: we promise a numeric count of this promise, starting from 1 (after waiting 3s) const p1 = new Promise((resolve, reject) =&gt; { // The executor function is called with the ability to resolve or reject the promise log.insertAdjacentHTML( “beforeend”, ${thisPromiseCount}) Promise constructor&lt;br&gt; ); // This is only an example to create asynchronism setTimeout(() =&gt; { // We fulfill the promise ! resolve(thisPromiseCount); }, Math.random() * 2000 + 1000); });// We define what to do when the promise is resolved with the then() call, // and what to do when the promise is rejected with the catch() call p1.then((val) =&gt; { // Log the fulfillment value log.insertAdjacentHTML(“beforeend”, ${val}) Promise fulfilled&lt;br&gt;); }).catch((reason) =&gt; { // Log the rejection reason console.log(Handle rejected promise (${reason}) here.); }); // end log.insertAdjacentHTML(“beforeend”, ${thisPromiseCount}) Promise made&lt;br&gt;);}const btn = document.getElementById(“make-promise”);btn.addEventListener(“click”, testPromise);```html&lt;button id=\"make-promise\"&gt;Make a promise!&lt;/button&gt;&lt;div id=\"log\"&gt;&lt;/div&gt;Modern Asynchronous JavaScript with Async and AwaitAsync function are a combination of promise and generators, and basically, they are higher level abstraction over promises. async/await is built on promise Example when async function return a promise. An await function is calling until the promise is resolved or rejected.```jsconst doSomethingAsync = () =&gt; {return new Promise(resolve =&gt; { setTimeout(() =&gt; resolve(‘I did something’), 3000)})}const doSomething = async () =&gt; { console.log(await doSomethingAsync())}console.log(‘Before’)doSomething()console.log(‘After’)- **Promise vs async/await**```js// thenableconst getFirstUserData = () =&gt; { return fetch('/users.json') // get users list .then(response =&gt; response.json()) // parse JSON .then(users =&gt; users[0]) // pick first user .then(user =&gt; fetch(`/users/${user.name}`)) // get user data .then(userResponse =&gt; userResponse.json()); // parse JSON};getFirstUserData();// implemented by async/awaitconst getFirstUserData = async () =&gt; { const response = await fetch('/users.json'); // get users list const users = await response.json(); // parse JSON const user = users[0]; // pick first user const userResponse = await fetch(`/users/${user.name}`); // get user data const userData = await userResponse.json(); // parse JSON return userData;};getFirstUserData(); Multiple async functions in series```jsconst promiseToDoSomething = () =&gt; {return new Promise(resolve =&gt; { setTimeout(() =&gt; resolve(‘I did something’), 10000)})}const watchOverSomeoneDoingSomething = async () =&gt; { const something = await promiseToDoSomething() return something + ‘\\nand I watched’}const watchOverSomeoneWatchingSomeoneDoingSomething = async () =&gt; { const something = await watchOverSomeoneDoingSomething() return something + ‘\\nand I watched as well’}watchOverSomeoneWatchingSomeoneDoingSomething().then(res =&gt; { console.log(res)})### ClosuresThe combination of function bundled together (enclosed) with references to it surround state (THE LEXICAL environment). a closure gives you access to an outer function's scope from an inner function.```jsfunction showHelp(help) { document.getElementById('help').textContent = help;}function makeHelpCallback(help) { return function () { showHelp(help); };}function setupHelp() { var helpText = [ { id: 'email', help: 'Your e-mail address' }, { id: 'name', help: 'Your full name' }, { id: 'age', help: 'Your age (you must be over 16)' }, ]; for (var i = 0; i &lt; helpText.length; i++) { var item = helpText[i]; document.getElementById(item.id).onfocus = makeHelpCallback(item.help); }}setupHelp();Scoping with let and constTraditionally (before ES6), javascript only had two kind of scope: function scope and global scope. Variables declare with var are either function scoped and global scope, depending on whether they are declared within a function or outside function. This can be tricky, because blocks with curly braces do not create scope With const you can create a block scoped: temporal dead zone! A closure is the combination of a function and the lexical environment within which that function was declared. This environment consits of any local variables that were in-scope at the time the closure was created. Emulating private methods with closures ```JSconst counter = (function () {let privateCounter = 0;function changeBy(val) { privateCounter += val;} return { increment() { changeBy(1); }, decrement() { changeBy(-1); }, value() { return privateCounter; },};})(); console.log(counter.value()); // 0.counter.increment();counter.increment();console.log(counter.value()); // 2.counter.decrement();console.log(counter.value()); // 1.JS, prior to class, didn't have a native way of declaring private methods, but it was possible to emulate private methods using closure. Private methods aren't just useful fo restricting access to code. They also prove a powerful way of managing your global namespace.#### Closure scope- Local scope (own scope)- Enclosing scope (can be block, function, or module scope)- Global scope```JS// global scopeconst e = 10;function sum(a) { return function (b) { return function (c) { // outer functions scope return function (d) { // local scope return a + b + c + d + e; }; }; };}console.log(sum(1)(2)(3)(4)); // log 20You can also write without anonymous functions:// global scopeconst e = 10;function sum(a) { return function sum2(b) { return function sum3(c) { // outer functions scope return function sum4(d) { // local scope return a + b + c + d + e; }; }; };}const sum2 = sum(1);const sum3 = sum2(2);const sum4 = sum3(3);const result = sum4(4);console.log(result); //log 20Event loopIn Nodejs Javascript, event loop is the “killer features” while nodejs run on single thread. There is just one thing happening at a timeThat’s actually helpful as it simplifier alot how a program without worry about concurrency issues. You just need to pay attention to write your code and avoid anything that could block the tread, like synchronous network call and infinite loops. In general, in most broswer there is an event loop for every browser tab, to make every process isolated and avoid webpage with infinite loop and heavy processsing to block your entire broswer. The environment manages multiple concurent event loops, to handle API call for examples. Webworker can handler multiple event loop as well So you need to concerned that your code will run on a single event loop, and write code with this thing in mind to avoid blocking it. Blocking the event loop Any javascript code that take long to return back control to the event loop will block the execution of any javascript code in the page, event block the UI page, and the user cannot click around, scroll page, and so on.All I/O primitive in JS are non-blocking. Network requests, filesystems operation, and so on. Being blocking is the exception, and this is why javascript is based so much on callback, and more recently on promise and async/await. The call stack(LIFO) The event loop continously check the call stack to see if there’s any function that need to run.While doing so, it adds any function call it find in the call stack and executes each one in order.You know the error stack trace you might be familiar. A simple event loop explanation The event loop on every iteration looks if there’s something in the call stack, and executes it until the call stack is empty.```jsconst bar = () =&gt; console.log(‘bar’) const baz = () =&gt; console.log(‘baz’)const foo = () =&gt; { console.log(‘foo’) bar() baz()}foo()### Queuing function executionThe above example looks normal, there's nothing special about it: JavaScript finds things to execute, runs them in order.Let's see how to defer a function until the stack is clear.The use case of setTimeout(() =&gt; {}, 0) is to call a function, but execute it once every other function in the code has executed.```JSconst bar = () =&gt; console.log('bar')const baz = () =&gt; console.log('baz')const foo = () =&gt; { console.log('foo') setTimeout(bar, 0) baz()}foo()When this code runs, first foo() is called. Inside foo() we first call setTimeout, passing bar as an argument, and we instruct it to run immediately as fast as it can, passing 0 as the timer. Then we call baz().The Message QueueWhen setTimeOut is called. Browser or Node.JS start a timer. Once the timer expire as in this case we put 0 as the timeout. The call backfunction intermediately put in the Message queue.Message queue is also where user initiated event like click, scroll, mouse or keyboard, fetch reponse before your code has opportunity to reach to them. Or also DOM event like onload.The loop gives the priority to the callstack, and it’s first find in the callstack, and once there’s nothing in there. Ir’s goes to pich up things in the message queue.ES6 Job QueueECMA script 2015 introduced concept of the job queue, which is used by Promise. It’s a way to execute a result of an async function as soon as posible, rather puting the end of the call stack.Promise is resole the current function ends will execute right after the function current function.Similar to a rollercoaster ride at an amusement park: the message queue puts you at the back of the queue, behind all the other people, where you will have to wait for your turn, while the job queue is the fastpass ticket that lets you take another ride right after you finished the previous one.const bar = () =&gt; console.log('bar')const baz = () =&gt; console.log('baz')const foo = () =&gt; { console.log('foo') setTimeout(bar, 0) new Promise((resolve, reject) =&gt; resolve('should be right after baz, before bar') ).then(resolve =&gt; console.log(resolve)) baz()}foo()That’s a big difference between Promises (and Async/await, which is built on promises) and plain old asynchronous functions through setTimeout() or other platform APIs.Finally, here’s what the call stack looks like for the example above:Differences between Node.js and the Browser NodeJs give Huge advance, you can perform all your work on the web, both client and server. The confor of programming everything - the frontend and the backend in a single languages. Another big difference is that in Node.js you control the environment. Unless you are building an open source application that anyone can deploy anywhere, you know which version of Node.js you will run the application on. This means that you can write all the modern ES6-7-8-9 JavaScript that your Node.js version supports. Node.js supports both the CommonJS and ES module systems (since Node.js v12), while in the browser we are starting to see the ES Modules standard being implemented. This means that you can use both require() and import in Node.js, while you are limited to import in the browser. The V8 JavaScript Engine V8 is the name of the JavaScript engine that powers Google Chrome. Firefox has SpiderMonkey Safari has JavaScriptCore (also called Nitro) Edge was originally based on Chakra but has more recently been rebuilt using Chromium and the V8 engine. The quest for performance V8 is written in C++, and it’s continously improved. It is portable and runs on Mac, Windows, Linux and several other systems. V8 is always evolving, just like the other javascript engines around, to speed up the Web and the Node.js ecosystem. Compilation Javascript is internally compiled by V8 with just in time(JIT), and compilation to speed up the excecution. Run Node.js scripts The usual way to run a Node.js program is to run globally available node command.If your main Node.js application file is app.js, you can call it by typing: chmod u+x app.jsnode app.js The content of file app.js```js#!/usr/bin/env node // your code### Restart the application automaticallyInstall `nodemon` module### How to exit from a Node.js program```JSprocess.exit(1);process.exitCode = 1;Another example:const express = require('express');const app = express();app.get('/', (req, res) =&gt; { res.send('Hi!');});const server = app.listen(3000, () =&gt; console.log('Server ready'));process.on('SIGTERM', () =&gt; { server.close(() =&gt; { console.log('Process terminated'); });});Express is a framework that uses the http module under the hood, app.listen() returns an instance of http. You would use https.createServer if you needed to serve your app using HTTPS, as app.listen only uses the http module.Node.js, accept arguments from the command linenode app.js var_1=abcHow to read environment variables from Node.jsUSER_ID=239482 USER_KEY=foobar node app.js// process by JSprocess.env.USER_ID; // \"239482\"process.env.USER_KEY; // \"foobar\" Note: process does not require a “require”, it’s automatically available.If you have multiple environment variables in your node project, you can create .env file in the root directory of your project, and use the dotenv package to load them during runtime. # .env fileUSER_ID=\"239482\"USER_KEY=\"foobar\"NODE_ENV=\"development\" The way you retrieve it is using the process object built into Node.jsIt exposes an argv property, which is an array that contains all the command line invocation arguments. process.argv.forEach((val, index) =&gt; { console.log(`${index}: ${val}`);});const args = require('minimist')(process.argv.slice(2));args.name; ```JSrequire(‘dotenv’).config();process.env.USER_ID; // “239482”process.env.USER_KEY; // “foobar”process.env.NODE_ENV; // “development”&gt; You can also run your js file with `node -r dotenv/config index.js` command if you don't want to import the package in your code.You can install `minimist` package using `npm`. and when retrieve variable from nodejs use: `node app.js --name=joe`### Output to the command line using Node.jsYou can pass multiple variables to `console.log````JSconst x = 'x';const y = 'y';console.log(x, y);console.log('My %s has %d ears', 'cat', 2);console.clear()const x = 1const y = 2const z = 3console.count( 'The value of x is ' + x + ' and has been checked .. how many times?')console.count( 'The value of x is ' + x + ' and has been checked .. how many times?')console.count( 'The value of y is ' + y + ' and has been checked .. how many times?')Print stach tracefunction2 = () =&gt; { console.trace()}function1 = () =&gt; function2();function1();Output consoleTrace at function2 (repl:1:33) at function1 (repl:1:25) at repl:1:1 at ContextifyScript.Script.runInThisContext (vm.js:44:33) at REPLServer.defaultEval (repl.js:239:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:440:10) at emitOne (events.js:120:20) at REPLServer.emit (events.js:210:7)Calculate time spentCalculate how much time a function you take to run: timetime, timeEndconst doSomething = () =&gt; console.log('test');const measureDoingSomething = () =&gt; { console.time('doSomething()'); // do something, and measure the time it takes doSomething(); console.timeEnd('doSomething()');};measureDoingSomething();Create a progress barProgress is an awesome package that help create an progress bar in console. Installing it using npm install progress.const ProgressBar = require('progress');const bar = new ProgressBar(':bar', { total: 10 });const timer = setInterval(() =&gt; { bar.tick(); if (bar.complete) { clearInterval(timer); }}, 100);Server side Pagination in Node.js with Sequelize &amp; MySQLServer side pagination is better for: Larger data set Faster initial page load Accessibility for those running javascript Complex view running Node.js Pagination with Sequelize and MySQL overview Assume display a table below Node.js Express Server will exports API for pagination (with/without filter), here are some url samples: /api/tutorials?page=1&amp;size=5 /api/tutorials?size=5: using default value for page /api/tutorials?title=data&amp;page=1&amp;size=3: pagination &amp; filter by title containing ‘data’ /api/tutorials/published?page=2: pagination &amp; filter by ‘published’ statusResult by json: { \"totalItems\": 8, \"tutorials\": [...], \"totalPages\": 3, \"currentPage\": 1} Node.js Sequelize for Pagination Sequelize provide way to implement pagination with offset and limit offset: quantity of items to skip limit: quantity of items to fetchExample: { offset: 3, limit: 2 }: skip first 3 items, fetch 4th and 5th items. Sequelize findAll model.findAll({limit: 2,offset: 3,where: { title: { [Op.like]: `%js%` } }, // conditions}); The result: [ { \"id\": 4, \"title\": \"bezkoder Tut#4 Rest Apis\", \"description\": \"Tut#4 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:07.000Z\", \"updatedAt\": \"2020-06-05T11:55:07.000Z\" }, { \"id\": 5, \"title\": \"bezkoder Tut#5 MySQL\", \"description\": \"Tut#5 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:11.000Z\", \"updatedAt\": \"2020-06-05T11:55:11.000Z\" }] Sequelize findAndCountAll Template model.findAndCountAll({limit: 2,offset: 3,where: {}, // conditions}); Result { \"count\": 8, \"rows\": [ { \"id\": 4, \"title\": \"bezkoder Tut#4 Rest Apis\", \"description\": \"Tut#4 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:07.000Z\", \"updatedAt\": \"2020-06-05T11:55:07.000Z\" }, { \"id\": 5, \"title\": \"bezkoder Tut#5 MySQL\", \"description\": \"Tut#5 Description\", \"published\": false, \"createdAt\": \"2020-06-05T11:55:11.000Z\", \"updatedAt\": \"2020-06-05T11:55:11.000Z\" } ]} Setup Node.js Express Application Install necessary modules: express, sequelize, mysql2Run the command: npm install express sequelize mysql2 cors --save The Node.js project structure that we only need to add some changes to make the pagination work well. Configure MySQL database &amp; Sequelize DB configure module.exports = {HOST: \"localhost\",USER: \"root\",PASSWORD: \"123456\",DB: \"testdb\",dialect: \"mysql\",pool: { max: 5, min: 0, acquire: 30000, idle: 10000}}; Initialize Sequelize Create app/models/index.js with the following code: const dbConfig = require(\"../config/db.config.js\");const Sequelize = require(\"sequelize\");const sequelize = new Sequelize(dbConfig.DB, dbConfig.USER, dbConfig.PASSWORD, {host: dbConfig.HOST,dialect: dbConfig.dialect,operatorsAliases: false,pool: { max: dbConfig.pool.max, min: dbConfig.pool.min, acquire: dbConfig.pool.acquire, idle: dbConfig.pool.idle}});const db = {};db.Sequelize = Sequelize;db.sequelize = sequelize; Don’t forget to call sync() method in server.js ...const app = express();app.use(...);const db = require(\"./app/models\");db.sequelize.sync();/*db.sequelize.sync({ force: true }).then(() =&gt; {console.log(\"Drop and re-sync db.\");});*/... Create Data Model In models folder, create tutorial.model.js file like this: odule.exports = (sequelize, Sequelize) =&gt; {const Tutorial = sequelize.define(\"tutorial\", { title: { type: Sequelize.STRING }, description: { type: Sequelize.STRING }, published: { type: Sequelize.BOOLEAN }});return Tutorial;}; After initializing Sequelize, we don’t need to write CRUD functions, Sequelize supports all of them. Now you can easily use following methods with pagination: get all Tutorials: findAll({ limit, offsCreate Node.js Express API layeret }) find all Tutorials by title: findAll({ where: { title: … }, limit, offset }) find and count all Tutorials: findAndCountAll({ limit, offset }) find and count all Tutorials by title: findAndCountAll({ where: { title: … }, limit, offset }) Controller with Pagination So, let’s write the function to map default response to desired structure: const getPagination = (page, size) =&gt; {const limit = size ? +size : 3;const offset = page ? page * limit : 0;return { limit, offset };};const getPagingData = (data, page, limit) =&gt; {const { count: totalItems, rows: tutorials } = data;const currentPage = page ? +page : 0;const totalPages = Math.ceil(totalItems / limit);return { totalItems, tutorials, totalPages, currentPage };}; Now the code in tutorial.controller.js will look like this: const db = require(\"../models\");const Tutorial = db.tutorials;const Op = db.Sequelize.Op;const getPagination = ...;const getPagingData = ...;// Retrieve all Tutorials from the database.exports.findAll = (req, res) =&gt; {const { page, size, title } = req.query;var condition = title ? { title: { [Op.like]: `%${title}%` } } : null;const { limit, offset } = getPagination(page, size);Tutorial.findAndCountAll({ where: condition, limit, offset }) .then(data =&gt; { const response = getPagingData(data, page, limit); res.send(response); }) .catch(err =&gt; { res.status(500).send({ message: err.message || \"Some error occurred while retrieving tutorials.\" }); });};// find all published Tutorial Create Node.js Express API layer module.exports = app =&gt; {const tutorials = require(\"../controllers/tutorial.controller.js\");var router = require(\"express\").Router();// Retrieve all Tutorialsrouter.get(\"/\", tutorials.findAll);// Retrieve all published Tutorialsrouter.get(\"/published\", tutorials.findAllPublished);...app.use('/api/tutorials', router);}; Accept input from the command line in Node.js readable stream such as process.stdin and use module readline to perform access input from the command line```JSconst readline = require(‘readline’).createInterface({input: process.stdin,output: process.stdout,}); readline.question(What's your name?, name =&gt; { console.log(Hi ${name}!); readline.close();});Another example to perform by `inquirer````jsconst inquirer = require('inquirer');const questions = [ { type: 'input', name: 'name', message: \"What's your name?\", },];inquirer.prompt(questions).then(answers =&gt; { console.log(`Hi ${answers.name}!`);});Expose functionality from a Node.js file using exportsWhen you want to import something you useconst library = require('.library')This is what the module.exports API offfered by the module system to allow us to do.You can do so in 2 ways// car.jsconst car = { brand: 'Ford', model: 'Fiesta',};module.exports = car;// index.jsconst car = require('./car')The second way is to add the exported object as a property of exports. This way allows you to export multiple objects, functions or data:const car = { brand: 'Ford', model: 'Fiesta',};exports.car = car;Or directlyexports.car = { brand: 'Ford', model: 'Fiesta',};And in the other file, you’ll use it by referencing a property of your import:const items = require('./car');const { car } = items;Or you can use a destructuring assigementconst { car } = require('./car')Example// car.jsexports.car = { brand: 'Ford', model: 'Fiesta',};module.exports = { brand: 'Tesla', model: 'Model S',};// app.jsconst tesla = require('./car');const ford = require('./car').car;console.log(tesla, ford);This will print { brand: ‘Tesla’, model: ‘Model S’ } undefined since the require function’s return value has been updated to the object that module.exports points to, so the property that exports added can’t be accessed.An introduction to the npm package manager npm yarn pnpmInstall from single packageOften you’ll see more flags added to this command: –save-dev installs and adds the entry to the package.json file devDependencies –no-save installs but does not add the entry to the package.json file dependencies –save-optional installs and adds the entry to the package.json file optionalDependencies –no-optional will prevent optional dependencies from being installedShorthands of the flags can also be used: -S: –save -D: –save-dev -O: –save-optionalTo Update npm updatenpm update &lt;package-name&gt; You can manage version by npm. Install or update package with version by command npm install &lt;package-name&gt;@&lt;version&gt; Running Tasks npm run &lt;task-name&gt; {\"scripts\": { \"watch\": \"webpack --watch --progress --colors --config webpack.conf.js\", \"dev\": \"webpack --progress --colors --config webpack.conf.js\", \"prod\": \"NODE_ENV=production webpack -p --config webpack.conf.js\",}} Where does npm install the packages? a local install a global installBy default, the package is installed in the current file tree, under the node_modules subfolder. A global installation is performed using the -g flag The package.json guide The file structure of package.json is present in Json file copy{\"name\": \"test-project\",\"version\": \"1.0.0\",\"description\": \"A Vue.js project\",\"main\": \"src/main.js\",\"private\": true,\"scripts\": { \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\", \"start\": \"npm run dev\", \"unit\": \"jest --config test/unit/jest.conf.js --coverage\", \"test\": \"npm run unit\", \"lint\": \"eslint --ext .js,.vue src test/unit\", \"build\": \"node build/build.js\"},\"dependencies\": { \"vue\": \"^2.5.2\"},\"devDependencies\": { \"autoprefixer\": \"^7.1.2\", \"babel-core\": \"^6.22.1\", \"babel-eslint\": \"^8.2.1\", \"babel-helper-vue-jsx-merge-props\": \"^2.0.3\", \"babel-jest\": \"^21.0.2\", \"babel-loader\": \"^7.1.1\", \"babel-plugin-dynamic-import-node\": \"^1.2.0\", \"babel-plugin-syntax-jsx\": \"^6.18.0\", \"babel-plugin-transform-es2015-modules-commonjs\": \"^6.26.0\", \"babel-plugin-transform-runtime\": \"^6.22.0\", \"babel-plugin-transform-vue-jsx\": \"^3.5.0\", \"babel-preset-env\": \"^1.3.2\", \"babel-preset-stage-2\": \"^6.22.0\", \"chalk\": \"^2.0.1\", \"copy-webpack-plugin\": \"^4.0.1\", \"css-loader\": \"^0.28.0\", \"eslint\": \"^4.15.0\", \"eslint-config-airbnb-base\": \"^11.3.0\", \"eslint-friendly-formatter\": \"^3.0.0\", \"eslint-import-resolver-webpack\": \"^0.8.3\", \"eslint-loader\": \"^1.7.1\", \"eslint-plugin-import\": \"^2.7.0\", \"eslint-plugin-vue\": \"^4.0.0\", \"extract-text-webpack-plugin\": \"^3.0.0\", \"file-loader\": \"^1.1.4\", \"friendly-errors-webpack-plugin\": \"^1.6.1\", \"html-webpack-plugin\": \"^2.30.1\", \"jest\": \"^22.0.4\", \"jest-serializer-vue\": \"^0.3.0\", \"node-notifier\": \"^5.1.2\", \"optimize-css-assets-webpack-plugin\": \"^3.2.0\", \"ora\": \"^1.2.0\", \"portfinder\": \"^1.0.13\", \"postcss-import\": \"^11.0.0\", \"postcss-loader\": \"^2.0.8\", \"postcss-url\": \"^7.2.1\", \"rimraf\": \"^2.6.0\", \"semver\": \"^5.3.0\", \"shelljs\": \"^0.7.6\", \"uglifyjs-webpack-plugin\": \"^1.1.1\", \"url-loader\": \"^0.5.8\", \"vue-jest\": \"^1.0.2\", \"vue-loader\": \"^13.3.0\", \"vue-style-loader\": \"^3.0.1\", \"vue-template-compiler\": \"^2.5.2\", \"webpack\": \"^3.6.0\", \"webpack-bundle-analyzer\": \"^2.9.0\", \"webpack-dev-server\": \"^2.9.1\", \"webpack-merge\": \"^4.1.0\"},\"engines\": { \"node\": \"&gt;= 6.0.0\", \"npm\": \"&gt;= 3.0.0\"},\"browserslist\": [\"&gt; 1%\", \"last 2 versions\", \"not ie &lt;= 8\"]} There are lots of things going on here: version indicates the current version name sets the application/package name description is a brief description of the app/package main sets the entry point for the application private if set to true prevents the app/package to be accidentally published on npm scripts defines a set of node scripts you can run dependencies sets a list of npm packages installed as dependencies devDependencies sets a list of npm packages installed as development dependencies engines sets which versions of Node.js this package/app works on browserslist is used to tell which browsers (and their versions) you want to support The package-lock.json file The goal of package-lock.json file is to keep track of the exact version of every package that is installed so that a product is 100% reproducible in the same way even if packages are updated by their maintainers. Find the installed version of an npm package To see the version of all installed npm packages, including their dependencies npm list for example ❯ npm list/Users/joe/dev/node/cowsay└─┬ cowsay@1.3.1├── get-stdin@5.0.1├─┬ optimist@0.6.1│ ├── minimist@0.0.10│ └── wordwrap@0.0.3├─┬ string-width@2.1.1│ ├── is-fullwidth-code-point@2.0.0│ └─┬ strip-ansi@4.0.0│ └── ansi-regex@3.0.0└── strip-eof@1.0.0 To get only your top-level packages (basically, the ones you told npm to install and you listed in the package.json), run npm list –depth=0: Install an older version of an npm package You might also be interested in listing all the previous versions of a package. You can do it with npm view versions:```bash❯ npm view cowsay versions [ ‘1.0.0’, ‘1.0.1’, ‘1.0.2’, ‘1.0.3’, ‘1.1.0’, ‘1.1.1’, ‘1.1.2’, ‘1.1.3’, ‘1.1.4’, ‘1.1.5’, ‘1.1.6’, ‘1.1.7’, ‘1.1.8’, ‘1.1.9’, ‘1.2.0’, ‘1.2.1’, ‘1.3.0’, ‘1.3.1’ ]Install package```bashnpm install &lt;package&gt;@&lt;version&gt;Semantic Versioning using npmAll versions have 3 digits: x.y.z the first digit is the major version the second digit is the minor version the third digit is the patch versionLet’s see those rules in detail: ^: It will only do updates that do not change the leftmost non-zero number i.e there can be changes in minor version or patch version but not in major version. If you write ^13.1.0, when running npm update, it can update to 13.2.0, 13.3.0 even 13.3.1, 13.3.2 and so on, but not to 14.0.0 or above. ~: if you write ~0.13.0 when running npm update it can update to patch releases: 0.13.1 is ok, but 0.14.0 is not. &gt;: you accept any version higher than the one you specify &gt;=: you accept any version equal to or higher than the one you specify &lt;=: you accept any version equal or lower to the one you specify &lt;: you accept any version lower than the one you specify =: you accept that exact version -: you accept a range of versions. Example: 2.1.0 - 2.6.2   : you combine sets. Example: &lt; 2.1   &gt; 2.6 no symbol: you accept only that specific version you specify (1.2.1) latest: you want to use the latest version available Uninstalling npm packages npm uninstall &lt;package-name&gt;npm uninstall -g &lt;package-name&gt; npm global or local packages In your code you can only require local packages: require('package-name') A package should be installed globally when it provides an executable command that you run from the shell (CLI), and it’s reused across projects.Great examples of popular global packages which you might know are npm vue-cli grunt-cli mocha react-native-cli gatsby-cli forever nodemonYou probably have some packages installed globally already on your system. You can see them by running npm list -g --depth 0 npm dependencies and devDependencies When you install an npm package, you are installing it as a dependency.When you install with option --save-dev, you are installing it as a development dependency.development dependencies are intended as development-only packages, that are unneeded in production. So when you run comman npm install in production you should need include flag production such as npm install --production. The npx Node.js Package Runner npm allow you to run that npm command without installing it first. If the command isn’t found, npx will install it into a central cache npx cowsay \"Hello\" Now, this is a funny useless command. Other scenarios include: running the vue CLI tool to create new applications and run them: npx @vue/cli create my-vue-app creating a new React app using create-react-app: npx create-react-app my-react-app Run some code using a different Node.js version npx node@10 -v #v10.18.1npx node@12 -v #v12.14.1 Run arbitrary code snippets directly from a URL npx https://gist.github.com/zkat/4bc19503fe9e9309e2bfaa2c58074d32 Understanding process.nextTick() Every time the event loop takes a full trip, we call it a tick. process.nextTick(() =&gt; {// do something}); The event loop is busy to run the current code. When this operation ends, the JS engine runs all the functions passed to nextTick calls during that operation.It the way we can tell the JS engine to process a function asynchronously, but as soon as possible, not queue it.Calling setTimeout(() =&gt; {}, 0) will execute the function at the end of next tick, much later than when using nextTick() which prioritizes the call and executes it just before the beginning of the next tick. Use nextTick() when you want to make sure that in the next event loop iteration that code is already executed.Understanding setImmediate()Any function passed as the setImmediate() argument is a callback that’s executed in the next iteration of the event loop.How is setImmediate() different from setTimeout(() =&gt; {}, 0) (passing a 0ms timeout), and from process.nextTick() and Promise.then()?A function passed to process.nextTick() is going to be executed on the current iteration of the event loop, after the current operation ends. This means it will always execute before setTimeout and setImmediate.A setTimeout() callback with a 0ms delay is very similar to setImmediate. The execution order will depend on various factors, but they will be both run in the next iteration of the event loop.A process.nextTick callback is added to process.nextTick queue. A Promise.then() callback is added to promise.microtask queue. A setTimeou, setImmediate callback is added to macrotask queueconst baz = () =&gt; console.log('baz');const foo = () =&gt; console.log('foo');const zoo = () =&gt; console.log('zoo');const start = () =&gt; { console.log('start'); setImmediate(baz); new Promise((resolve, reject) =&gt; { resolve('bar'); }).then((resolve) =&gt; { console.log(resolve); process.nextTick(zoo); }); process.nextTick(foo);};start();// start foo bar zoo bazThis code will first call start(), then call foo() in process.nextTick queue. After that, it will handle promises microtask queue, which prints bar and adds zoo() in process.nextTick queue at the same time. Then it will call zoo() which has just been added. In the end, the baz() in macrotask queue is called.The Node.js Event emitterOn the backend side, Node.js offers us the option to build a similar system using event module.This module, inparticular, offers the EventEmitter class, which we’ll use to handle our events.You initializa that usingconst EventEmitter = require('events')const eventEmitter = new EventEmitter();This object exposes, among many others, the on and emit methods. emit is used to trigger an event on is used to add a callback function that’s going to be executed when the event is triggered.For example, let’s create a start event, and as a matter of providing a sample, we react to that by just logging to the console: eventEmitter.on('start', () =&gt; { console.log('started');}); When we run eventEmitter.emit('start'); You can pass argument to the event handler by passing them as additional argument to emit```jseventEmitter.on(‘start’, (start, end) =&gt; {console.log(started from ${start} to ${end});}); eventEmitter.emit(‘start’, 1, 100);The EventEmitter object also exposes several other methods to interact with events, like- once(): add a one-time listener- removeListener() / off(): remove an event listener from an event- removeAllListeners(): remove all listeners for an event.References [docs](https://nodejs.org/api/events.html)## REST API Development with Node.js, Express, and MongoDBTutorial basic to help create a rest api framework with node.js by express and mongodb using JavaScript ES2015. References to [github project](https://github.com/maitraysuthar/rest-api-nodejs-mongodb)### Software Requirements- Node.js 8+- MongoDB 3.6+ (Recommended 4+)### How to install- b1```shgit clone https://github.com/maitraysuthar/rest-api-nodejs-mongodb.git b2 cd rest-api-nodejs-mongodbnpm install b3 Setting up environments You will find a file named .env.example on root directory of project. Create a new file by copying and pasting the file and then renaming it to just .env cp .env.example .env The file .env is already ignored, so you never commit your credentials. Change the values of the file to your environment. Helpful comments added to .env.example file to understand the constants. Project structure .├── app.js├── package.json├── bin│ └── www├── controllers│ ├── AuthController.js│ └── BookController.js├── models│ ├── BookModel.js│ └── UserModel.js├── routes│ ├── api.js│ ├── auth.js│ └── book.js├── middlewares│ ├── jwt.js├── helpers│ ├── apiResponse.js│ ├── constants.js│ ├── mailer.js│ └── utility.js├── test│ ├── testConfig.js│ ├── auth.js│ └── book.js└── public ├── index.html └── stylesheets └── style.css b5: Running API server locally npm run dev Integration Write new model: If you need to add more models to the project just create a new file in /models/ and use them in the controllers. Creating new routes: If you need to add more routes to the project just create a new file in /routes/ and add it in /routes/api.js it will be loaded dynamically. Creating new controllers: If you need to add more controllers to the project just create a new file in /controllers/ and use them in the routes. Running Test Cases Running Eslint npm run lint Prequise npm Express JS Mongo DB Rest API integrate Authentication and CRUDpackage.json example package manage installed {\"name\": \"rest-api-nodejs-mongodb\",\"version\": \"1.0.0\",\"private\": true,\"scripts\": { \"start\": \"node ./bin/www\", \"dev\": \"nodemon ./bin/www\", \"test\": \"nyc _mocha --timeout 10000 --exit --report lcovonly -- -R spec\", \"lint\": \"eslint --fix --config .eslintrc.json \\\"**/*.js\\\"\"},\"dependencies\": { \"bcrypt\": \"^3.0.6\", \"codacy-coverage\": \"^3.4.0\", \"cookie-parser\": \"~1.4.3\", \"cors\": \"^2.8.5\", \"debug\": \"~2.6.9\", \"dotenv\": \"^8.2.0\", \"express\": \"~4.16.0\", \"express-jwt\": \"^5.3.1\", \"express-validator\": \"^6.2.0\", \"jsonwebtoken\": \"^8.5.1\", \"mocha-lcov-reporter\": \"^1.3.0\", \"moment\": \"^2.24.0\", \"mongoose\": \"^5.7.6\", \"morgan\": \"~1.9.0\", \"nodemailer\": \"^6.3.1\"},\"devDependencies\": { \"chai\": \"^4.2.0\", \"chai-http\": \"^4.3.0\", \"eslint\": \"^6.5.1\", \"mocha\": \"^6.2.2\", \"nodemon\": \"^1.19.4\", \"nyc\": \"^14.1.1\"}} Authentication NodeJs package express is popular package help to create server. JWT ```jsconst jwt = require(“express-jwt”);const secret = process.env.JWT_SECRET; const authenticate = jwt({ secret: secret});module.exports = authenticate;The mechanism of authentication when user register or login into server that user will receive a token was generated by server in response to user. To generate token by JWT, nodejs provide a package `express-jwt`.Both `JWT_SECRET` and `JWT_TIMEOUT_DURATION` variable is configure in `.env` file.MONGODB_URL=mongodb://127.0.0.1:27017/rest-api-nodejs-mongodbExample Connection String:-mongodb://127.0.0.1:27017/rest-api-nodejs-mongodbmongodb://[MongodbHost]:[PORT]/[DatabaseName]JWT_SECRET=abcdefghijklmnopqrstuvwxyz1234567890Example Secret:- abcdefghijklmnopqrstuvwxyz1234567890JWT_TIMEOUT_DURATION=”2 hours”You can place duration available here: https://github.com/auth0/node-jsonwebtoken#usageSearch for “expiresIn” option on above link.EMAIL_SMTP_HOST=YourSMTPHostEMAIL_SMTP_PORT=YourSMTPPortEMAIL_SMTP_USERNAME=YourSMTPUsernameEMAIL_SMTP_PASSWORD=YourSMTPPasswordtrue for 465, false for other portsEMAIL_SMTP_SECURE=false#### Routercreate file `auth.js` inside `routes` folder with content:```jsvar express = require(\"express\");const AuthController = require(\"../controllers/AuthController\");var router = express.Router();router.post(\"/register\", AuthController.register);router.post(\"/login\", AuthController.login);router.post(\"/verify-otp\", AuthController.verifyConfirm);router.post(\"/resend-verify-otp\", AuthController.resendConfirmOtp);module.exports = router; Import express module and controller AuthController var express = require(\"express\");const AuthController = require(\"../controllers/AuthController\"); Router apis: register, login, verify-otp, ‘resend-verify-otp’. Such apis was require package nodemailer and variable configure in .env.index servevar express = require(\"express\");var router = express.Router();/* GET home page. */router.get(\"/\", function(req, res) { res.render(\"index\", { title: \"Express\" });});module.exports = router;api routervar express = require(\"express\");var authRouter = require(\"./auth\");var bookRouter = require(\"./book\");var app = express();app.use(\"/auth/\", authRouter);app.use(\"/book/\", bookRouter);module.exports = app;book routervar express = require(\"express\");const BookController = require(\"../controllers/BookController\");var router = express.Router();router.get(\"/\", BookController.bookList);router.get(\"/:id\", BookController.bookDetail);router.post(\"/\", BookController.bookStore);router.put(\"/:id\", BookController.bookUpdate);router.delete(\"/:id\", BookController.bookDelete);module.exports = router;AuthControllerconst UserModel = require(\"../models/UserModel\");const { body,validationResult } = require(\"express-validator\");const { sanitizeBody } = require(\"express-validator\");//helper file to prepare responses.const apiResponse = require(\"../helpers/apiResponse\");const utility = require(\"../helpers/utility\");const bcrypt = require(\"bcrypt\");const jwt = require(\"jsonwebtoken\");const mailer = require(\"../helpers/mailer\");const { constants } = require(\"../helpers/constants\");/** * User registration. * * @param {string} firstName * @param {string} lastName * @param {string} email * @param {string} password * * @returns {Object} */exports.register = [ // Validate fields. body(\"firstName\").isLength({ min: 1 }).trim().withMessage(\"First name must be specified.\") .isAlphanumeric().withMessage(\"First name has non-alphanumeric characters.\"), body(\"lastName\").isLength({ min: 1 }).trim().withMessage(\"Last name must be specified.\") .isAlphanumeric().withMessage(\"Last name has non-alphanumeric characters.\"), body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\").custom((value) =&gt; { return UserModel.findOne({email : value}).then((user) =&gt; { if (user) { return Promise.reject(\"E-mail already in use\"); } }); }), body(\"password\").isLength({ min: 6 }).trim().withMessage(\"Password must be 6 characters or greater.\"), // Sanitize fields. sanitizeBody(\"firstName\").escape(), sanitizeBody(\"lastName\").escape(), sanitizeBody(\"email\").escape(), sanitizeBody(\"password\").escape(), // Process request after validation and sanitization. (req, res) =&gt; { try { // Extract the validation errors from a request. const errors = validationResult(req); if (!errors.isEmpty()) { // Display sanitized values/errors messages. return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { //hash input password bcrypt.hash(req.body.password,10,function(err, hash) { // generate OTP for confirmation let otp = utility.randomNumber(4); // Create User object with escaped and trimmed data var user = new UserModel( { firstName: req.body.firstName, lastName: req.body.lastName, email: req.body.email, password: hash, confirmOTP: otp } ); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email }; return apiResponse.successResponseWithData(res,\"Registration Success.\", userData); }); }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err); }) ; }); } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * User login. * * @param {string} email * @param {string} password * * @returns {Object} */exports.login = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), body(\"password\").isLength({ min: 1 }).trim().withMessage(\"Password must be specified.\"), sanitizeBody(\"email\").escape(), sanitizeBody(\"password\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { UserModel.findOne({email : req.body.email}).then(user =&gt; { if (user) { //Compare given password with db's hash. bcrypt.compare(req.body.password,user.password,function (err,same) { if(same){ //Check account confirmation. if(user.isConfirmed){ // Check User's account active or not. if(user.status) { let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email, }; //Prepare JWT token for authentication const jwtPayload = userData; const jwtData = { expiresIn: process.env.JWT_TIMEOUT_DURATION, }; const secret = process.env.JWT_SECRET; //Generated JWT token with Payload and secret. userData.token = jwt.sign(jwtPayload, secret, jwtData); return apiResponse.successResponseWithData(res,\"Login Success.\", userData); }else { return apiResponse.unauthorizedResponse(res, \"Account is not active. Please contact admin.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account is not confirmed. Please confirm your account.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }];/** * Verify Confirm otp. * * @param {string} email * @param {string} otp * * @returns {Object} */exports.verifyConfirm = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), body(\"otp\").isLength({ min: 1 }).trim().withMessage(\"OTP must be specified.\"), sanitizeBody(\"email\").escape(), sanitizeBody(\"otp\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { var query = {email : req.body.email}; UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ //Check account confirmation. if(user.confirmOTP == req.body.otp){ //Update user as confirmed UserModel.findOneAndUpdate(query, { isConfirmed: 1, confirmOTP: null }).catch(err =&gt; { return apiResponse.ErrorResponse(res, err); }); return apiResponse.successResponse(res,\"Account confirmed success.\"); }else{ return apiResponse.unauthorizedResponse(res, \"Otp does not match\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }];/** * Resend Confirm otp. * * @param {string} email * * @returns {Object} */exports.resendConfirmOtp = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), sanitizeBody(\"email\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { var query = {email : req.body.email}; UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ // Generate otp let otp = utility.randomNumber(4); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ user.isConfirmed = 0; user.confirmOTP = otp; // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } return apiResponse.successResponse(res,\"Confirm otp sent.\"); }); }); }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }]; ExpressJs support MVC architechture, In controller have actions provide authen such as register, login, confirmOTP, resendConfirm Model UserModel store and query data from mongoDB. const UserModel = require(\"../models/UserModel\");const { body,validationResult } = require(\"express-validator\");const { sanitizeBody } = require(\"express-validator\");//helper file to prepare responses.const apiResponse = require(\"../helpers/apiResponse\");const utility = require(\"../helpers/utility\");const bcrypt = require(\"bcrypt\");const jwt = require(\"jsonwebtoken\");const mailer = require(\"../helpers/mailer\");const { constants } = require(\"../helpers/constants\"); Validation package express-validator take the body and validation module body(\"firstName\").isLength({ min: 1 }).trim().withMessage(\"First name must be specified.\") .isAlphanumeric().withMessage(\"First name has non-alphanumeric characters.\"), body(\"lastName\").isLength({ min: 1 }).trim().withMessage(\"Last name must be specified.\") .isAlphanumeric().withMessage(\"Last name has non-alphanumeric characters.\"), body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\").custom((value) =&gt; { return UserModel.findOne({email : value}).then((user) =&gt; { if (user) { return Promise.reject(\"E-mail already in use\"); } }); }), body(\"password\").isLength({ min: 6 }).trim().withMessage(\"Password must be 6 characters or greater.\") Each line is element of array impement by export function. Express will run through each element and store variable in memory and same stack. The validation, The request parameter, The process element, so on. (req, res) =&gt; { try { // Extract the validation errors from a request. const errors = validationResult(req); if (!errors.isEmpty()) { // Display sanitized values/errors messages. return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { //hash input password bcrypt.hash(req.body.password,10,function(err, hash) { // generate OTP for confirmation let otp = utility.randomNumber(4); // Create User object with escaped and trimmed data var user = new UserModel( { firstName: req.body.firstName, lastName: req.body.lastName, email: req.body.email, password: hash, confirmOTP: otp } ); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email }; return apiResponse.successResponseWithData(res,\"Registration Success.\", userData); }); }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err); }) ; }); } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } } code inside is same with JS code. After validation, the action register move to process logic register with user attribute save to DB. A mail send to user when sucessful saved. Try catch for unexpected errors: bcrypt module for hash password and this is a function js in ECMASCRIPT 2015 bcrypt.hash(req.body.password,10,function(err, hash) { }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err);}) ; (req, res) is object request and response and parameters of function register. Response send to user // sucessreturn apiResponse.successResponseWithData(res,\"Registration Success.\", userData);// errorreturn apiResponse.ErrorResponse(res,err); About the api resonpose structure in json format and js code, used jsonwebtoken and express-jwt module for generate token and authentication user request.```jsexports.successResponse = function (res, msg) { var data = { status: 1, message: msg }; return res.status(200).json(data);};exports.successResponseWithData = function (res, msg, data) { var resData = { status: 1, message: msg, data: data }; return res.status(200).json(resData);};exports.ErrorResponse = function (res, msg) { var data = { status: 0, message: msg, }; return res.status(500).json(data);};exports.notFoundResponse = function (res, msg) { var data = { status: 0, message: msg, }; return res.status(404).json(data);};exports.validationErrorWithData = function (res, msg, data) { var resData = { status: 0, message: msg, data: data }; return res.status(400).json(resData);};exports.unauthorizedResponse = function (res, msg) { var data = { status: 0, message: msg, }; return res.status(401).json(data);};- Register function is trigger a procedure send email to user register with otp in a email, after email register and confirm otp send a user register save to db and api response register api send to user```jsmailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email }; return apiResponse.successResponseWithData(res,\"Registration Success.\", userData); }); }).catch(err =&gt; { console.log(err); return apiResponse.ErrorResponse(res,err); }) ;Login methodJs codeexports.login = [ body(\"email\").isLength({ min: 1 }).trim().withMessage(\"Email must be specified.\") .isEmail().withMessage(\"Email must be a valid email address.\"), body(\"password\").isLength({ min: 1 }).trim().withMessage(\"Password must be specified.\"), sanitizeBody(\"email\").escape(), sanitizeBody(\"password\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); }else { UserModel.findOne({email : req.body.email}).then(user =&gt; { if (user) { //Compare given password with db's hash. bcrypt.compare(req.body.password,user.password,function (err,same) { if(same){ //Check account confirmation. if(user.isConfirmed){ // Check User's account active or not. if(user.status) { let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email, }; //Prepare JWT token for authentication const jwtPayload = userData; const jwtData = { expiresIn: process.env.JWT_TIMEOUT_DURATION, }; const secret = process.env.JWT_SECRET; //Generated JWT token with Payload and secret. userData.token = jwt.sign(jwtPayload, secret, jwtData); return apiResponse.successResponseWithData(res,\"Login Success.\", userData); }else { return apiResponse.unauthorizedResponse(res, \"Account is not active. Please contact admin.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account is not confirmed. Please confirm your account.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); }else{ return apiResponse.unauthorizedResponse(res, \"Email or Password wrong.\"); } }); } } catch (err) { return apiResponse.ErrorResponse(res, err); } }]; bcrypt module provide function compare password with user.password. Promise function search in DB and find user exist with email. UserModel.findOne({email : req.body.email}).then(user =&gt; {...});bcrypt.compare(req.body.password,user.password,function (err,same) {}); Check User status and send response with JWT token generate for user: userData.token = jwt.sign(jwtPayload, secret, jwtData); in expired time. if(user.status) { let userData = { _id: user._id, firstName: user.firstName, lastName: user.lastName, email: user.email, }; //Prepare JWT token for authentication const jwtPayload = userData; const jwtData = { expiresIn: process.env.JWT_TIMEOUT_DURATION, }; const secret = process.env.JWT_SECRET; //Generated JWT token with Payload and secret. userData.token = jwt.sign(jwtPayload, secret, jwtData); return apiResponse.successResponseWithData(res,\"Login Success.\", userData); }else { return apiResponse.unauthorizedResponse(res, \"Account is not active. Please contact admin.\"); } Verify OTP verify otp from user with database. var query = {email : req.body.email};UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ //Check account confirmation. if(user.confirmOTP == req.body.otp){ //Update user as confirmed UserModel.findOneAndUpdate(query, { isConfirmed: 1, confirmOTP: null }).catch(err =&gt; { return apiResponse.ErrorResponse(res, err); }); return apiResponse.successResponse(res,\"Account confirmed success.\"); }else{ return apiResponse.unauthorizedResponse(res, \"Otp does not match\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); }}); Resend OTP query to get email searching email on db and resend otp to user request resend otp. var query = {email : req.body.email};UserModel.findOne(query).then(user =&gt; { if (user) { //Check already confirm or not. if(!user.isConfirmed){ // Generate otp let otp = utility.randomNumber(4); // Html email body let html = \"&lt;p&gt;Please Confirm your Account.&lt;/p&gt;&lt;p&gt;OTP: \"+otp+\"&lt;/p&gt;\"; // Send confirmation email mailer.send( constants.confirmEmails.from, req.body.email, \"Confirm Account\", html ).then(function(){ user.isConfirmed = 0; user.confirmOTP = otp; // Save user. user.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } return apiResponse.successResponse(res,\"Confirm otp sent.\"); }); }); }else{ return apiResponse.unauthorizedResponse(res, \"Account already confirmed.\"); } }else{ return apiResponse.unauthorizedResponse(res, \"Specified email not found.\"); }}); Promise function in request often used in Express to asynchronous request nodejs. Mailer function Mailer to send email is support by nodemailer module. send function was eported```jsconst nodemailer = require(“nodemailer”); // create reusable transporter object using the default SMTP transportlet transporter = nodemailer.createTransport({ host: process.env.EMAIL_SMTP_HOST, port: process.env.EMAIL_SMTP_PORT, //secure: process.env.EMAIL_SMTP_SECURE, // lack of ssl commented this. You can uncomment it. auth: { user: process.env.EMAIL_SMTP_USERNAME, pass: process.env.EMAIL_SMTP_PASSWORD }});exports.send = function (from, to, subject, html){ // send mail with defined transport object // visit https://nodemailer.com/ for more options return transporter.sendMail({ from: from, // sender address e.g. no-reply@xyz.com or “Fred Foo 👻” foo@example.com to: to, // list of receivers e.g. bar@example.com, baz@example.com subject: subject, // Subject line e.g. ‘Hello ✔’ //text: text, // plain text body e.g. Hello world? html: html // html body e.g. ‘Hello world?’ });};### Utily to generate OTPGenerate otp for confirm user request```jsexports.randomNumber = function (length) { var text = \"\"; var possible = \"123456789\"; for (var i = 0; i &lt; length; i++) { var sup = Math.floor(Math.random() * possible.length); text += i &gt; 0 &amp;&amp; sup == i ? \"0\" : possible.charAt(sup); } return Number(text);};Constants dataConfirmEmails from, admin.exports.constants = { admin: { name: \"admin\", email: \"admin@admin.com\" }, confirmEmails: { from : \"no-reply@test-app.com\" }};User modelUser model to query data from monggodbvar mongoose = require(\"mongoose\");var UserSchema = new mongoose.Schema({ firstName: {type: String, required: true}, lastName: {type: String, required: true}, email: {type: String, required: true}, password: {type: String, required: true}, isConfirmed: {type: Boolean, required: true, default: 0}, confirmOTP: {type: String, required:false}, otpTries: {type: Number, required:false, default: 0}, status: {type: Boolean, required: true, default: 1}}, {timestamps: true});// Virtual for user's full nameUserSchema .virtual(\"fullName\") .get(function () { return this.firstName + \" \" + this.lastName; });module.exports = mongoose.model(\"User\", UserSchema);HTTP server handle request and response#!/usr/bin/env node/** * Module dependencies. */var app = require('../app');var debug = require('debug')('rest-api-nodejs-mongodb:server');var http = require('http');/** * Get port from environment and store in Express. */var port = normalizePort(process.env.PORT || '3000');app.set('port', port);/** * Create HTTP server. */var server = http.createServer(app);/** * Listen on provided port, on all network interfaces. */server.listen(port);server.on('error', onError);server.on('listening', onListening);/** * Normalize a port into a number, string, or false. */function normalizePort(val) { var port = parseInt(val, 10); if (isNaN(port)) { // named pipe return val; } if (port &gt;= 0) { // port number return port; } return false;}/** * Event listener for HTTP server \"error\" event. */function onError(error) { if (error.syscall !== 'listen') { throw error; } var bind = typeof port === 'string' ? 'Pipe ' + port : 'Port ' + port; // handle specific listen errors with friendly messages switch (error.code) { case 'EACCES': console.error(bind + ' requires elevated privileges'); process.exit(1); break; case 'EADDRINUSE': console.error(bind + ' is already in use'); process.exit(1); break; default: throw error; }}/** * Event listener for HTTP server \"listening\" event. */function onListening() { var addr = server.address(); var bind = typeof addr === 'string' ? 'pipe ' + addr : 'port ' + addr.port; debug('Listening on ' + bind);}Book rest api handleBookcontroller will handle function as actions: book list, book detail, book store, book update, and book delete.Book controllerconst Book = require(\"../models/BookModel\");const { body,validationResult } = require(\"express-validator\");const { sanitizeBody } = require(\"express-validator\");const apiResponse = require(\"../helpers/apiResponse\");const auth = require(\"../middlewares/jwt\");var mongoose = require(\"mongoose\");mongoose.set(\"useFindAndModify\", false);// Book Schemafunction BookData(data) { this.id = data._id; this.title= data.title; this.description = data.description; this.isbn = data.isbn; this.createdAt = data.createdAt;}/** * Book List. * * @returns {Object} */exports.bookList = [ auth, function (req, res) { try { Book.find({user: req.user._id},\"_id title description isbn createdAt\").then((books)=&gt;{ if(books.length &gt; 0){ return apiResponse.successResponseWithData(res, \"Operation success\", books); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", []); } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book Detail. * * @param {string} id * * @returns {Object} */exports.bookDetail = [ auth, function (req, res) { if(!mongoose.Types.ObjectId.isValid(req.params.id)){ return apiResponse.successResponseWithData(res, \"Operation success\", {}); } try { Book.findOne({_id: req.params.id,user: req.user._id},\"_id title description isbn createdAt\").then((book)=&gt;{ if(book !== null){ let bookData = new BookData(book); return apiResponse.successResponseWithData(res, \"Operation success\", bookData); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", {}); } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book store. * * @param {string} title * @param {string} description * @param {string} isbn * * @returns {Object} */exports.bookStore = [ auth, body(\"title\", \"Title must not be empty.\").isLength({ min: 1 }).trim(), body(\"description\", \"Description must not be empty.\").isLength({ min: 1 }).trim(), body(\"isbn\", \"ISBN must not be empty\").isLength({ min: 1 }).trim().custom((value,{req}) =&gt; { return Book.findOne({isbn : value,user: req.user._id}).then(book =&gt; { if (book) { return Promise.reject(\"Book already exist with this ISBN no.\"); } }); }), sanitizeBody(\"*\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); var book = new Book( { title: req.body.title, user: req.user, description: req.body.description, isbn: req.body.isbn }); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); } else { //Save book. book.save(function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } let bookData = new BookData(book); return apiResponse.successResponseWithData(res,\"Book add Success.\", bookData); }); } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book update. * * @param {string} title * @param {string} description * @param {string} isbn * * @returns {Object} */exports.bookUpdate = [ auth, body(\"title\", \"Title must not be empty.\").isLength({ min: 1 }).trim(), body(\"description\", \"Description must not be empty.\").isLength({ min: 1 }).trim(), body(\"isbn\", \"ISBN must not be empty\").isLength({ min: 1 }).trim().custom((value,{req}) =&gt; { return Book.findOne({isbn : value,user: req.user._id, _id: { \"$ne\": req.params.id }}).then(book =&gt; { if (book) { return Promise.reject(\"Book already exist with this ISBN no.\"); } }); }), sanitizeBody(\"*\").escape(), (req, res) =&gt; { try { const errors = validationResult(req); var book = new Book( { title: req.body.title, description: req.body.description, isbn: req.body.isbn, _id:req.params.id }); if (!errors.isEmpty()) { return apiResponse.validationErrorWithData(res, \"Validation Error.\", errors.array()); } else { if(!mongoose.Types.ObjectId.isValid(req.params.id)){ return apiResponse.validationErrorWithData(res, \"Invalid Error.\", \"Invalid ID\"); }else{ Book.findById(req.params.id, function (err, foundBook) { if (foundBook === null){ return apiResponse.notFoundResponse(res,\"Book not exists with this id\"); }else{ //Check authorized user if(foundBook.user.toString() !== req.user._id){ return apiResponse.unauthorizedResponse(res, \"You are not authorized to do this operation.\"); }else{ //update book. Book.findByIdAndUpdate(req.params.id, book, {},function (err) { if (err) { return apiResponse.ErrorResponse(res, err); } else { let bookData = new BookData(book); return apiResponse.successResponseWithData(res,\"Book update Success.\", bookData); } }); } } }); } } } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];/** * Book Delete. * * @param {string} id * * @returns {Object} */exports.bookDelete = [ auth, function (req, res) { if(!mongoose.Types.ObjectId.isValid(req.params.id)){ return apiResponse.validationErrorWithData(res, \"Invalid Error.\", \"Invalid ID\"); } try { Book.findById(req.params.id, function (err, foundBook) { if(foundBook === null){ return apiResponse.notFoundResponse(res,\"Book not exists with this id\"); }else{ //Check authorized user if(foundBook.user.toString() !== req.user._id){ return apiResponse.unauthorizedResponse(res, \"You are not authorized to do this operation.\"); }else{ //delete book. Book.findByIdAndRemove(req.params.id,function (err) { if (err) { return apiResponse.ErrorResponse(res, err); }else{ return apiResponse.successResponse(res,\"Book delete Success.\"); } }); } } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];Mongodb will manage by mongoose module.set mode useFindAndModify. Create schema// Book Schemafunction BookData(data) { this.id = data._id; this.title= data.title; this.description = data.description; this.isbn = data.isbn; this.createdAt = data.createdAt;}Book controller require authenticate from middleware jwt and found user of request. Implement auth such as a element and export in controller action.Book listexports.bookList = [ auth, function (req, res) { try { Book.find({user: req.user._id},\"_id title description isbn createdAt\").then((books)=&gt;{ if(books.length &gt; 0){ return apiResponse.successResponseWithData(res, \"Operation success\", books); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", []); } }); } catch (err) { //throw error in json response with status 500. return apiResponse.ErrorResponse(res, err); } }];bookDetailBook.findOne({_id: req.params.id,user: req.user._id},\"_id title description isbn createdAt\").then((book)=&gt;{ if(book !== null){ let bookData = new BookData(book); return apiResponse.successResponseWithData(res, \"Operation success\", bookData); }else{ return apiResponse.successResponseWithData(res, \"Operation success\", {}); }});Used promise Book.findOne({_id: req.params.id,user: req.user._id},\"_id title description isbn createdAt\").then((book)=&gt;{});. id of book is param of request, and user is param of request after authenticated.bookUpdate params santizied input by function sanitizeBody(\"*\").escape() Use mongoose.Types.ObjectId.isValid(req.params.id) to validate ID in request params.bookDelete Promise for find and delete book Book.findByIdAndRemove(req.params.id,function (err) { if (err) { return apiResponse.ErrorResponse(res, err); }else{ return apiResponse.successResponse(res,\"Book delete Success.\"); }}); Book Model ```jsvar mongoose = require(“mongoose”); var Schema = mongoose.Schema;var BookSchema = new Schema({ title: {type: String, required: true}, description: {type: String, required: true}, isbn: {type: String, required: true}, user: { type: Schema.ObjectId, ref: “User”, required: true },}, {timestamps: true});module.exports = mongoose.model(“Book”, BookSchema);### Test- testConfig.js```js//During the automated test the env variable, We will set it to \"test\"process.env.NODE_ENV = \"test\";process.env.MONGODB_URL = \"mongodb://127.0.0.1:27017/rest-api-nodejs-mongodb-test\";//Require the dev-dependencieslet chai = require(\"chai\");let chaiHttp = require(\"chai-http\");let server = require(\"../app\");let should = chai.should();chai.use(chaiHttp);//Export this to use in multiple filesmodule.exports = { chai: chai, server: server, should: should};Import necessary module for test. Add these lines before test code.const { chai, server, should } = require(\"./testConfig\");const UserModel = require(\"../models/UserModel\");const BookModel = require(\"../models/BookModel\"); auth.js describe(\"Auth\", () =&gt; { // Before each test we empty the database before((done) =&gt; { UserModel.deleteMany({}, (err) =&gt; { done(); }); }); // Prepare data for testing const testData = { \"firstName\":\"test\", \"lastName\":\"testing\", \"password\":\"Test@123\", \"email\":\"maitraysuthar@test12345.com\" }; /** Test the /POST route*/ describe(\"/POST Register\", () =&gt; { it(\"It should send validation error for Register\", (done) =&gt; { chai.request(server) .post(\"/api/auth/register\") .send({\"email\": testData.email}) .end((err, res) =&gt; { res.should.have.status(400); done(); }); }); });}); book.jsdescribe(\"Book\", () =&gt; { //Before each test we empty the database before((done) =&gt; { BookModel.deleteMany({}, (err) =&gt; { done(); }); }); // Prepare data for testing const userTestData = { \"password\":\"Test@123\", \"email\":\"maitraysuthar@test12345.com\" }; // Prepare data for testing const testData = { \"title\":\"testing book\", \"description\":\"testing book desc\", \"isbn\":\"3214htrff4\" }; /* * Test the /POST route */ describe(\"/POST Login\", () =&gt; { it(\"it should do user Login for book\", (done) =&gt; { chai.request(server) .post(\"/api/auth/login\") .send({\"email\": userTestData.email,\"password\": userTestData.password}) .end((err, res) =&gt; { res.should.have.status(200); res.body.should.have.property(\"message\").eql(\"Login Success.\"); userTestData.token = res.body.data.token; done(); }); }); });}); app.jsFile to configure app and config variables```JSvar express = require(“express”);var path = require(“path”);var cookieParser = require(“cookie-parser”);var logger = require(“morgan”);require(“dotenv”).config();var indexRouter = require(“./routes/index”);var apiRouter = require(“./routes/api”);var apiResponse = require(“./helpers/apiResponse”);var cors = require(“cors”);// DB connectionvar MONGODB_URL = process.env.MONGODB_URL;var mongoose = require(“mongoose”);mongoose.connect(MONGODB_URL, { useNewUrlParser: true, useUnifiedTopology: true }).then(() =&gt; { //don’t show the log when it is test if(process.env.NODE_ENV !== “test”) { console.log(“Connected to %s”, MONGODB_URL); console.log(“App is running … \\n”); console.log(“Press CTRL + C to stop the process. \\n”); }}) .catch(err =&gt; { console.error(“App starting error:”, err.message); process.exit(1); });var db = mongoose.connection;var app = express();//don’t show the log when it is testif(process.env.NODE_ENV !== “test”) { app.use(logger(“dev”));}app.use(express.json());app.use(express.urlencoded({ extended: false }));app.use(cookieParser());app.use(express.static(path.join(__dirname, “public”)));//To allow cross-origin requestsapp.use(cors());//Route Prefixesapp.use(“/”, indexRouter);app.use(“/api/”, apiRouter);// throw 404 if URL not foundapp.all(“*”, function(req, res) { return apiResponse.notFoundResponse(res, “Page not found”);});app.use((err, req, res) =&gt; { if(err.name == “UnauthorizedError”){ return apiResponse.unauthorizedResponse(res, err.message); }});module.exports = app;- DB configure from monggo DB```js// DB connectionvar MONGODB_URL = process.env.MONGODB_URL;var mongoose = require(\"mongoose\");mongoose.connect(MONGODB_URL, { useNewUrlParser: true, useUnifiedTopology: true }).then(() =&gt; { //don't show the log when it is test if(process.env.NODE_ENV !== \"test\") { console.log(\"Connected to %s\", MONGODB_URL); console.log(\"App is running ... \\n\"); console.log(\"Press CTRL + C to stop the process. \\n\"); }}) .catch(err =&gt; { console.error(\"App starting error:\", err.message); process.exit(1); });var db = mongoose.connection;Build an HTTP Serverconst http = require('http')const port = process.env.PORT || 3000const server = http.createServer((req, res) =&gt; { res.statusCode = 200 res.setHeader('Content-Type', 'text/html') res.end('&lt;h1&gt;Hello, World!&lt;/h1&gt;')})server.listen(port, () =&gt; { console.log(`Server running at port ${port}`)})Let’s analyze it briefly. We include the http module. We use the module to create an HTTP server. Default port is 3000, the listen callback function is called.Whenever a new request is received, the request event is called, providing two object: a request(an http.IncomingMessage object) and a response (an http.ServerResponse object).res.statusCode = 200;res.setHeader('Content-Type', 'text/html');res.end('&lt;h1&gt;Hello, World!&lt;/h1&gt;');Making HTTP requests with Node.jsGET requestconst axios = require('axios');axios .get('https://example.com/todos') .then(res =&gt; { console.log(`statusCode: ${res.status}`); console.log(res); }) .catch(error =&gt; { console.error(error); });const https = require('https');const options = { hostname: 'example.com', port: 443, path: '/todos', method: 'GET',};const req = https.request(options, res =&gt; { console.log(`statusCode: ${res.statusCode}`); res.on('data', d =&gt; { process.stdout.write(d); });});req.on('error', error =&gt; { console.error(error);});req.end();Post requestSimiliar to making GET request, you can use Axios library to perform POST requestconst axios = require('axios');axios .post('https://whatever.com/todos', { todo: 'Buy the milk', }) .then(res =&gt; { console.log(`statusCode: ${res.status}`); console.log(res); }) .catch(error =&gt; { console.error(error); });Or alternative using https module:const https = require('https');const data = JSON.stringify({ todo: 'Buy the milk',});const options = { hostname: 'whatever.com', port: 443, path: '/todos', method: 'POST', headers: { 'Content-Type': 'application/json', 'Content-Length': data.length, },};const req = https.request(options, res =&gt; { console.log(`statusCode: ${res.statusCode}`); res.on('data', d =&gt; { process.stdout.write(d); });});req.on('error', error =&gt; { console.error(error);});req.write(data);req.end();PUT and DELETEPUT and DELETE is same with POST and GET method but method is PUT or DELETE.Get HTTP request body data using Node.jsIf you are using Express, that’s quite simple: use the express.json() middleware which is available is Express v4.16.0 onwards.const axios = require('axios');axios.post('https://whatever.com/todos', { todo: 'Buy the milk',});This is the matching server side codeconst express = require('express');const app = express();app.use( express.urlencoded({ extended: true, }));app.use(express.json());app.post('/todos', (req, res) =&gt; { console.log(req.body.todo);});If you’re not using Express and you want to do this in vanilla Node.js, you need to do a bit more work, of course, as Express abstracts a lot of this for you.const server = http.createServer((req, res) =&gt; { // we can access HTTP headers req.on('data', chunk =&gt; { console.log(`Data chunk available: ${chunk}`); }); req.on('end', () =&gt; { // end of data });});So to access the data, assuming we expect to receive a string, we must concatenate the into a string when listening to the stream data, amd when the stream end, we parse the string to JSON:const server = http.createServer(async (req, res) =&gt; { const buffers = []; for await (const chunk of req) { buffers.push(chunk); } const data = Buffer.concat(buffers).toString(); console.log(JSON.parse(data).todo); // 'Buy the milk' res.end();});Node.js file statsYou call it passing a file path, and once Node.js gets the file details it will call the callback function you pass, with 2 parameters: an error message, and the file stats:const fs = require('fs');fs.stat('/Users/joe/test.text', (err, stats) =&gt; { if (err) { console.error(err); return; } stats.isFile(); stats.isDirectory(); stats.isSymbolicLink(); stats.size;});You can use promise-based fsPromises.stat() method offerd by the fs/promises module if you like:const fs = require('fs/promises');async function example() { try { stats.isFile(); // true stats.isDirectory(); // false stats.isSymbolicLink(); // false stats.size; // 1024000 //= 1MB } catch (err) { console.log(err); }}example();Node.js File PathsYou include this module in your files usingconst path = require('path');const notes = '/users/joe/notes.txt';path.dirname(notes); // /users/joepath.basename(notes); // notes.txtpath.extname(notes); // .txtpath.basename(notes, path.extname(notes)); // notesWorking with pathsconst name = 'joe';path.join('/', 'users', name, 'notes.txt'); // '/users/joe/notes.txt'path.resolve('joe.txt'); // '/Users/joe/joe.txt' if run from my home folderpath.normalize('/users/joe/..//test.txt'); // '/users/test.txt'Neither resolve nor normalize will check if the path exists. They just calculate a path based on the information they got.Check file exists by:const stats = await fs.stat('/Users/joe/test.txt');stats.isFile(); // truestats.isDirectory(); // falsestats.isSymbolicLink(); // falseReading files with Node.jsAll three of fs.readFile(), fs.readFileSync() and fsPromises.readFile() read the full content of the file in memory before returning the data.This means that big files are going to have a major impact on your memory consumption and speed of execution of the program.In this case, a better option is to read the file content using streams. fs.readFile```JSconst fs = require(‘fs’);fs.readFile(‘/Users/joe/test.txt’, ‘utf8’, (err, data) =&gt; { if (err) { console.error(err); return; } console.log(data);});- fs.readFileSync()```JSconst fs = require('fs');try { const data = fs.readFileSync('/Users/joe/test.txt', 'utf8'); console.log(data);} catch (err) { console.error(err);} fsPromises.readFile() and fs/promises module```JSconst fs = require(‘fs/promises’);async function example() { try { const data = await fs.readFile(‘/Users/joe/test.txt’, { encoding: ‘utf8’ }); console.log(data); } catch (err) { console.log(err); }}example();## Writing files with Node.jsThe easiest way to write to files in Node.js is to use the `fs.writeFile()` API.- fs.writeFile```JSconst fs = require('fs');const content = 'Some content!';fs.writeFile('/Users/joe/test.txt', content, err =&gt; { if (err) { console.error(err); } // file written successfully}); fs.writeFileSync()```JSconst fs = require(‘fs’);const content = ‘Some content!’;try { fs.writeFileSync(‘/Users/joe/test.txt’, content); // file written successfully} catch (err) { console.error(err);}- fsPromises.writeFile() method offered by the `fs/promises` module:```JSconst fs = require('fs/promises');async function example() { try { const content = 'Some content!'; await fs.writeFile('/Users/joe/test.txt', content); } catch (err) { console.log(err); }}example();// for modify flagfs.writeFile('/Users/joe/test.txt', content, { flag: 'a+' }, err =&gt; {});The flags you’ll likely use are r+ open the file for reading and writing w+ open the file for reading and writing, positioning the stream at the beginning of the file. The file is created if it does not exist a open the file for writing, positioning the stream at the end of the file. The file is created if it does not exist a+ open the file for reading and writing, positioning the stream at the end of the file. The file is created if it does not exist Append to a file fs.appendFile()```JSconst content = ‘Some content!’;fs.appendFile(‘file.log’, content, err =&gt; { if (err) { console.error(err); } // done!});- fsPromises.appendFile()```JSconst fs = require('fs/promises');async function example() { try { const content = 'Some content!'; await fs.appendFile('/Users/joe/test.txt', content); } catch (err) { console.log(err); }}example();Using streamsAll those methods write the full content to the file before returning the control back to your program (in the async version, this means executing the callback)In this case, a better option is to write the file content using streams.Working with folders in Node.jsThe Node.js fs core module provides many handy methods you can use to work with folders.Check if a folder existsUse fs.access() (and its promise-based fsPromises.access() counterpart) to check if the folder exists and Node.js can access it with its permissions.const fs = require('fs');const folderName = '/Users/joe/test';try { if (!fs.existsSync(folderName)) { fs.mkdirSync(folderName); }} catch (err) { console.error(err);}Read the content of a directoryconst fs = require('fs');const folderPath = '/Users/joe';fs.readdirSync(folderPath);fs.readdirSync(folderPath).map(fileName =&gt; { return path.join(folderPath, fileName);});const isFile = fileName =&gt; { return fs.lstatSync(fileName).isFile();};fs.readdirSync(folderPath) .map(fileName =&gt; { return path.join(folderPath, fileName); }) .filter(isFile);Rename a folderfs.rename() or fs.renameSync() or fsPromises.rename(). The first parameter is the current path, the second the new path:const fs = require('fs');fs.rename('/Users/joe', '/Users/roger', err =&gt; { if (err) { console.error(err); } // done});fs.renameSync() is synchromous verionconst fs = require('fs');try { fs.renameSync('/Users/joe', '/Users/roger');} catch (err) { console.error(err);} fsPromises.rename() is the promise-based version:```JSconst fs = require(‘fs/promises’);async function example() { try { await fs.rename(‘/Users/joe’, ‘/Users/roger’); } catch (err) { console.log(err); }}example();### Remove a folderUse `fs.rmdir()` or `fs.rmdirSync()` or `fsPromises.rmdir()` to remove a folder.```JSconst fs = require('fs');fs.rm(dir, { recursive: true, force: true }, err =&gt; { if (err) { throw err; } console.log(`${dir} is deleted!`);}); You can install and make use of the fs-extra module. It’s a drop-in replacement of the fs module, which provides more features on top of it. npm install fs-extra ```jsconst fs = require(‘fs-extra’); const folder = ‘/Users/joe’;fs.remove(folder, err =&gt; { console.error(err);});// promisefs.remove(folder) .then(() =&gt; { // done }) .catch(err =&gt; { console.error(err); });// async/awaitasync function removeFolder(folder) { try { await fs.remove(folder); // done } catch (err) { console.error(err); }}const folder = ‘/Users/joe’;removeFolder(folder);## The Node.js fs moduleThis is core module. When you require module. You can access to all its method, wich include:- fs.access(): check if the file exists and Node.js can access it with its permissions- fs.appendFile(): append data to a file. If the file does not exist, it's created- fs.chmod(): change the permissions of a file specified by the filename passed. Related: fs.lchmod(), fs.fchmod()- fs.chown(): change the owner and group of a file specified by the filename passed. Related: fs.fchown(), fs.lchown()- fs.close(): close a file descriptor- fs.copyFile(): copies a file- fs.createReadStream(): create a readable file stream- fs.createWriteStream(): create a writable file stream- fs.link(): create a new hard link to a file- fs.mkdir(): create a new folder- fs.mkdtemp(): create a temporary directory- fs.open(): opens the file and returns a file descriptor to allow file manipulation- fs.readdir(): read the contents of a directory- fs.readFile(): read the content of a file. Related: fs.read()- fs.readlink(): read the value of a symbolic link- fs.realpath(): resolve relative file path pointers (., ..) to the full path- fs.rename(): rename a file or folder- fs.rmdir(): remove a folder- fs.stat(): returns the status of the file identified by the filename passed. Related: fs.fstat(), fs.lstat()- fs.symlink(): create a new symbolic link to a file- fs.truncate(): truncate to the specified length the file identified by the filename passed. Related: fs.ftruncate()- fs.unlink(): remove a file or a symbolic link- fs.unwatchFile(): stop watching for changes on a file- fs.utimes(): change the timestamp of the file identified by the filename passed. Related: fs.futimes()- fs.watchFile(): start watching for changes on a file. Related: fs.watch()- fs.writeFile(): write data to a file. Related: fs.write()One peculiar thing about the `fs` module is that all the methods are asynchronous by default, but they can also work synchronously by appending `Sync` such as `fs.renameSync`, `fs.writeSync`.```JS// Example: Read a file and change its content and read// it again using callback-based API.const fs = require('fs');const fileName = '/Users/joe/test.txt';fs.readFile(fileName, 'utf8', (err, data) =&gt; { if (err) { console.log(err); return; } console.log(data); const content = 'Some content!'; fs.writeFile(fileName, content, err2 =&gt; { if (err2) { console.log(err2); return; } console.log('Wrote some content!'); fs.readFile(fileName, 'utf8', (err3, data3) =&gt; { if (err3) { console.log(err3); return; } console.log(data3); }); });}); for antoher example```JS// Example: Read a file and change its content and read// it again using callback-based API.const fs = require(‘fs’);const fileName = ‘/Users/joe/test.txt’;fs.readFile(fileName, ‘utf8’, (err, data) =&gt; { if (err) { console.log(err); return; } console.log(data); const content = ‘Some content!’; fs.writeFile(fileName, content, err2 =&gt; { if (err2) { console.log(err2); return; } console.log(‘Wrote some content!’); fs.readFile(fileName, ‘utf8’, (err3, data3) =&gt; { if (err3) { console.log(err3); return; } console.log(data3); }); });});// Example: Read a file and change its content and read// it again using promise-based API.const fs = require(‘fs/promises’);async function example() { const fileName = ‘/Users/joe/test.txt’; try { const data = await fs.readFile(fileName, ‘utf8’); console.log(data); const content = ‘Some content!’; await fs.writeFile(fileName, content); console.log(‘Wrote some content!’); const newData = await fs.readFile(fileName, ‘utf8’); console.log(newData); } catch (err) { console.log(err); }}example();### The Node.js os moduleThis module provides many functions that you can use to retrieve information from the underlying operating system and the computer the program runs on, and interact with it.```jsconst os = require('os'); os.EOL gives the line delimiter sequence. It’s \\n on Linux and macOS, and \\r\\n on Windows. os.constants.signals tells us all the constants related to handling process signals, like SIGHUP, SIGKILL and so on. os.constants.errno sets the constants for error reporting, like EADDRINUSE, EOVERFLOW and more. os.arch() os.cpus() os.endianness() os.freemen() os.homedir() os.hostname() os.loadavg() os.networkInterfaces() os.platform() os.release() os.tmpdir() os.totalmem() os.type() os.uptime() os.userInfo() The Node.js events module The event module provides us the EventEmitter class, which is key to working with events in Node.js const EventEmitter = require('events');const door = new EventEmitter(); The event listener has these in-built events: newListener when a listener is added removeListener when a listener is removedemitter.addListener()Alias for emitter.on().emitter.emit()Emits an event. It synchronously calls every listener in the order they were registered. door.emit('slam'); // emitting the event \"slam\" emitter.eventNames() door.eventNames(); emitter.getMaxListeners()Get the maximum amount of listeners one can add to an EventEmitter object, which defaults to 10 but can be increased or lowered by using setMaxListeners() door.getMaxListeners(); emitter.listenerCount() door.listenerCount('open'); emitter.listeners() door.listeners('open'); emitter.off()Alias for emitter.removeListener() added in Node.js 10emitter.on()Adds a callback function that’s called when an event is emitted. door.on('open', () =&gt; {console.log('Door was opened');}); emitter.once()Adds a callback function that’s called when an event is emitted for the first time after registering this. This callback is only going to be called once, never again.```JSconst EventEmitter = require(‘events’); const ee = new EventEmitter();ee.once(‘my-event’, () =&gt; { // call callback function once});**emitter.prependListener()**When you add a listender using `on` or `addListener`, It's added last in the queue of listenders, and called last. Using `preendListener`it's added, added, and called, before other listeners.**emitter.prependOnceListener()**When you add a listener using `once`, it's added last in the queue of listeners, and called last. Using `preendOnceListener` it's added, and called, before other listeners.**emitter.removeAllListeners()**Removes all listeners of an EventEmitter object listening to a specific event:```jsdoor.removeAllListeners('open');emitter.removeListener()Remove a specific listener. You can do this by saving the callback function to a variable, when added, so you can reference it later:const doSomething = () =&gt; {};door.on('open', doSomething);door.removeListener('open', doSomething);emitter.setMaxListeners()Sets the maximum amount of listeners one can add to an EventEmitter object, which defaults to 10 but can be increased or lowered.door.setMaxListeners(50);The Node.js http moduleThe HTTP core module is a key module to Node.js networking.It can be included usingconst http = require('http');Propertieshttp.METHODS&gt; require('http').METHODS[ 'ACL', 'BIND', 'CHECKOUT', 'CONNECT', 'COPY', 'DELETE', 'GET', 'HEAD', 'LINK', 'LOCK', 'M-SEARCH', 'MERGE', 'MKACTIVITY', 'MKCALENDAR', 'MKCOL', 'MOVE', 'NOTIFY', 'OPTIONS', 'PATCH', 'POST', 'PROPFIND', 'PROPPATCH', 'PURGE', 'PUT', 'REBIND', 'REPORT', 'SEARCH', 'SUBSCRIBE', 'TRACE', 'UNBIND', 'UNLINK', 'UNLOCK', 'UNSUBSCRIBE' ]http.STATUS_CODESThis property lists all the HTTP status codes and their description:&gt; require('http').STATUS_CODES{ '100': 'Continue', '101': 'Switching Protocols', '102': 'Processing', '200': 'OK', '201': 'Created', '202': 'Accepted', '203': 'Non-Authoritative Information', '204': 'No Content', '205': 'Reset Content', '206': 'Partial Content', '207': 'Multi-Status', '208': 'Already Reported', '226': 'IM Used', '300': 'Multiple Choices', '301': 'Moved Permanently', '302': 'Found', '303': 'See Other', '304': 'Not Modified', '305': 'Use Proxy', '307': 'Temporary Redirect', '308': 'Permanent Redirect', '400': 'Bad Request', '401': 'Unauthorized', '402': 'Payment Required', '403': 'Forbidden', '404': 'Not Found', '405': 'Method Not Allowed', '406': 'Not Acceptable', '407': 'Proxy Authentication Required', '408': 'Request Timeout', '409': 'Conflict', '410': 'Gone', '411': 'Length Required', '412': 'Precondition Failed', '413': 'Payload Too Large', '414': 'URI Too Long', '415': 'Unsupported Media Type', '416': 'Range Not Satisfiable', '417': 'Expectation Failed', '418': \"I'm a teapot\", '421': 'Misdirected Request', '422': 'Unprocessable Entity', '423': 'Locked', '424': 'Failed Dependency', '425': 'Unordered Collection', '426': 'Upgrade Required', '428': 'Precondition Required', '429': 'Too Many Requests', '431': 'Request Header Fields Too Large', '451': 'Unavailable For Legal Reasons', '500': 'Internal Server Error', '501': 'Not Implemented', '502': 'Bad Gateway', '503': 'Service Unavailable', '504': 'Gateway Timeout', '505': 'HTTP Version Not Supported', '506': 'Variant Also Negotiates', '507': 'Insufficient Storage', '508': 'Loop Detected', '509': 'Bandwidth Limit Exceeded', '510': 'Not Extended', '511': 'Network Authentication Required' }http.globalAgentPoints to the global instance of the Agent object, which is an instance of the http.Agent class.It’s used to manage connections persistence and reuse for HTTP clients, and it’s a key component of Node.js HTTP networking.More in the http.Agent class description later on.Methodshttp.createServer()Return a new instance of the http.Server class.const server = http.createServer((req, res) =&gt; { // handle every single request with this callback});http.request()Makes an HTTP request to a server, creating an instance of the http.ClientRequest class.http.get()Similar to http.request(), but automatically sets the HTTP method to GET, and calls req.end() automatically.ClassesThe HTTP module provides 5 classes: http.Agent http.ClientRequest http.Server http.ServerResponse http.IncomingMessage http.Agent Node.js creates a global instance of the http.Agent class to manage connections persistence and reuse for HTTP clients, a key component of Node.js HTTP networking.This object make sure that every request made to a server is queued and a single socket is reused.It also maintains a pool of sockets. This is key for performance reasons. http.ClientRequest An http.ClientRequest object is created when http.request() or http.get() is called.When a response is received, the response event is called with response, with an http.IncomingMessage instance as argument. http.Server This class is commonly instantiated and returned when creating a new server using http.createServer().Once ypu have a server object, you have access to its methods: close() stops the serve from accepting new connections listen() starts the HTTP server and listens for connections http.ServerResponse const server = http.createServer((req, res) =&gt; {// res is an http.ServerResponse object}); These methods are used to interact with HTTP headers: getHeaderNames() get the list of the names of the HTTP headers already set getHeaders() get a copy of the HTTP headers already set setHeader(‘headername’, value) sets an HTTP header value getHeader(‘headername’) gets an HTTP header already set removeHeader(‘headername’) removes an HTTP header already set hasHeader(‘headername’) return true if the response has that header set headersSent() return true if the headers have already been sent to the clientAfter processing the headers you can send them to the client by calling response.writeHead(), which accepts the statusCode as the first parameter, the optional status message, and the headers object.To send data to the client in the response body, you use write(). It will send buffered data to the HTTP response stream.If the headers were not sent yet using response.writeHead(), it will send the headers first, with the status code and message that’s set in the request, which you can edit by setting the statusCode and statusMessage properties values: response.statusCode = 500;response.statusMessage = 'Internal Server Error'; http.IncomingMessage An http.IncomingMessage object is created by: http.Server when listening to the request eventhttp.ClientRequest when listening to the response eventIt can be used to access the response: status using its statusCode and statusMessage methods headers using its headers method or rawHeaders HTTP method using its method method HTTP version using the httpVersion method URL using the url method underlying socket using the socket methodThe data is accessed using streams, since http.IncomingMessage implements the Readable Stream interface. Node.js Buffers What is a buffer? A buffer is an area of memory. Most Javascript developers are much less familiar with this concept compared to programmers using a system programming language (like C, C++, or Go), which interact directly with memory every day.It represents a fixed-size chunk of memory (can’t be resized) allocated outside of the V8 JavaScript engine.You can think of a buffer like an array of integers, which each represent a byte of data.It is implemented by the Node.js Buffer class. Why do we need a buffer? A buffer is an area of memory. Most javascript developers are much less familiar with this concept, compared to programmers using a system programming, which interact directly with memory every day.It represents a fixed-size chunk of memory allocated outside of the V8 javascript engine.It is implemented by the Node.js Buffer class Why do we need a buffer? Buffers were introduced to help developers deal with binary data, in an ecosystem that traditionally only dealt with strings rather than binaries.Buffers in Node.js are not related to the concept of buffering data. That is what happens when a stream processor receives data faster than it can digest. How to create a buffer A buffer is created using the Buffer.from(), Buffer.alloc(), and Buffer.allocUnsafe() methods. const buf = Buffer.from('Hey!');const buf = Buffer.alloc(1024);const buf = Buffer.allocUnsafe(1024); Buffer.from(array) Buffer.from(arrayBuffer[, byteOffset[, length]]) Buffer.from(buffer) Buffer.from(string[, encoding]) Using a buffer Access the content of a buffer const buf = Buffer.from('Hey!');console.log(buf[0]); // 72console.log(buf[1]); // 101console.log(buf[2]); // 121const buf = Buffer.alloc(4);buf.write('Hey!');const buf = Buffer.from('Hey!');buf[1] = 111; // o in UTF-8console.log(buf.toString()); // Hoy!// Slice a bufferconst buf = Buffer.from('Hey!');buf.subarray(0).toString(); // Hey!const slice = buf.subarray(0, 2);console.log(slice.toString()); // Hebuf[1] = 111; // oconsole.log(slice.toString()); // Ho// Copy a bufferconst buf = Buffer.from('Hey!');const bufcopy = Buffer.alloc(4); // allocate 4 bytesbufcopy.set(buf);const buf = Buffer.from('Hey?');const bufcopy = Buffer.from('Moo!');bufcopy.set(buf.subarray(1, 3), 1);console.log(bufcopy.toString()); // 'Mey!' Node.js Streams What are streams Streams are one of the fundamental concepts that power Node.js applications.They are a way to handle reading/writing files, network communications, or any kind of end-to-end information exchange in an efficient way.Streams are not a concept unique to Node.js. They were introduced in the Unix operating system decades ago, and programs can interact with each other passing streams through the pipe operator (|).For example, in the traditional way, when you tell the program to read a file, the file is read into memory, from start to finish, and then you process it.Using streams you read it piece by piece, processing its content without keeping it all in memory.The Node.js stream module provides the foundation upon which all streamng apis are built. All streams are instances of EventEmitter. Why streams Streams basically provide two major advantages over using other data handling methods: Memory efficiency: you don’t need to load large amounts of data in memory before you are able to process it Time efficiency: It takes way less time to start processing data, since you can start processing as soon as you have it, rather than waiting till the whole data payload is avaiable An example of a stream A typical examples is reading files from a disk.Using the Node.js fs module, you can read a file, and serve it over HTTP when a new connection is established to your HTTP server:```JSconst http = require(‘http’);const fs = require(‘fs’); const server = http.createServer(function (req, res) { fs.readFile(${__dirname}/data.txt, (err, data) =&gt; { res.end(data); });});server.listen(3000);- `readFile()` reads the full contents of the file, and invokes the callback function when it's done.- `res.end(data)` in the callback will return the file contents to the HTTP client.If the file is big, the operation will take quite bit of time. Here is the same thing written using streams:```JSconst http = require('http');const fs = require('fs');const server = http.createServer((req, res) =&gt; { const stream = fs.createReadStream(`${__dirname}/data.txt`); stream.pipe(res);});server.listen(3000);Instead of waiting until the file is fully read, we start streaming it to the HTTP client as soon as we have a chunk of data ready to be sent.pipe()The above example uses the line stream.pipe(res): the pipe() method is called on the file stream.What does this code do? It takes the source, and pipes it into a destication.You call it on the source stream, so in this case, the file sream is piped to the HTTP response.The return value of the pipe() method is the destination stream, which is a very convenient thing that let us chain multiple pipe() calls, like this:src.pipe(dest1).pipe(dest2);// ORsrc.pipe(dest1);dest1.pipe(dest2);Streams-powered Node.js APIsDue to their advantages, many Node.js core modules provide native stream handling capabilities, most notably: process.stdin returns a stream connected to stdin process.stdout returns a stream connected to stdoutprocess.stderr returns a stream connected to stderr fs.createReadStream() creates a readable stream to a file fs.createWriteStream() creates a writable stream to a file net.connect() initiates a stream-based connection http.request() returns an instance of the http.ClientRequest class, which is a writable stream zlib.createGzip() compress data using gzip (a compression algorithm) into a stream zlib.createGunzip() decompress a gzip stream. zlib.createDeflate() compress data using deflate (a compression algorithm) into a stream zlib.createInflate() decompress a deflate stream Different types of streams There are four classes of streams: Readable: a stream which could be used for read data from it. In other words, its readonly. Writable: a stream which could be used for write data to it. It is writeonly. Duplex: a stream which can read and write data, basically its a combination of a Readable and Writable stream. Transform: a Duplex stream which reads data, transforms the data, and then writes the transformed data in the desired format. How to create a readable stream const Stream = require('stream');readableStream._read = () =&gt; {};const readableStream = new Stream.Readable({read() {},});readableStream.push('hi!');readableStream.push('ho!'); How to get data from a readable stream ```JSconst Stream = require(‘stream’); const readableStream = new Stream.Readable({ read() {},});const writableStream = new Stream.Writable();writableStream._write = (chunk, encoding, next) =&gt; { console.log(chunk.toString()); next();};readableStream.pipe(writableStream);readableStream.push(‘hi!’);readableStream.push(‘ho!’);You can also consume a readable stream directly, using the `readable` event```JSreadableStream.on('readable', () =&gt; { console.log(readableStream.read());});How to create a writable streamconst Stream = require('stream');const writableStream = new Stream.Writable();writableStream._write = (chunk, encoding, next) =&gt; { console.log(chunk.toString()); next();};process.stdin.pipe(writableStream);How to send data to a writable streamwritableStream.write('hey!\\n');Signaling a writable stream that you ended writingUse the end() method:const Stream = require('stream');const readableStream = new Stream.Readable({ read() {},});const writableStream = new Stream.Writable();writableStream._write = (chunk, encoding, next) =&gt; { console.log(chunk.toString()); next();};readableStream.pipe(writableStream);readableStream.push('hi!');readableStream.push('ho!');readableStream.on('close', () =&gt; writableStream.end());writableStream.on('close', () =&gt; console.log('ended'));readableStream.destroy();How to create a transform streamWe get the Transform stream from the stream module, and we initialize it and implement the transform._transform() method.First create a transform stream object:const { Transform } = require('stream');const transformStream = new Transform();then implementt _transform:transformStream._transform = (chunk, encoding, callback) =&gt; { transformStream.push(chunk.toString().toUpperCase()); callback();};Pipe readable stream:process.stdin.pipe(transformStream).pipe(process.stdout);Node.js, the difference between development and productionNode.js assumes it’s always running in a development environment. You can signal Node.js that you are running in production by settung the NODE_ENV=production environment variable.export NODE_ENV=production# orNODE_ENV=production node app.jsIn Node.js conditional to handle multiple environmentsif (process.env.NODE_ENV === 'development') { // ...}if (process.env.NODE_ENV === 'production') { // ...}if (['production', 'staging'].includes(process.env.NODE_ENV)) { // ...}// ORif (process.env.NODE_ENV === 'development') { app.use(express.errorHandler({ dumpExceptions: true, showStack: true }));}if (process.env.NODE_ENV === 'production') { app.use(express.errorHandler());}Error handling in Node.jsCreating exceptionsuse throw keyword.Error objectsthrow new Error('Ran out of coffee');orclass NotEnoughCoffeeError extends Error { // ...}throw new NotEnoughCoffeeError();Handling exceptionsAn exception handler is a try/catch statement.try { // lines of code} catch (e) {}Catching uncaught exceptionsTo solve this, you listen for the uncaughtException event on the process object:process.on('uncaughtException', err =&gt; { console.error('There was an uncaught error', err); process.exit(1); // mandatory (as per the Node.js docs)});Exceptions with promisesUsing promises you can chain different operations, and handle errors at the end:doSomething1() .then(doSomething2) .then(doSomething3) .catch(err =&gt; console.error(err));How do you know where the error occurred? You dont really know, but you can handle errors in each of function you call(doSomethingX), and inside the error handler throw a new error, that’s going to call outside catch handler:const doSomething1 = () =&gt; { // ... try { // ... } catch (err) { // ... handle it locally throw new Error(err.message); } // ...};To be able to handle errors locally without handling them in the function we call, we can break the chain. You can create a function in each then() and process the exception:doSomething1() .then(() =&gt; { return doSomething2().catch(err =&gt; { // handle error throw err; // break the chain! }); }) .then(() =&gt; { return doSomething3().catch(err =&gt; { // handle error throw err; // break the chain! }); }) .catch(err =&gt; console.error(err));Error handling with async/awaitUsing async/await, you still need to catch errors, and you do it this way:async function someFunction() { try { await someOtherFunction(); } catch (err) { console.error(err.message); }}How to log an object in Node.jsWhen you type console.log() into a JavaScript program that runs in the browser, that is going to create a nice entry in Browser Console:console.log(obj)We don’t have suck luxury when we log something to the console, because that’s going to output the object to the shell if you run the Node.js program manually, or to the log file. You get a string representation of the object.const obj = { name: 'joe', age: 35, person1: { name: 'Tony', age: 50, person2: { name: 'Albert', age: 21, person3: { name: 'Peter', age: 23, }, }, },};console.log(obj);// pretty way to printconsole.log(JSON.stringify(obj, null, 2));require('util').inspect.defaultOptions.depth = null;console.log(obj);If you don’t want to touch any kinds of defaultOptions, a perfect alternative is console.dir.// `depth` tells util.inspect() how many times to recurse while formatting the object, default is 2console.dir(obj, { depth: 10,});// ...or pass `null` to recurse indefinitelyconsole.dir(obj, { depth: null,});// %o tells console.log() to string-format and log obj in its placeconsole.log('%o', obj);Node.js with TypeScriptWhat is TypeScriptTypeScript is a very popular open-source language maintained and developed by Microsoft, it’s loved and used by a lot of software developers around the world.Basically, it’s a superset of javascript that adds new capabilities to the language. Most notable addition are static tupe definitions, something that is not present in plain Javascript. Thanks to types, it’s possible, for example, to declare what king of arguments we are expecting and what is returned really powerful tool and opens new world of possibilities in javascript projects. It makes our code more secure and robust by preventing a lot of bugs before code is even shipped - it catches problems during writing the code and integrates wonderfully with code editors like Visual Studio Code.We can talk about other TypeScript benefits later, let’s see some examples now!ExamplesTake a look at this code snippet and then we can unpack it together:type User = { name: string; age: number;};function isAdult(user: User): boolean { return user.age &gt;= 18;}const justine: User = { name: 'Justine', age: 23,};const isJustineAnAdult: boolean = isAdult(justine);First part with type keyword is responsible for declaring our custom type of objects representing users. Later we utilize this newly created type to create function isAdult that accepts one argument of type User and returns boolean. After this we create justine, our example data that can be used for calling previously defined function. Finally, we create new variable with information whether justine is an adult or not.There are additional things about this example that you should know. Firstly, if we would not comply with declared types, TypeScript would alarm us that something is wrong and prevent misuse. Secondly, not everything must be typed explicitly - TypeScript is very smart and can deduce types for us. For example, variable isJustineAnAdult would be of type boolean even if we didn’t type it explicitly or justine would be valid argument for our function even if we didn’t declare this variable as of User type.Okay, so we have some TypeScript code. Now how do we run it?First thing to do is to install TypeScript in our project:npm i -D typescriptnpx tsc example.tsThis command will result in a new file named example.js that we can run using Node.js. Now when we know how to compile and run TypeScript code let’s see TypeScript bug-preventing capabilities in action!This is how we will modify our code:type User = { name: string; age: number;};function isAdult(user: User): boolean { return user.age &gt;= 18;}const justine: User = { name: 'Justine', age: 'Secret!',};const isJustineAnAdult: string = isAdult(justine, \"I shouldn't be here!\");And this is what TypeScript has to say about this:example.ts:12:3 - error TS2322: Type 'string' is not assignable to type 'number'.12 age: \"Secret!\", ~~~ example.ts:3:3 3 age: number; ~~~ The expected type comes from property 'age' which is declared here on type 'User'example.ts:15:7 - error TS2322: Type 'boolean' is not assignable to type 'string'.15 const isJustineAnAdult: string = isAdult(justine, \"I shouldn't be here!\"); ~~~~~~~~~~~~~~~~example.ts:15:51 - error TS2554: Expected 1 arguments, but got 2.15 const isJustineAnAdult: string = isAdult(justine, \"I shouldn't be here!\"); ~~~~~~~~~~~~~~~~~~~~~~Found 3 errors.More about TypeScriptTypescript offers a whole lot of greate mechanisms like interfaces, classes, ulity types and so on. Also, on bigger projects you can declare your TypeScript compiler configuration in a separate file and granulary adjust how it works, how strict it is and where it stores compiled files for example. You can read more about all this awesome stuff in the offcial TypeScript.docsSome of the other benefits of TypeScript that are worth mentioning are that it can be adopted progressively, it helps making code more readable and understanable and it allows developers to use modern language features while shipping code for older Node.js versions.TypeScript in the Node.js worldTypeScript is well-establised in the Node.js world and used by many companies, open-sources projects tools and frameworks, Some of the notable examples of open-source projects using TypeScript are: NestJS - robust and fully-featured framework that makes creating scalable and well-architected systems easy and pleasant TypeORM - great ORM influenced by other well-known tools from other languages like Hibernate, Doctrine or Entity Framework Prisma - next-generation ORM featuring a declarative data model, generated migrations and fully type-safe database queries RxJS - widely used library for reactive programming AdonisJS - A fully featured web framework with Node.js FoalTs - The Elegant Nodejs FrameworkAnd many, many more great projects… Maybe even your next one! Asynchronous flow control At its core, JavaScript is designed to be non-blocking on the “main” thread, this is where views are rendered. You can imagine the importance of this in the browser. When the main thread becomes blocked it results in the infamous “freezing” that end users dread, and no other events can be dispatched resulting in the loss of data acquisition, for example.This creates some unique constraints that only a functional style of programming can cure. This is where callbacks come in to the picture. However, callbacks can become challenging to handle in more complicated procedures. This often results in “callback hell” where multiple nested functions with callbacks make the code more challenging to read, debug, organize, etc.async1(function (input, result1) { async2(function (result2) { async3(function (result3) { async4(function (result4) { async5(function (output) { // do something with output }); }); }); });});Of course, in real life there would most likely be additional lines of code to handle result1, result2, etc., thus, the length and complexity of this issue usually results in code that looks much more messy than the example above.This is where functions come in to great use. More complex operations are made up of many functions: initiator style / input middleware terminatorThe “initiator style / input” is the first function in the sequence. This function will accept the original input, if any, for the operation. The operation is an executable series of functions, and the original input will primarily be: variables in a global environment direct invocation with or without arguments values obtained by file system or network requestsA middleware function will return another function, and a terminator function will invoke the callback. The following illustrates the flow to network or file system requests. Here the latency is 0 because all these values are available in memory.```JSfunction final(someInput, callback) { callback(${someInput} and terminated by executing callback );}function middleware(someInput, callback) { return final(${someInput} touched by middleware , callback);}function initiate() { const someInput = ‘hello this is a function ‘; middleware(someInput, function (result) { console.log(result); // requires callback to return result });}initiate();### State managementFunctions may or may not be state dependent. State dependency aries when the input or other variable of a function relies on an outside function.In this way there are two primary strategies for state management:1. passing in variables directly to a function, and2. acquiring a variable value from a cache, session, file, database, network, or other outside source.Note, I did not mention global variable. Managing state with global variables is often a sloppy anti-pattern that make it difficult or impossible to guarantee state. Global variables in complex programs should be avoided when possible.### Control flowIf an object is available in memory, iteration is possible, and there will not be a change to control flow:```JSfunction getSong() { let _song = ''; let i = 100; for (i; i &gt; 0; i -= 1) { _song += `${i} beers on the wall, you take one down and pass it around, ${ i - 1 } bottles of beer on the wall\\n`; if (i === 1) { _song += \"Hey let's get some more beer\"; } } return _song;}function singSong(_song) { if (!_song) throw new Error(\"song is '' empty, FEED ME A SONG!\"); console.log(_song);}const song = getSong();// this will worksingSong(song);However, if the data exists outside of memory the iteration will no longer work:function getSong() { let _song = ''; let i = 100; for (i; i &gt; 0; i -= 1) { /* eslint-disable no-loop-func */ setTimeout(function () { _song += `${i} beers on the wall, you take one down and pass it around, ${ i - 1 } bottles of beer on the wall\\n`; if (i === 1) { _song += \"Hey let's get some more beer\"; } }, 0); /* eslint-enable no-loop-func */ } return _song;}function singSong(_song) { if (!_song) throw new Error(\"song is '' empty, FEED ME A SONG!\"); console.log(_song);}const song = getSong('beer');// this will not worksingSong(song);// Uncaught Error: song is '' empty, FEED ME A SONG!You will be able to perform almost all of your operations with the following 3 patterns: In series: functions will be executed in a strict sequential order, this one is most similar to for loops:```JS// operations defined elsewhere and ready to executeconst operations = [ { func: function1, args: args1 }, { func: function2, args: args2 }, { func: function3, args: args3 },];function executeFunctionWithArgs(operation, callback) { // executes function const { args, func } = operation; func(args, callback);}function serialProcedure(operation) { if (!operation) process.exit(0); // finished executeFunctionWithArgs(operation, function (result) { // continue AFTER callback serialProcedure(operations.shift()); });}serialProcedure(operations.shift());2. Full parallel: when ordering is not an issue, such as emailing a list of 1,000,000 email recipients.```JSlet count = 0;let success = 0;const failed = [];const recipients = [ { name: 'Bart', email: 'bart@tld' }, { name: 'Marge', email: 'marge@tld' }, { name: 'Homer', email: 'homer@tld' }, { name: 'Lisa', email: 'lisa@tld' }, { name: 'Maggie', email: 'maggie@tld' },];function dispatch(recipient, callback) { // `sendEmail` is a hypothetical SMTP client sendMail( { subject: 'Dinner tonight', message: 'We have lots of cabbage on the plate. You coming?', smtp: recipient.email, }, callback );}function final(result) { console.log(`Result: ${result.count} attempts \\ &amp; ${result.success} succeeded emails`); if (result.failed.length) console.log(`Failed to send to: \\ \\n${result.failed.join('\\n')}\\n`);}recipients.forEach(function (recipient) { dispatch(recipient, function (err) { if (!err) { success += 1; } else { failed.push(recipient.name); } count += 1; if (count === recipients.length) { final({ count, success, failed, }); } });}); Limited parallel: parallel with limit, such as successfully emailing 1,000,000 recipients from a list of 10E7 users.```JSlet successCount = 0;function final() { console.log(dispatched ${successCount} emails); console.log(‘finished’);}function dispatch(recipient, callback) { // sendEmail is a hypothetical SMTP client sendMail( { subject: ‘Dinner tonight’, message: ‘We have lots of cabbage on the plate. You coming?’, smtp: recipient.email, }, callback );}function sendOneMillionEmailsOnly() { getListOfTenMillionGreatEmails(function (err, bigList) { if (err) throw err;function serial(recipient) { if (!recipient || successCount &gt;= 1000000) return final(); dispatch(recipient, function (_err) { if (!_err) successCount += 1; serial(bigList.shift()); });}serial(bigList.shift()); }); }sendOneMillionEmailsOnly();Each has its own use cases, benefits, and issues you can experiement and read about in more detail. Most importanly, remember to modularize your operations and use callbacks! If you feel any doubt, treat everything as if it were middleware!## Node.js with WebAssemblyWebAssembly is a high-performance assembly-like language that can be compiled from a myriad of languages including C/C++, Rust, and AssemblyScript. As of right now, it is supported by Chrome, Firefox, Safari, Edge, and Node.js!The WebAssembly specification details two file formats, a binary format called a WebAssembly Module with a .wasm extension and corresponding text representation called WebAssembly Text format with a .wat extension.### Key Concepts- Module - A compiled WebAssembly binary, ie a .wasm file.- Memory - A resizable ArrayBuffer.- Table - A resizable typed array of references not stored in Memory.- Instance - An instantiation of a Module with its Memory, Table, and variables.```JSconsole.log(WebAssembly);/*Object [WebAssembly] { compile: [Function: compile], validate: [Function: validate], instantiate: [Function: instantiate]}*/Generating WebAssembly ModulesThere are multiple methods available to generate WebAssembly binary files including: Writing WebAssembly (.wat) by hand and converting to binary format using tools such as wabt Using emscripten with a C/C++ application Using wasm-pack with a Rust application Using AssemblyScript if you prefer a TypeScript-like experience Some of these tools generate not only the binary file, but the JavaScript “glue” code and corresponding HTML files to run in the browser. How to use it Once you have a WebAssembly module, you can use the Node.js WebAssembly object to instantiate it.```JS// Assume add.wasm file exists that contains a single function adding 2 provided argumentsconst fs = require(‘fs’); const wasmBuffer = fs.readFileSync(‘/path/to/add.wasm’);WebAssembly.instantiate(wasmBuffer).then(wasmModule =&gt; { // Exported function live under instance.exports const { add } = wasmModule.instance.exports; const sum = add(5, 6); console.log(sum); // Outputs: 11});```Interacting with the OSWebAssembly modules cannot directly access OS functionality on its own. A third-party tool Wasmtime can be used to access this functionality. Wasmtime utilizes the WASI API to access the OS functionality.Resources General WebAssembly Information MDN Docs Write WebAssembly by hand" }, { "title": "gRPC – An RPC library and framework", "url": "/posts/grpc-an-rpc-library-and-framework/", "categories": "", "tags": "", "date": "2022-05-10 00:00:00 +0700", "snippet": "", "content": "" }, { "title": "How to create ecommerce website use Laravel 9, VueJs, Docker, Supervisord, Brooadcast and Redis Queue (part 1)", "url": "/posts/how-to-create-ecommerce-website-use-laravel-9-vuejs-docker-supervisord-brooadcast-and-redis-queue-part-1/", "categories": "", "tags": "", "date": "2022-05-09 00:00:00 +0700", "snippet": "IntrodutionCreate an E-Commerce website with laravel 9 and VueJs : Dockerize setup all in one API integration. Redis for queue job in concurrency request. Supervisor manage queue job(Laravel ho...", "content": "IntrodutionCreate an E-Commerce website with laravel 9 and VueJs : Dockerize setup all in one API integration. Redis for queue job in concurrency request. Supervisor manage queue job(Laravel horizon). Laravel Broadcast event to observe status of serve and update to client. Deploy website to AWS EC2.Checkout source code on my gitPart 1 InstallDockerize setup all in oneDocker InstallLaravel with VueJs, Redis, Supervisod, and Horizon installRedis InstallPusher InstallComplete Dockerfile and .yml fileFROM php:8.0-fpm # Copy composer.lock and composer.json into the working directoryCOPY composer.lock composer.json /var/www/html/ # Set working directoryWORKDIR /var/www/html/ # Install dependencies for the operating system softwareRUN apt-get update &amp;&amp; apt-get install -y \\ build-essential \\ libpng-dev \\ libjpeg62-turbo-dev \\ libfreetype6-dev \\ locales \\ zip \\ jpegoptim optipng pngquant gifsicle \\ vim \\ libzip-dev \\ unzip \\ git \\ libonig-dev \\ curl \\ build-essential \\ openssl \\ libssl-dev curl \\ supervisor \\ software-properties-common RUN groupadd dev# node installENV NODE_VERSION=15.14.0RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bashENV NVM_DIR=/usr/local/nvm/.nvmRUN mkdir /usr/local/nvmRUN mkdir /usr/local/nodeRUN chown -R root:dev /usr/local/nvmRUN chmod -R 775 /usr/local/nvmRUN chown -R root:dev /usr/local/nodeRUN chmod -R 775 /usr/local/nodeRUN cp -R /root/.nvm/ /usr/local/nvm/ENV NVM_DIR=/usr/local/nvm/.nvmRUN export NVM_DIR=/usr/local/nvm/.nvmRUN . \"/usr/local/nvm/.nvm/nvm.sh\" &amp;&amp; nvm install ${NODE_VERSION}RUN . \"/usr/local/nvm/.nvm/nvm.sh\" &amp;&amp; nvm use v${NODE_VERSION}RUN . \"/usr/local/nvm/.nvm/nvm.sh\" &amp;&amp; nvm alias default v${NODE_VERSION}ENV PATH=\"/usr/local/nvm/.nvm/versions/node/v${NODE_VERSION}/bin/:${PATH}\"RUN node --versionRUN npm --version# Clear cacheRUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* # Install extensions for phpRUN docker-php-ext-install pdo_mysql mbstring zip exif pcntl RUN docker-php-ext-configure gd --with-freetype --with-jpegRUN docker-php-ext-install gdRUN pecl install redis \\ &amp;&amp; docker-php-ext-enable redis # Install composer (php package manager)RUN curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer # Copy existing application directory contents to the working directoryCOPY . /var/www/htmlENV ENABLE_CRONTAB 1ENV ENABLE_HORIZON 1ENTRYPOINT [\"sh\", \"/var/www/html/docker-entrypoint.sh\"]COPY ./supervisor.d/*.* /etc/supervisor/conf.d/RUN composer installRUN composer dump-autoloadRUN chmod 777 install.shRUN [\"/bin/bash\", \"-c\", \"./install.sh\"]# Assign permissions of the working directory to the www-data userRUN chown -R www-data:www-data \\ /var/www/html/storage \\ /var/www/html/bootstrap/cacheRUN chmod -R 777 /var/www/html/storageEXPOSE 9000CMD supervisord -n -c /etc/supervisor/supervisord.conf# Expose port 9000 and start php-fpm server (for FastCGI Process Manager)# CMD [\"php-fpm\"]Service distributed by docker-compose.ymlversion: '2.0'services: #PHP Service app: build: context: . dockerfile: Dockerfile image: cloudsigma.com/php container_name: app restart: unless-stopped tty: true environment: SERVICE_NAME: app SERVICE_TAGS: dev working_dir: /var/www/html/ volumes: - ./:/var/www/html/ - ./php/laravel.ini:/usr/local/etc/php/conf.d/laravel.ini networks: - app-network #Nginx Service webserver: image: nginx:alpine container_name: webserver restart: unless-stopped tty: true ports: - \"80:80\" - \"443:443\" volumes: - ./:/var/www/html/ - ./nginx/conf.d/:/etc/nginx/conf.d/ networks: - app-network #MySQL Service db: image: mysql:5.7.32 container_name: db restart: unless-stopped tty: true ports: - \"3306:3306\" environment: MYSQL_DATABASE: lecommerce MYSQL_ROOT_PASSWORD: 123456 SERVICE_TAGS: dev SERVICE_NAME: mysql volumes: - dbdata:/var/lib/mysql/ - ./mysql/my.cnf:/etc/mysql/my.cnf networks: - app-network redis: image: redis:alpine container_name: redis ports: - \"6379:6379\" volumes: - cache_data:/data networks: - app-network#Docker Networksnetworks: app-network: driver: bridge#Volumesvolumes: dbdata: driver: local cache_data: driver: localdocker-entrypoint.shIntroduction of Docker ENTRYPOINT. Docker entrypoint is a Dockerfile directive or instruction that is used to specify the executable which should run when a container is started from a Docker image. It has two forms, the first one is the ‘exec’ form and the second one is the ‘shell’ form.I was used a PHP-FPM based image, this file was useful when use Ubuntu image or deploy to Linux based system:#!/bin/shset -e# Enable Laravel scheduleif [[ \"${ENABLE_CRONTAB:-0}\" = \"1\" ]]; then echo \"* * * * * php /var/www/html/artisan schedule:run &gt;&gt; /dev/null 2&gt;&amp;1\" &gt;&gt; /etc/crontabs/www-datafiexec \"$@\"Part 2" }, { "title": "SOLID principles, and SOLID in frameworks (part 1)", "url": "/posts/solid-principles-and-solid-in-frameworks/", "categories": "Blogging, Tutorial", "tags": "solid, rails, laravel, PHP, ruby, python", "date": "2022-04-16 00:00:00 +0700", "snippet": "Solid principles contains five best praticles when construct source code by OOP methodology.Single responsible principlesThe single-responsibility principle (SRP) is a computer-programming principl...", "content": "Solid principles contains five best praticles when construct source code by OOP methodology.Single responsible principlesThe single-responsibility principle (SRP) is a computer-programming principle that states that every module, class or function in a computer program should have responsibility over a single part of that program’s functionality, and it should encapsulate that part. All of that module, class or function’s services should be narrowly aligned with that responsibility.E.g: Defime module User manage user, define User class represent User in system, define Role class manage Role in System, define Permission class represent Permission in system, Policy class represent policy in system. Every software component should have one and only one reponsiblity reason to change.cohesion Cohesion is the degree to which the various part of a software components relatedHigher cohesion helps attain better adherence to the SRP.coupling Coupling is defined as level of the inter dependency betweens various of software components.Loose coupling helps attain better adherence to the SRP.class Rectangle { public $width; public $height; public function __construct($width, $height) { $this-&gt;width = $width; $this-&gt;height = $height; }}class Square { public $length; public function __construct($length) { $this-&gt;length = $length; }}class AreaCalculator { protected $shapes; public function __construct($shapes = array()) { $this-&gt;shapes = $shapes; } public function sum() { $area = []; foreach($this-&gt;shapes as $shape) { if($shape instanceof Square) { $area[] = pow($shape-&gt;length, 2); } else if($shape instanceof Rectangle) { $area[] = $shape-&gt;width * $shape-&gt;height; } } return array_sum($area); }}Open closed princliple Software components should be closed for modification, but open for extension Closed for modification: New features getting added to sofrware component, should not have modify code Open for extensiion: A software should be extenable to add a new feature or to add a new behaviour to it.Eg: Define two class of Shape with attributes is closed for moification and the method is open for extendsion, example like we want calculator for area of rectangle.interface Shape{ /** * Calculate the area of the shape. * * @return mixed */ public function getArea();}class Rectangle implements Shape{ private $width; private $height; public function getArea() { return $this-&gt;width * $this-&gt;height; }}class Circle implements Shape{ private $radius; private function getArea() { return pow($this-&gt;radius, 2) * PI; }}/** * Calculate the total area of the shapes. * * @param Illuminate\\Support\\Collection $shapes * @return float */ public function Area($shapes) { $area = 0; $shapes-&gt;each(function ($shape) use (&amp;$area) { $area += $shape-&gt;getArea(); }); return $area; }}Good: Ease of adding new features Lead to minimal cost of developing and testing software requires decoupling, which in turn automatically foallows the SRPCaution: Do not bindly, you will end up with a huge number of class that can complicated your overall design. Make a subjective, rather than an objective decision.The Open/Closed Principle is designed to make you write your code in such a manner that the core functionality is as unambiguous and concise as possibleLiskov subtitution priciple Obejct should be replaceable with their subtypes without affecting the correctness of the programabstract class Bird {\t\tabstract public function fly();}class Pengiue extends Bird {\tpublic function fly() \t{\t\t\treturn false;\t}}class Eagle extends Bird {\tpublic function fly()\t{\t\t\treturn true\t}} break the hierachy Tell, don’t askInterface Segregation principle No client should be forced to depend on methods it does not useTechniques to identify violations of ISP Fat interfaces. Interface with low cohesion. Empty method implentations.&lt;?php// Interface Segregation Principle Violationinterface Workable{ public function canCode(); public function code(); public function test();}class Programmer implements Workable{ public function canCode() { return true; } public function code() { return 'coding'; } public function test() { return 'testing in localhost'; }}class Tester implements Workable{ public function canCode() { return false; } public function code() { throw new Exception('Opps! I can not code'); } public function test() { return 'testing in test server'; }}class ProjectManagement{ public function processCode(Workable $member) { if ($member-&gt;canCode()) { $member-&gt;code(); } }}// Refactoredinterface Codeable{ public function code();}interface Testable{ public function test();}class Programmer implements Codeable, Testable{ public function code() { return 'coding'; } public function test() { return 'testing in localhost'; }}class Tester implements Testable{ public function test() { return 'testing in test server'; }}class ProjectManagement{ public function processCode(Codeable $member) { $member-&gt;code(); }}Dependentcy Injection principle High level module should not depend on low level module, Both should depend on abstrctions.Abstraction should not depend on details. Details should depend on abstraction Without dependency injectionclass GoogleMaps{ public function getCoordinatesFromAddress($address) { // calls Google Maps webservice }}class OpenStreetMap{ public function getCoordinatesFromAddress($address) { // calls OpenStreetMap webservice }}class StoreService{ public function getStoreCoordinates($store) { $geolocationService = new GoogleMaps(); // or $geolocationService = GoogleMaps::getInstance() if you use singletons return $geolocationService-&gt;getCoordinatesFromAddress($store-&gt;getAddress()); }}With dependency injectionclass StoreService { private $geolocationService; public function __construct(GeolocationService $geolocationService) { $this-&gt;geolocationService = $geolocationService; } public function getStoreCoordinates($store) { return $this-&gt;geolocationService-&gt;getCoordinatesFromAddress($store-&gt;getAddress()); }}interface GeolocationService { public function getCoordinatesFromAddress($address);}class GoogleMaps implements GeolocationService { ...class OpenStreetMap implements GeolocationService { ...Inversion of control Inversion of control (IOC) defines the way objects are used, but it does not specify how to create them. IOC defines the relationship between the high-level class and detail class, where the high-level class depends on detail class. High- and low-level classes are related by abstracting a detail class in IOC.&lt;?php//Define ClassAclass ClassA{ public $ClassB; public $ClassC; public function ClassA() { echo \"&lt;h2 style='color:red'&gt; Initialization of ClassA &lt;/h2&gt;\"; } public function method() { $this-&gt;ClassB = new ClassB(); $this-&gt;ClassC = new ClassC(); $this-&gt;ClassB-&gt;method(); $this-&gt;ClassC-&gt;method(); }}//Define ClassBclass ClassB{ public function ClassB() { echo \"&lt;h2 style='color:blue'&gt; Initialization of ClassB &lt;/h2&gt;\"; } public function method() { echo \"&lt;h3 style='color:blue'&gt; The output from ClassB &lt;/h3&gt;\"; }}//Define ClassCclass ClassC{ public function ClassC() { echo \"&lt;h2 style='color:green'&gt; Initialization of ClassC &lt;/h2&gt;\"; } public function method() { echo \"&lt;h3 style='color:green'&gt; The output from ClassC &lt;/h3&gt;\"; }}//Create object of ClassA$object = new ClassA();//Call the method() function$object-&gt;method();?&gt;Totally SOLID principles complement each other, and work together in unison, to achieve the common purpose of well-designed software" }, { "title": "How to build a Shortened URL By Ruby On Rails like Bitly or Twitter", "url": "/posts/shortened-url-by-ruby-on-rails/", "categories": "Framework, Rails", "tags": "rails, api, rspec, shortenURL", "date": "2022-04-16 00:00:00 +0700", "snippet": "How to build a Shortened URL By Ruby On Rails like Bitly or TwitterGive a website shortenedUrl to convert longURL to short URL with alpha lexigraphy encoded/decoded response to User. Give two endp...", "content": "How to build a Shortened URL By Ruby On Rails like Bitly or TwitterGive a website shortenedUrl to convert longURL to short URL with alpha lexigraphy encoded/decoded response to User. Give two endpoint encoded decoded Extra default enpoint is index, and retore lasted encoded url About algorithm used on website. I use Bijective algorithm to make a alpha lexigraphy short url. Give rate limit process to request IP and prevent attack vector. I was use gem rack-attack for this case to handle limit request per IP is 5 request to 2 second. other wise, better to authentic user before they use api. To do this prevent, can implement a mechanism authentic by a system by api key for user indetify and in session. System was give an URL with a unique shortURL to decrease weight in database and open scale up if have mutilple dabase installed. To prepare for scale up: 1. Give a better cache system support Rate limit. 2. With URL was created before, we change to operation select instead create same URL. 3. Given a load ballancing to suppot this load is even better than server serve every single request. 4. Architecture from Monolith or micro service and under control of load balancing give us a good reputation.Install System Ruby version ^2.5~3.0 Rails rack-attack gem Rspec factory_bot_rails fakerTo install run command below:bundleAPI in useThree enpoint with two default enpoint(/api/v1/shortner/): index /api/v1/shortner: Get methodjson response: { \"message\": \"Hello shortner\", \"encoded\": \"q\", \"url\": \"http://google.com\"} encoded /api/v1/shortner/encoded: Post methodJson request: { \"url\": \"http://google.com\"} Json Response { \"url\": \"http://google.com\", \"decoded\": \"b\"} decoded request /api/v1/shortner/decoded: Post methodJson Request { \"encoded\": \"e\"} Json Response { \"url\": \"http://google.com\", \"status\": 200} Last URL saved Last user saved was restore in Database. I was use default by sqlite3. But for better performance and production case need to setup dabase with MYSQL or PostgresSQL.encoded def generate_slug last_id = ShortenUrl.last ? ShortenUrl.last.id : 1 self.slug = Bijective.bijective_encode(last_id) enddecodedurl = ShortenUrl.find_by_slug params[:encoded]url.urlAlgorithm to conver longURL to Shortclass Bijective ALPHABET = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\".split(//) # make your own alphabet using: # (('a'..'z').to_a + ('A'..'Z').to_a + (0..9).to_a).shuffle.join def self.bijective_encode(i) # from http://refactormycode.com/codes/125-base-62-encoding # with only minor modification return ALPHABET[0] if i == 0 s = '' base = ALPHABET.length while i &gt; 0 s &lt;&lt; ALPHABET[i.modulo(base)] i /= base end s.reverse end def self.bijective_decode(s) # based on base2dec() in Tcl translation # at http://rosettacode.org/wiki/Non-decimal_radices/Convert#Ruby i = 0 base = ALPHABET.length s.each_char { |c| i = i * base + ALPHABET.index(c) } i endendTest application with SpecTest case was specified with controller and function endoint. To run test from root of project run command:rspecSumarizeGive a website for serve enpoint to convert long url to short url, you can find source code at my github repository." }, { "title": "Decoupling Ruby on Rails: Delegation vs Dependency Injection", "url": "/posts/decoupling-ruby-on-rails-delegation-vs-dependency-injection/", "categories": "Fullstack, Backend", "tags": "ruby, solid, dependency, rails, oop", "date": "2022-04-16 00:00:00 +0700", "snippet": "Delegation &amp;&amp; Dependency InjectionDelegationRuby’s standard library provides us SimpleDelegator as an easy way to implement the decorator pattern. You pass in your object to the constructor...", "content": "Delegation &amp;&amp; Dependency InjectionDelegationRuby’s standard library provides us SimpleDelegator as an easy way to implement the decorator pattern. You pass in your object to the constructor, and then any method calls to the delegator are forwarded to your object.E.g:class PrawnWrapper &lt; SimpleDelegator def initialize(document: nil) document ||= Prawn::Document.new(...) super(document) endendWe can then update our reports to inherit from this class, and they will still function the same as before, using the default document created in our initializer. The magic happens when we use this in our overview report:class OverviewReport &lt; PrawnWrapper ... def render sales = SaleReport.new(..., document: self) sales.sales_table costs = CostReport.new(..., document: self) costs.costs_pie_chart ... endendwe have essentially made it as if SalesReport is now a subclass of OverviewReport. In our case, this means that all the calls to prawn’s API now go SalesReport -&gt; OverviewReport -&gt; Prawn::Document.How SimpleDelegator Works: deletegation as servicesThe way SimpleDelegator works under the hood is basically to use Ruby’s method_missing functionality to forward method calls to another object.So SimpleDelegator (or a subclass of it) receives a method call. If it implements that method, great; it will execute it just as any other object would. However, it if does not have that method defined, then it will hit method_missing. method_missing will then attempt to call that method on the object given to its constructor.require 'simple_delegator'class Thing def one 'one' end def two 'two' endendclass ThingDecorator &lt; SimpleDelegator def two 'three!' endendThingDecorator.new(Thing.new).one #=&gt; \"one\"ThingDecorator.new(Thing.new).two #=&gt; \"three!\" The key difference: SimpleDelegator takes the object it will delegate to as an argument in its constructor. This means we can pass in different objects at runtime.This is what allows use to redirect the calls to a prawn object in Solution 2 above. If we call a single report, the prawn calls go to a new document created in the constructor. The overview report, however, can change this so that calls to prawn are forwarded to its document.Delegate in Active RecordForward methods or attributes from class to subclassclass Greeter &lt; ActiveRecord::Base def hello 'hello' end def goodbye 'goodbye' endendclass Foo &lt; ActiveRecord::Base belongs_to :greeter delegate :hello, to: :greeterendFoo.new.hello # =&gt; \"hello\"Foo.new.goodbye # =&gt; NoMethodError: undefined method `goodbye' for #&lt;Foo:0x1af30c&gt;Dependency Injection(DI)ConceptIn software engineering, dependency injection is a design pattern in which an object receives other objects that it depends on. A form of inversion of control, dependency injection aims to separate the concerns of constructing objects and using them, leading to loosely coupled programs.[1][2][3] The pattern ensures that an object which wants to use a given service should not have to know how to construct those services. Instead, the receiving object (or ‘client’) is provided with its dependencies by external code (an ‘injector’), which it is not aware of.[4] Dependency injection solves the following problems:[5] How can a class be independent from the creation of the objects it depends on? How can an application, and the objects it uses support different configurations? How can the behavior of a piece of code be changed without editing it directly? Fundamentally, dependency injection consists of passing parameters to a method. As mentioned previously, one common solution to this kind of problem is to refactor the code to use Dependency Injection. That is, rather than having all these reports call methods on self, we will instead pass in our PDF document as an argument.Dependency injection in servicesThis would give us something more like:class CostReport &lt; Prawn::Document... def title(pdf = self) pdf.text \"Cost Report\" pdf.move_down 20 ... endendThis works, but there is some overhead here. For one thing, every single drawing method now has to take the pdf argument, and every single call to prawn now has to go through this pdf argument.Dependency injection has some benefits: it pushes us toward decoupled components in our system and allows us to pass in mocks or stubs to make unit testing easier.However, we are not reaping the rewards of these benefits in our scenario. We are already strongly coupled to the prawn API, so changing to a different PDF library would almost certainly require an entire rewrite of the code.Testing is also not a big concern here, because in our case testing generated PDF reports with automated tests is too cumbersome to be worthwhile.Delegation and Dependency InjectionI want to mention, when Developer need to implement Dependency Injection in Rails is make something double and couple because It’s likely we want to create multiple services different and less couple with base class used While Ruby on Rails is every where we can fine or define an Object and we spent manage effort with each class services.So it less use than delegation.Let think about a Document class want to define a method to render a report. By DI we mush define a method with parameter is Interface represent of class handling services.But in delegate we make a less couple but still maintain connection between two class, by passing a method from other class to use in this class. This behavious is same but one is Rails define and support, one is design pattern so we can choose to use an approach with each use case.Use Case when DI is always better: Declare services with long time developement but need behavious we wanted, So we can boot a class use DI when bootstrap a service so give a best performance while delegate is must require the clarify of other class method and this required a check behavious: input and output before using delegate." }, { "title": "Loop with decrement order in Python, Ruby, PHP.", "url": "/posts/loop-with-decrement-order-in-Python-Ruby-PHP/", "categories": "Fullstack, Python", "tags": "python, ruby, php, loop, for-loop", "date": "2022-03-12 00:00:00 +0700", "snippet": "Loop in decrease modePythonIn Python, decrement is not clearly supported by syntax.By default, the loop is used to iterate over the sequence: a list, a tuple, a dictionary, a set, or a string.For &...", "content": "Loop in decrease modePythonIn Python, decrement is not clearly supported by syntax.By default, the loop is used to iterate over the sequence: a list, a tuple, a dictionary, a set, or a string.For &lt;variable&gt; in range(start index, stop index, step) Variable: A variable is a value that starts from the start index and ends at the stop index in for loop. Start index: It is an optional value. If not passed, it starts from 0. It is the integer value from which the for loop value gets starts iterating. Stop index: It is the integer value from which the for loop value gets stops iterating.Step: It is the optional value. If not passed it increment the value by 1. It is an integer value that defines the increment and decrement of the loop.1. Using Start, Stop Index, and step to Decrement for loop in Python#Start index and stop index valuestartindex = 10stopindex = 0step = -1 #applying for loopfor i in range(startindex, stopindex, step): print(i)2. Using reversed() Function to Decrement for loop in PythonReversed() function is used to loop over a sequence in reverse order#applying reversed() function for i in reversed(range(5)): print('Output : ',i)3. Using whileThis way look isn’t an elegant way, but while was introduction is an infinite loop. So everything is hard to approach you can start to think about using while#applying while and -= operations = 10while s&gt;0: print(\"Output : \",s) s -= 1RubyFirst way:for i in (10).downto(0) puts iendSecond way:(10).downto(0) do |i| puts iendThird way:i=10;until i&lt;0 puts i i-=1endFour way:i = 10while i &gt; 0 i -= 1endPHPElegant way:for($i=10,$j=1;$i&gt;=1;$i--){ echo $i.'&lt;br&gt;'; echo $j.'&lt;br&gt;'; $j++;}" }, { "title": "Repository pattern và repository pattern trong laravel", "url": "/posts/Repository-pattern-va-repository-pattern-trong-laravel/", "categories": "Fullstack, Backend, DesignPattern", "tags": "design-pattern, laravel", "date": "2021-07-31 00:00:00 +0700", "snippet": "Repository patternKhái niệmDesign Pattern là một kỹ thuật trong lập trình hướng đối tượng, nó khá quan trọng và mọi lập trình viên muốn giỏi đều phải biết. Được sử dụng thường xuyên trong các ngôn ...", "content": "Repository patternKhái niệmDesign Pattern là một kỹ thuật trong lập trình hướng đối tượng, nó khá quan trọng và mọi lập trình viên muốn giỏi đều phải biết. Được sử dụng thường xuyên trong các ngôn ngữ OOP. Nó sẽ cung cấp cho bạn các “mẫu thiết kế”, giải pháp để giải quyết các vấn đề chung, thường gặp trong lập trình. Các vấn đề mà bạn gặp phải có thể bạn sẽ tự nghĩ ra cách giải quyết nhưng có thể nó chưa phải là tối ưu. Design Pattern giúp bạn giải quyết vấn đề một cách tối ưu nhất, cung cấp cho bạn các giải pháp trong lập trình OOP.Repository pattern là một trong các design pattern, đóng vai trò như một lớp trung gian kết nối giữa tầng business logic và database layer. Điều này sẽ làm bớt việc viết logic tại controller cũng như ở model. Những lý do ta nên sử dụng mẫu Repository Pattern: Code dễ dàng maintain. Tăng tính bảo mật và rõ ràng cho code. Lỗi ít hơn. tránh việc lặp code.Định nghĩa theo sách Patterns of Enterprise Application Architecture, Martin Fowler A repository performs the tasks of an intermediary between the domain model layers and data mapping, acting in a similar way to a set of domain objects in memory. Client objects declaratively build queries and send them to the repositories for answers. Conceptually, a repository encapsulates a set of objects stored in the database and operations that can be performed on them, providing a way that is closer to the persistence layer. Repositories, also, support the purpose of separating, clearly and in one direction, the dependency between the work domain and the data allocation or mapping.Thay vì việc viết logic vào trong controller, bạn có thể tạo thư mục Repository và tạo file repository trùng với class model. Sử dụng bằng các inject vào trong controller thông qua construct.Laravel repository patternReferencesDesign the infrastructure persistence layerTổng hợp các bài hướng dẫn về Design Pattern" }, { "title": "Phương thức thanh toán Softbank cài đặt bằng ngôn ngữ PHP", "url": "/posts/softbank-payment-japan/", "categories": "Tutorial, Payment", "tags": "php, softbank, au, paypay", "date": "2021-07-18 00:00:00 +0700", "snippet": "Phương thức Softbank payment bằng PHPTôi sẽ bắt đầu với nội dung bài viết luôn, với lý do là có biết bao phương thức payment và nếu bạn làm việc ở thị trường Nhật thì softbank payment là một phương...", "content": "Phương thức Softbank payment bằng PHPTôi sẽ bắt đầu với nội dung bài viết luôn, với lý do là có biết bao phương thức payment và nếu bạn làm việc ở thị trường Nhật thì softbank payment là một phương thức thanh toán khá thông dụng.Liên kết: Địa chỉ softbank: https://www.sbpayment.jp/ Địa chỉ cho lập trình viên: https://developer.sbpayment.jp/ Bài viết tham khảo: https://kipalog.com/posts/Tich-hop-softbank-payment-cho-trang-web-cua-ban Softbank paymentSoftbank payment là một phương thức thanh toán khá thông dụng tại các website thương mại điện tử tại Nhật, nó đáp ứng một số lượng lớn các thẻ thanh toán nội địa cũng như master, visa, ngoài ra alipay v..v… Softbank payment cung cấp 2 cách để thanh toán(link type, API type): Api tích hợp Liên kết tới trang trung gian của softbank payment. \\ Phương thức bảo mật: 3D secure (3Ds), giống như phương thức thanh toán xác thực 2 bướcSoftbank payment cũng có dịch vụ khá giống với stripe: Online payment service Payment service for stores: pot, qr, mobile Partner program Tích hợp softbank payment Đăng ký tài khoản các bạn vào link sau đây để đăng ký tài khoản nhé:https://developer.sbpayment.jp/user_regist/consent Softbank cung cấp môi trường sanbox nếu bạn muốn test hoặc trải nghiệm. Trong bài viết này tôi muốn giới thiệu đến phương thức thanh toán sự dụng liên kết tới bên thứ ba Cài đặt form Mẫu form tương ứng: gồm các thông số kèm order chi tiết. Thông tin form, order order.pay_method = \"\";order.merchant_id = \"30132\";order.service_id = \"001\"; Thông tin merchant credit：クレジットカード決済credit3d：クレジットカード決済（本人認証サービス（3D セキュア））unionpay：銀聯ネット決済webcvs：Web コンビニ決済payeasy：Pay-easy 決済banktransfer：総合振込決済cyberedy：楽天 Edy 決済（楽天 Edy）mobileedy：楽天 Edy 決済（モバイル楽天 Edy）suica：モバイル Suica 決済webmoney：WebMoney 決済netcash：Net Cash 決済bitcash：BitCash 決済prepaid：JCB PREMO 決済docomo：ドコモ払いauone：au かんたん決済softbank：S!まとめて支払いyahoowallet：Yahoo!ウォレット決済yahoowalletdg：Yahoo!ウォレット決済（デジコン版）rakuten：楽天ペイ（オンライン決済）recruit：リクルートかんたん支払いalipay：Alipay 国際決済paypal：Paypal 決済netmile：ネットマイル決済mysoftbank：ソ フトバ ンク まとめて支払い（Ａ）softbank2：ソフトバンクまとめて支払い（Ｂ）saisonpoint：永久不滅ポイントlinepay：LINE Paytpoint：T ポイントプログラム（オンライン決済）applepay：Apple Paynppostpay：NP 後払い Mình chọn phương thức paypay , bạn có thể nhập chực tiếp paypay không cần nhập mã id của các phương thức.Bạn vào trang paypay và đăng ký tài khoản developer tại đây https://www.paypal.com \\ Tôi có lấy mẫu từ trang developer:function f_submit() { var order = new Order(); order.pay_method = \"1234\"; order.merchant_id = \"30132\"; order.service_id = \"001\"; order.cust_code = \"Merchant_TestUser_999999\"; order.sps_cust_no = \"1234\"; order.sps_payment_no = \"1234\"; order.order_id = \"a1a1be0a082ac31eedbd126c8b0b0d5a\"; order.item_id = \"T_0003\"; order.pay_item_id = \"\"; order.item_name = \"テスト商品\"; order.tax = \"\"; order.amount = \"1\"; order.pay_type = \"0\"; order.auto_charge_type = \"\"; order.service_type = \"0\"; order.div_settele = \"\"; order.last_charge_month = \"\"; order.camp_type = \"\"; order.tracking_id = \"\"; order.terminal_type = \"0\"; order.success_url = \"http://stbfep.sps-system.com/MerchantPaySuccess.jsp\"; order.cancel_url = \"http://stbfep.sps-system.com/MerchantPayCancel.jsp\"; order.error_url = \"http://stbfep.sps-system.com/MerchantPayError.jsp\"; order.pagecon_url = \"http://stbfep.sps-system.com/MerchantPayResultRecieveSuccess.jsp\"; order.free1 = \"\"; order.free2 = \"\"; order.free3 = \"\"; order.free_csv_input = \"LAST_NAME=鈴木,FIRST_NAME=太郎,LAST_NAME_KANA=スズキ,FIRST_NAME_KANA=タロウ,FIRST_ZIP=210,SECOND_ZIP=0001,ADD1=岐阜県,ADD2=あああ市あああ町,ADD3=,TEL=12345679801,MAIL=aaaa@bb.jp,ITEM_NAME=TEST ITEM\"; order.request_date = \"20210718190350\"; order.limit_second = \"\"; order.hashkey = \"c48e0e2c7d04f0954594f14c7801bd430ca6263e\"; var orderDetail = new OrderDetail(); orderDetail.dtl_rowno = \"1\"; orderDetail.dtl_item_id = \"dtlItem_1\"; orderDetail.dtl_item_name = \"明細商品名1\"; orderDetail.dtl_item_count = \"1\"; orderDetail.dtl_tax = \"1\"; orderDetail.dtl_amount = \"1\"; orderDetail.dtl_free1 = \"\"; orderDetail.dtl_free2 = \"\"; orderDetail.dtl_free3 = \"\"; order.orderDetail.push(orderDetail); // フリーCSV order.free_csv = base64.encode(order.free_csv_input, 1); //チェックサム order.sps_hashcode = Sha1.hash( order.toString() ); feppost(order);}// オブジェクト定義[OrderDetail]function OrderDetail(){ this.toString = function() { var result = this.dtl_rowno + this.dtl_item_id + this.dtl_item_name + this.dtl_item_count + this.dtl_tax + this.dtl_amount + this.dtl_free1 + this.dtl_free2 + this.dtl_free3; return result; }}// オブジェクト定義[Order]function Order(){ this.orderDetail = new Array(); this.toString = function() { var resultOrderDetail = \"\"; for (i = 0; i &lt; this.orderDetail.length; i++) { resultOrderDetail = resultOrderDetail + this.orderDetail[i].toString(); } var result = this.pay_method + this.merchant_id + this.service_id + this.cust_code + this.sps_cust_no + this.sps_payment_no + this.order_id + this.item_id + this.pay_item_id + this.item_name + this.tax + this.amount + this.pay_type + this.auto_charge_type + this.service_type + this.div_settele + this.last_charge_month + this.camp_type + this.tracking_id + this.terminal_type + this.success_url + this.cancel_url + this.error_url + this.pagecon_url + this.free1 + this.free2 + this.free3 + this.free_csv + resultOrderDetail + this.request_date + this.limit_second + this.hashkey; return result; };}// 日時の取得function getYYYYMMDDHHMMSS(){ var now = new Date(); return now.getFullYear() + zeroPadding(now.getMonth() + 1) + zeroPadding(now.getDate()) + zeroPadding(now.getHours()) + zeroPadding(now.getMinutes()) + zeroPadding(now.getSeconds());}function zeroPadding(num) { if (num &lt; 10) { num = \"0\" + num; }\treturn num + \"\";}function feppost(order) { var connectUrl = \"https://stbfep.sps-system.com/f01/FepBuyInfoReceive.do\"; var form = $('&lt;form&gt;&lt;/form&gt;',{action:connectUrl,target:'receiver',method:'POST'}).hide(); var body = $('body'); body.append(form); form.append($('&lt;input&gt;',{type:'hidden',name:'pay_method' ,value:order.pay_method })); form.append($('&lt;input&gt;',{type:'hidden',name:'merchant_id' ,value:order.merchant_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'service_id' ,value:order.service_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'cust_code' ,value:order.cust_code })); form.append($('&lt;input&gt;',{type:'hidden',name:'sps_cust_no' ,value:order.sps_cust_no })); form.append($('&lt;input&gt;',{type:'hidden',name:'sps_payment_no' ,value:order.sps_payment_no })); form.append($('&lt;input&gt;',{type:'hidden',name:'order_id' ,value:order.order_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'item_id' ,value:order.item_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'pay_item_id' ,value:order.pay_item_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'item_name' ,value:order.item_name })); form.append($('&lt;input&gt;',{type:'hidden',name:'tax' ,value:order.tax })); form.append($('&lt;input&gt;',{type:'hidden',name:'amount' ,value:order.amount })); form.append($('&lt;input&gt;',{type:'hidden',name:'pay_type' ,value:order.pay_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'auto_charge_type' ,value:order.auto_charge_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'service_type' ,value:order.service_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'div_settele' ,value:order.div_settele })); form.append($('&lt;input&gt;',{type:'hidden',name:'last_charge_month' ,value:order.last_charge_month })); form.append($('&lt;input&gt;',{type:'hidden',name:'camp_type' ,value:order.camp_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'tracking_id' ,value:order.tracking_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'terminal_type' ,value:order.terminal_type })); form.append($('&lt;input&gt;',{type:'hidden',name:'success_url' ,value:order.success_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'cancel_url' ,value:order.cancel_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'error_url' ,value:order.error_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'pagecon_url' ,value:order.pagecon_url })); form.append($('&lt;input&gt;',{type:'hidden',name:'free1' ,value:order.free1 })); form.append($('&lt;input&gt;',{type:'hidden',name:'free2' ,value:order.free2 })); form.append($('&lt;input&gt;',{type:'hidden',name:'free3' ,value:order.free3 })); form.append($('&lt;input&gt;',{type:'hidden',name:'free_csv' ,value:order.free_csv })); form.append($('&lt;input&gt;',{type:'hidden',name:'request_date' ,value:order.request_date })); form.append($('&lt;input&gt;',{type:'hidden',name:'limit_second' ,value:order.limit_second })); form.append($('&lt;input&gt;',{type:'hidden',name:'hashkey' ,value:order.hashkey })); form.append($('&lt;input&gt;',{type:'hidden',name:'sps_hashcode' ,value:order.sps_hashcode })); for (i = 0; i &lt; order.orderDetail.length; i++) { form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_rowno' ,value:order.orderDetail[i].dtl_rowno })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_item_id' ,value:order.orderDetail[i].dtl_item_id })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_item_name' ,value:order.orderDetail[i].dtl_item_name })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_item_count' ,value:order.orderDetail[i].dtl_item_count })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_tax' ,value:order.orderDetail[i].dtl_tax })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_amount' ,value:order.orderDetail[i].dtl_amount })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_free1' ,value:order.orderDetail[i].dtl_free1 })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_free2' ,value:order.orderDetail[i].dtl_free2 })); form.append($('&lt;input&gt;',{type:'hidden',name:'dtl_free3' ,value:order.orderDetail[i].dtl_free3 })); } form.submit();}bước 3: Ajax call back $.ajax({ method: 'POST', data: { free_csv: order.free_csv_input }, url: urlGetFreeCsv }).done((encrypt) =&gt; { //チェックサム order.free_csv = encrypt; $.ajax({ method: 'POST', data: { text: order.toString() }, url: urlgenHashKey }).done((sps_hashcode) =&gt; { order.sps_hashcode = sps_hashcode; feppost(order); }); });3D-Secure callbackVì sử dụng 3Ds nên phía ngân hàng bắt buộc mình phải mã hoá một số nội dung liên quan đên thông tin tài khoản ( one time token ) bắt mình phải mã hoá dữ liệu theo thuật toán 3DES như sau:bước 1: Tạo spsHashcode public function generateSpsHashcode() { $data = $this-&gt;request-&gt;data; if (isset($data['text']) &amp;&amp; $data['text']) { $text = $data['text']; echo $this-&gt;generateSpsHashcode($text); } die; }private function generateSpsHashcode($string) { $hashKey = Configure::read('Purchase.hashKey'); return sha1($string.$hashKey); }bước 2: tạo free csvpublic function generateFreeCsv() { $data = $this-&gt;request-&gt;data; if (isset($data['free_csv']) &amp;&amp; $data['free_csv']) { $text = $data['free_csv']; echo $this-&gt;encrypt3DES($text);die; } die; }private function encrypt3DES($text) { $text = $this-&gt;padding3DES($text); $cipher = 'DES-EDE3-CBC'; $secretKey = Configure::read('Purchase.secretKey'); $iv = Configure::read('Purchase.iv'); if (in_array($cipher, openssl_get_cipher_methods())) { $ciphertext = openssl_encrypt($text, $cipher, $secretKey, $options=0, $iv); } return $ciphertext; }private function padding3DES($text) { $mod = strlen($text) % 8; if ($mod === 0) { return $text; } $padding = ''; for ($i = 0; $i &lt; 8 - $mod; $i++) { $padding .= ' '; } return $text.$padding; }Callback URL code Success Error Cancel Pagecon callback: Đây là url quan trọng để khi bạn xác thực thành công thông tin thẻ, ngân hàng sẽ request đến url này của bạn báo rằng KH có thể thanh toán được, ở đây mình phải tiến hành xử lí cho tài khoản đã thanh toán tuỳ vào nghiệp vụ của trang web, sau đó trả về OK nếu thành công hoặc NG nếu thất bại.Kết LuậnĐối với phương thức thanh toán bằng softbank, tôi cũng chỉ khái quát được nội dung và cách kết nối. Phần xử lý callback có lẽ mình sẽ viết kỹ hơn vào bài sau nếu có thời gian.Chú thích 3D-secure: is a protocol designed to be an additional security layer for online credit and debit card transactions. The name refers to the “three domains” which interact using the protocol: the merchant/acquirer domain, the issuer domain, and the interoperability domain." }, { "title": "Trình soạn thảo cho jekyll", "url": "/posts/jekyll-admin-simple-but-power-editor-for-jekyll/", "categories": "Blogging, Tutorial", "tags": "jekyll, rails, bugs", "date": "2021-07-18 00:00:00 +0700", "snippet": "Jekyll admin editorJekyll admin được cung cấp bới Opensource, với hệ thống support khá khiêm tốn hiện tại jekyll-admin đang dừng lại version 0.11.0 nhưng nếu chỉ là khiếm tốn thì tôi sẽ không giới ...", "content": "Jekyll admin editorJekyll admin được cung cấp bới Opensource, với hệ thống support khá khiêm tốn hiện tại jekyll-admin đang dừng lại version 0.11.0 nhưng nếu chỉ là khiếm tốn thì tôi sẽ không giới thiệu với các bạn. Nếu bạn đã sử dụng jekyll, một loại blog cá nhân được viết bằng markdown, tôi cũng đã giới thiệu trong một số bài viết trước đó.Jekyll admin là gì ?như tiêu đề của bài viết :heheƯu điểm Đơn giản Dễ sử dụng Ngắn gọn markdown support Configuration support Trình quản lý jekyll khá ổn .. mà vẫn thiếu thiếu, chả hiểu tại sao Hoàn thiện tốt với jekyll Cung cấp thẻ tags và thẻ categories(dùng làm menus) Giao diện khoa họcNhược điểmDo là một trang web quản lý nội dung bài việt của jekyll, nó không thể tránh khỏi những nhược điểm:Đơn giản Nhiều lỗi được báo không chính xác(còn tôi thì đoán chính xác là do ông nào try catch rồi vứt cái locale.yml …)Phiên bản hiện nay của jekyll-admin là 0.11.0 cũng đã lâu rồi so với jekyll, và với mỗi lần bạn update jekyll có thể sẽ gặp lỗi. \\Trong bài viết, tôi sẽ đề cập tới một lỗi ruby 2.4.0 jekyll 4.2.0 jekyll-admin 0.11.0Lỗi jekyll admin server cannot init configurationbạn fix thế này nhé, bước 1: chọn 1 editor, chọn vào thư mục .rvm hoặc rbenv nếu rbenv là trình quản lý ruby, chọn vào thư mục gems/lib/jekell-admin/server.rb bước 2: function configuration bạn chịnh sửa nội dung như sau: # Computed configuration, with updates and defaults # Returns an instance of Jekyll::Configuration def configuration @configuration ||= site.config.merge(overrides) if @configuration.is_a?(Hash) @configuration = @configuration.each_with_object(Jekyll::Configuration.new) { |(k, v), hsh| hsh[k] = v } end end bước 3: nếu bạn bảo sao tự dưng lại lỗi, ừ thì do phiên bản ruby ^^!" }, { "title": "Rails dynamic notifications by ajax", "url": "/posts/rails-dynamic-notifications/", "categories": "Fullstack, Rails", "tags": "rails, javascript, notification, ajax", "date": "2021-04-09 00:00:00 +0700", "snippet": "Controller def get_data_notification per_page = 5 page = params[:page].present? ? params[:page].to_i : 1 all_notiff = NotificationServices::GetList.new(current_account).run() @total_no...", "content": "Controller def get_data_notification per_page = 5 page = params[:page].present? ? params[:page].to_i : 1 all_notiff = NotificationServices::GetList.new(current_account).run() @total_notification = all_notiff.count @count_unread = all_notiff.where(mark_readed: false).count @notifications = all_notiff.page(page).per(per_page) @current_page_notification = page if page.present? @next_page_notification = @total_notification &gt; page*per_page ? page + 1 : nil else @next_page_notification = @total_notification &gt; per_page ? 2 : nil end end def index per_page = 5 page = params[:page].present? ? params[:page].to_i : 1 all_notiff = NotificationServices::GetList.new(current_account).run() @total_notification = all_notiff.count @count_unread = all_notiff.where(mark_readed: false).count @notifications = all_notiff.page(page).per(per_page) @current_page_notification = page if page.present? @next_page_notification = @total_notification &gt; page*per_page ? page + 1 : nil else @next_page_notification = @total_notification &gt; per_page ? 2 : nil end @page = page endERB page notification erb&lt;% if notifications.present? %&gt; &lt;div id=\"itemNotification\"&gt; &lt;% notifications.each.with_index do |nt, index| %&gt; &lt;% if index &gt; 0 || current_page_notification &gt; 1 %&gt; &lt;hr class=\"m-0\"&gt; &lt;% end %&gt; &lt;%= link_to mark_unread_pages_notification_path(nt), class: \"dropdown-item d-flex #{ 'unread-message' unless nt.mark_readed }\", style:\"position: relative;\" do %&gt; &lt;div class=\"mr-2\"&gt; &lt;div class=\"icon-circle text-center text-uppercase text-white bg-primary bottom-2px\" style=\"height: 30px; width: 30px;\"&gt; &lt;span class=\"position-relative font-size-12px\"&gt;&lt;%= parse_letter_name(nt.owner_first_name, nt.owner_last_name) if nt.owner_id.present? %&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;p class=\"font-size-16px font-weight-bold mb-1\"&gt;&lt;%= notification_title(nt.view_type, nt.team_name, nt.owner_fullname) %&gt;&lt;/p&gt; &lt;div class=\"clearfix\"&gt;&lt;/div&gt; &lt;p class=\"font-size-16px text-gray-500 mb-3\"&gt;\"&lt;%= truncate(notification_description(nt, nt.view_type, nt.team_name, nt.objective_name).html_safe, length: 40) %&gt;\"&lt;/p&gt; &lt;div class=\"font-size-16px text-gray-500\"&gt;&lt;%= parse_ago_time(nt.created_at, current_account.timezone) %&gt;&lt;/div&gt; &lt;/div&gt; &lt;% unless nt.mark_readed %&gt; &lt;div class=\"dot-unread-message\"&gt;&lt;/div&gt; &lt;% end %&gt; &lt;% end %&gt; &lt;% end %&gt; &lt;/div&gt;&lt;% elsif !defined?(@page) %&gt; &lt;div class=\"dropdown-item font-size-16px text-gray-500\"&gt; No notification &lt;/div&gt;&lt;% end %&gt;&lt;script&gt;&lt;/script&gt;1.1 pagination erb&lt;% if next_page_notification.present? %&gt;&lt;div id=\"itemPagination\" class=\"d-none\"style=\"cursor: pointer;\" onclick=\"getMoreNotification('&lt;% next_page_notification %&gt;')\" data-total=\"&lt;%= @total_notification %&gt;\"&gt; &lt;form&gt; &lt;hr class=\"m-0\"&gt; &lt;div class=\"clearfix\"&gt;&lt;/div&gt; &lt;div class=\"padding-y-10px text-center\"&gt; &lt;p class=\"m-0 has-spinner\" id=\"loadmore_btn\"&gt;&lt;/p&gt; &lt;/div&gt; &lt;/form&gt;&lt;/div&gt;&lt;script type=\"text/javascript\"&gt; var nextpage = &lt;%= @next_page_notification %&gt;; $(\"#dataNotification\").on(\"scroll\", function() { var total = &lt;%= @total_notification %&gt;; if ((nextpage - 1) * 5 &gt; total) return false; if($(this).scrollTop() + $(this).innerHeight() &gt;= $(this)[0].scrollHeight) { $(\"#itemPagination\").removeClass(\"d-none\"); var btn = $(\"#loadmore_btn\"); $(\"#loadmore_btn\").buttonLoader('start'); $.ajax({ type: 'GET', url: \"/pages/notifications\", data: { page: nextpage }, success: function(data) { $(\"#loadmore_btn\").buttonLoader('stop'); nextpage ++; } }); } }); (function($) { $.fn.buttonLoader = function(action) { var self = $(\"#loadmore_btn\"); if (action == 'start') { if ($(self).attr(\"disabled\") == \"disabled\") { } //disable buttons when loading state $('.has-spinner').attr(\"disabled\", \"disabled\"); $(self).attr('data-btn-text', $(self).text()); //binding spinner element to button and changing button text $(self).html('&lt;span class=\"spinner\"&gt;&lt;i class=\"fa fa-2x fa-spinner fa-spin\"&gt;&lt;/i&gt;&lt;/span&gt;'); $(self).addClass('active'); } //stop loading animation if (action == 'stop') { var self = $(\"#loadmore_btn\"); $(self).html(''); $(self).removeClass('active'); //enable buttons after finish loading $('.has-spinner').removeAttr(\"disabled\"); } } })(jQuery);&lt;/script&gt;&lt;% end %&gt;CSS.spinner { display: inline-block; opacity: 0; width: 0; -webkit-transition: opacity 0.25s, width 0.25s; -moz-transition: opacity 0.25s, width 0.25s; -o-transition: opacity 0.25s, width 0.25s; transition: opacity 0.25s, width 0.25s;}.has-spinner.active { cursor: progress;}.has-spinner.active .spinner { opacity: 1; width: auto;}.has-spinner.btn.active .spinner { min-width: 20px;}JS javascript for pagination $(\"#dataNotification\").append(\"&lt;%= j render 'layouts/shared/pages/item_notifications', notifications: @notifications, current_page_notification: @current_page_notification %&gt;\");$(\"#dataPagination\").empty().append(\"&lt;%= j render 'layouts/shared/pages/pagination_notifications', next_page_notification: @next_page_notification %&gt;\"); load js&lt;script type=\"text/javascript\"&gt; var nextpage = &lt;%= @next_page_notification %&gt;; $(\"#dataNotification\").on(\"scroll\", function() { var total = &lt;%= @total_notification %&gt;; if ((nextpage - 1) * 5 &gt; total) return false; if($(this).scrollTop() + $(this).innerHeight() &gt;= $(this)[0].scrollHeight) { $(\"#itemPagination\").removeClass(\"d-none\"); var btn = $(\"#loadmore_btn\"); $(\"#loadmore_btn\").buttonLoader('start'); $.ajax({ type: 'GET', url: \"/pages/notifications\", data: { page: nextpage }, success: function(data) { $(\"#loadmore_btn\").buttonLoader('stop'); nextpage ++; } }); } }); (function($) { $.fn.buttonLoader = function(action) { var self = $(\"#loadmore_btn\"); if (action == 'start') { if ($(self).attr(\"disabled\") == \"disabled\") { } //disable buttons when loading state $('.has-spinner').attr(\"disabled\", \"disabled\"); $(self).attr('data-btn-text', $(self).text()); //binding spinner element to button and changing button text $(self).html('&lt;span class=\"spinner\"&gt;&lt;i class=\"fa fa-2x fa-spinner fa-spin\"&gt;&lt;/i&gt;&lt;/span&gt;'); $(self).addClass('active'); } //stop loading animation if (action == 'stop') { var self = $(\"#loadmore_btn\"); $(self).html(''); $(self).removeClass('active'); //enable buttons after finish loading $('.has-spinner').removeAttr(\"disabled\"); } } })(jQuery);&lt;/script&gt;" }, { "title": "Add highlight input filed", "url": "/posts/add-highlight-input-filed/", "categories": "Fullstack, Stylesheets", "tags": "input, bootstrap, blue-highline", "date": "2021-04-08 00:00:00 +0700", "snippet": "textarea:focus,input[type=\"text\"]:focus,input[type=\"password\"]:focus,input[type=\"datetime\"]:focus,input[type=\"datetime-local\"]:focus,input[type=\"date\"]:focus,input[type=\"month\"]:focus,input[type=\"t...", "content": "textarea:focus,input[type=\"text\"]:focus,input[type=\"password\"]:focus,input[type=\"datetime\"]:focus,input[type=\"datetime-local\"]:focus,input[type=\"date\"]:focus,input[type=\"month\"]:focus,input[type=\"time\"]:focus,input[type=\"week\"]:focus,input[type=\"number\"]:focus,input[type=\"email\"]:focus,input[type=\"url\"]:focus,input[type=\"search\"]:focus,input[type=\"tel\"]:focus,input[type=\"color\"]:focus,.uneditable-input:focus { border-color: rgba(44, 130, 201, 1); box-shadow: 0 1px 1px rgba(0, 0, 0, 0.075) inset, 0 0 8px rgba(44, 130, 201, 1); outline: 0 none;}" }, { "title": "Rails validation form", "url": "/posts/rails-validation-form/", "categories": "Fullstack, Rails, Validation", "tags": "rails, html, javascript", "date": "2021-03-28 23:29:00 +0700", "snippet": "By HTML5 and Javascripteg, a submit formUsing setCustomValidity function by default js and oninput=\"checkPasscode(); , required on input field.&lt;form&gt;\t&lt;label for=\"passcode\"&gt;Enter Passco...", "content": "By HTML5 and Javascripteg, a submit formUsing setCustomValidity function by default js and oninput=\"checkPasscode(); , required on input field.&lt;form&gt;\t&lt;label for=\"passcode\"&gt;Enter Passcode:&lt;/label&gt;\t&lt;input id=\"passcode\" \t\t type=\"password\" \t\t placeholder=\"Your passcode\" \t\t oninput=\"checkPasscode();\"\t\t required/&gt;\t&lt;button type=\"submit\"&gt;Submit&lt;/button&gt;&lt;/form&gt;function checkPasscode() {\tvar passcode_input = document.querySelector(\"#passcode\");\t\tif (passcode_input.value != \"Ivy\") {\t\tpasscode_input.setCustomValidity(\"Wrong. It's 'Ivy'.\");\t} else {\t\tpasscode_input.setCustomValidity(\"\"); // be sure to leave this empty!\t\talert(\"Correct!\");\t}}" }, { "title": "Things after install ubuntu 20.04 for developer", "url": "/posts/things-after-install-ubuntu-2004-for-developer/", "categories": "Operation, Ubuntu", "tags": "ubuntu, ubuntu20.04, setup", "date": "2021-03-27 00:00:00 +0700", "snippet": "First stepHere is download file of the install scripFor a quick setup, I recommend that you create a file called install.sh and run the following command every time you install:chmod a+x ./install...", "content": "First stepHere is download file of the install scripFor a quick setup, I recommend that you create a file called install.sh and run the following command every time you install:chmod a+x ./install.sh./install.shFirst step we should update repository ubuntu:sudo apt-get updatesudo apt-get upgrade -yUbuntu packages for developersudo apt-get install build-essential -ysudo apt-get install ubuntu-restricted-extras -yAdd Ubuntu repositorysudo apt-add-repository universe #universe packagessudo add-apt-repository ppa:ondrej/php #php ppasudo add-apt-repository ppa:gerardpuig/ppa # ubuntu cleanersudo add-apt-repository ppa:otto-kesselgulasch/gimp # gimpsudo add-apt-repository ppa:openshot.developers/ppa # openshotqtsudo add-apt-repository -y ppa:teejee2008/ppa #Ubuntu package and toolssudo apt-get install build-essential -ysudo apt-get install ubuntu-restricted-extras -ysudo apt-get install gnome-tweak-tool -ysudo apt-get install synaptic -ywget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.debsudo apt install ./google-chrome-stable_current_amd64.debsudo snap install vlc -ysudo snap install skype -ysudo apt-get install ubuntu-cleaner -ysudo apt-get install simplescreenrecorder -ysudo apt-get install gimp -ysudo apt-get install cool-retro-term -ysudo apt-get install openshot-qt -ysudo apt-get install gnome-tweaks -ysudo apt-get install chrome-gnome-shell -ysudo apt-get install firefox-gnome-shell -ysudo apt-get install rar unrar p7zip-full p7zip-rar -ysudo apt-get install timeshift -ysudo apt-get install ibus-unikey -yibus restartJava installsudo apt-get install openjdk-11-jdk -yLAMP installsudo apt install apache2 -ysudo ufw allow in \"Apache\"sudo apt-get install -y mysql-server mysql-client libmysqlclient-dev libpq-devsudo apt install php libapache2-mod-php php-mysql -ysudo apt-get install libgdbm-dev libncurses5-dev automake libtool bison libffi-devInstall Composer and PHP7.4curl -sS https://getcomposer.org/installer -o composer-setup.phpphp -r \"if (hash_file('SHA384', 'composer-setup.php') === '$HASH') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;\"sudo php composer-setup.php --install-dir=/usr/local/bin --filename=composersudo apt-get install -y zip unzip software-properties-commonsudo apt-get install -y php7.4 php7.4-gd php7.4-mbstring php7.4-xml php-zip php7.4-mysqlGit and Configuresudo apt-get install gitgit config --global color.ui truegit config --global user.name \"\"git config --global user.email \"\"ssh-keygen -t rsa -b 4096 -C \"\"MysqlConfigure mysql passwordsudo mysql_secure_installationRuby On Railsgpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDBcurl -sSL https://get.rvm.io | bash -s stablesource ~/.rvm/scripts/rvmsruby -vrvm install 3.0.0rvm use 3.0.0gem install bundlerPythonsudo apt-get install -y python2curl https://bootstrap.pypa.io/get-pip.py --output get-pip.pysudo python2 get-pip.pypip3 install scrapysudo apt-get install -y python3-pipNodejssudo apt install npm -ycurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh | bashnvm list-remotenvm install v14.0.0nvm use v14.0.0npm install --global yarnDocker# Update ap-certificatessudo apt-get updatesudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-releasecurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get updatesudo apt-get remove docker docker-engine docker.io containerd runcsudo apt-get install -y docker-ce docker-ce-cli containerd.iosudo apt-get install -y py-pip python3-dev libffi-dev openssl-dev gcc libc-dev rust cargo makesudo usermod -aG docker ${USER}Install Docker Composesudo curl -L \"https://github.com/docker/compose/releases/download/1.28.6/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composesudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-composesource ~/.bashrcDocker Swampsudo apt install -y apt-transport-https ca-certificates curl software-properties-commoncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"sudo apt-get updateapt-cache policy docker-cesudo apt install docker-ceAfter install docker swarm, first step is configure nodesudo vim /etc/hostsAdd following host192.168.1.10\tmanager192.168.1.11\tworker-01192.168.1.12\tworker-02Create Docker Swarm Clustersudo docker swarm init --advertise-addr 192.168.1.10Swarm initialized: current node (fsuaqqpihi2eabmmq8gldzhpv) is now a manager.To add a worker to this swarm, run the following command:sudo docker swarm join --token SWMTKN-1-018kvdektwa74z8fajb5c1u6jyz6qfk4ood8u4qotw7go9jj0p-cfpnh7omy86xcgoh45vau2kaj 192.168.1.10:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.Postgresqlsudo apt install postgresql postgresql-contrib -ysudo curl https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo apt-key addsudo sh -c 'echo \"deb https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/$(lsb_release -cs) pgadmin4 main\" &gt; /etc/apt/sources.list.d/pgadmin4.list &amp;&amp; apt update'sudo apt install pgadmin4 -yConfigure PostgresqlBy default, Postgres uses a concept called “roles” to handle authentication and authorization. These are, in some ways, similar to regular Unix-style users.The installation procedure created a user account called postgres.sudo -i -u postgrespsqlTo exit use this command\\q Create User createuser --interactive Another way if you like linux command```bashsudo -u postgres createuser –interactive –pwpromptOutputEnter name of role to add: sammyShall the new role be a superuser? (y/n) y Create DB createdb sammy Linux Command sudo -u postgres createdb sammy Sync with LinuxTo log in with ident based authentication, you’ll need a Linux user with the same name as your Postgres role and database. sudo adduser sammy Now you can login by this command: sudo -i -u sammypsql Or by that way sudo -u sammy psql full login: psql -h myhost -d mydb -U myuser After tolig you can switch DB by psql -d postgresTo check about connaction: \\conninfoHow To Install PostgreSQL on Ubuntu 20.04 [Quickstart] Service sudo systemctl stop postgresqlsudo systemctl disable postgresql MongoDBwget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -sudo apt-get install gnupgwget -qO - https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -echo \"deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.listsudo apt-get updatesudo apt-get install -y mongodb-orgsudo systemctl disable mongodJekyll#jekyllsudo snap install ruby --classicecho '# Install Ruby Gems to ~/gems' &gt;&gt; ~/.bashrcecho 'export GEM_HOME=\"$HOME/gems\"' &gt;&gt; ~/.bashrcecho 'export PATH=\"$HOME/gems/bin:$PATH\"' &gt;&gt; ~/.bashrcgem install jekyll bundlerTerminal ToolZSHsudo apt-get install zsh -ysh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"echo '# Install Ruby Gems to ~/gems' &gt;&gt; ~/.zshrcecho 'export GEM_HOME=\"$HOME/gems\"' &gt;&gt; ~/.zshrcecho 'export PATH=\"$HOME/gems/bin:$PATH\"' &gt;&gt; ~/.zshrcsudo apt-get install vim -ysudo apt-get install konsole terminator -ychsh -s $(which zsh)Fishsudo apt-get install fish fish-common ondir zsh-syntax-highlightingSoftwareVideo Edit - PEEKsudo add-apt-repository ppa:peek-developers/stablesudo apt updatesudo apt install peekQA tester - screen record and capturesudo apt install flameshot kazam -yGit, PHP storm, gitkraken, postman, utube, hollywordsudo apt install git-flow git-cola -ysudo snap install gitkraken --classicsudo snap install postmansudo snap install phpstorm --classicsudo snap install utubesudo apt-get install hollywoodgnome weathersudo add-apt-repository ppa:gnome-shell-extensionssudo apt-get updatesudo apt-get install gnome-shell-extension-weathersudo dpkg -i mysql-workbench-community-dbgsym_8.0.23-1ubuntu20.04_amd64.debClean APT and configure source list rechecksudo apt-get autocleansudo apt-get autoremovesoftware-properties-gtk" }, { "title": "Cài đặt môi trường Ruby on Rails trên ubuntu 20.04", "url": "/posts/cai-dat-moi-truong-ruby-on-rails-tren-ubuntu/", "categories": "Framework, Rails", "tags": "rails, git, postgresql, mysql", "date": "2021-03-22 00:00:00 +0700", "snippet": "Cài đặt môi trường Ruby on Rails trên ubuntuCài đặt truyền thống RubyĐể cài đặt môi trường code ruby trên ubuntu rất đơn giản vì mặc định ubuntu đã cài đặt sẵn ruby theo từng phiên bản của ubuntu.r...", "content": "Cài đặt môi trường Ruby on Rails trên ubuntuCài đặt truyền thống RubyĐể cài đặt môi trường code ruby trên ubuntu rất đơn giản vì mặc định ubuntu đã cài đặt sẵn ruby theo từng phiên bản của ubuntu.ruby -vTuy nhiên đa số phiên bản ruby này là ruby core để chạy các ứng dụng và chương trình của linux nên không thuận tiện cho chúng ta. Cài đặt lại ruby tại trangsudo apt install curlcurl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash -curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.listsudo apt-get updatesudo apt-get install git-core zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev nodejs yarnCài đặt qua công cụ quản lý RubyViệc sử dụng công cụ quản lý rails giúp ta dễ dàng chuyển đổi qua các phiên bản rails theo từng dự án cài đặt tương ứng với một phiên bản ruby cần sử dụng,Thông thường file .ruby-version sẽ cho biết phiên bản ruby và nếu cài đặt công cụ quản lý ruby sẽ tự động cài đặt như ta mong muốn.1. rbenvcài đặt thông qua câu lệnh rbenvcdgit clone https://github.com/rbenv/rbenv.git ~/.rbenvecho 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bashrcecho 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bashrcexec $SHELLgit clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-buildecho 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' &gt;&gt; ~/.bashrcexec $SHELLrbenv install 3.0.0rbenv global 3.0.0ruby -v2. rvmrvm cũng tương tự nhưng khác câu lệnh với rbenvsudo apt-get install libgdbm-dev libncurses5-dev automake libtool bison libffi-devgpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDBcurl -sSL https://get.rvm.io | bash -s stablesource ~/.rvm/scripts/rvmrvm install 3.0.0rvm use 3.0.0 --defaultruby -vCài đặt Railsgem install rails -v 6.1.1Đối với rbenv, cần rehash lạirbenv rehashKiểm tra lại rails vớirails -vcài đặt bundlergem install bundlerCài đặt DatabasePostgressqlbước 1: cài đặt thư việnsudo apt install postgresql-11 libpq-devbước 2: cài đặt passwordsudo -u postgres createuser zmerrychristmas -s# If you would like to set a password for the user, you can do the followingsudo -u postgres psqlpostgres=# \\password 123456Mysqlsudo apt-get install mysql-server mysql-client libmysqlclient-devCài đặt Gitsudo apt-get install gitCấu hình gitgit config --global color.ui truegit config --global user.name \"YOUR NAME\"git config --global user.email \"YOUR@EMAIL.com\"ssh-keygen -t rsa -b 4096 -C \"YOUR@EMAIL.com\"Cầu hình git với ssh-keyssh-keygen -t ed25519 -C \"your_email@example.com\"cat ~/.ssh/id_rsa.pubssh -T git@github.com # test connection Kiểm tra lạiTạo một project với rails#### If you want to use SQLite (not recommended)rails new myapp#### If you want to use MySQLrails new myapp -d mysql#### If you want to use Postgres# Note that this will expect a postgres user with the same username# as your app, you may need to edit config/database.yml to match the# user you created earlierrails new myapp -d postgresql# Move into the application directorycd myapp# If you setup MySQL or Postgres with a username/password, modify the# config/database.yml file to contain the username/password that you specified# Create the databaserake db:createrails server" }, { "title": "Bug thần thánh permission denied @ rb_sysopen", "url": "/posts/bug-rails-gem-server-sysopen/", "categories": "Framework, Rails, Bug", "tags": "rails, bug, rvm", "date": "2021-03-22 00:00:00 +0700", "snippet": "Bug rack/server.rb:362:in initialize: Permission denied @ rb_sysopen (Errno::EACCES)phiên bản sử dụng là rvmTry first update gem ownersudo chown -R userowner:yourgroup ~/.rvmUpdate permissionrvmsu...", "content": "Bug rack/server.rb:362:in initialize: Permission denied @ rb_sysopen (Errno::EACCES)phiên bản sử dụng là rvmTry first update gem ownersudo chown -R userowner:yourgroup ~/.rvmUpdate permissionrvmsudo rvm get stable --auto-dotfilesrvm fix-permissions systemupdate user in group rvmrvm group add rvm $USERÌf not work try update Gemgem update gem cleanup" }, { "title": "Tạo Rails API với Grape", "url": "/posts/Tao-rails-api-voi-grape/", "categories": "Framework, Rails, api, grape", "tags": "rails, api, grape", "date": "2021-03-22 00:00:00 +0700", "snippet": "Trong việc module hóa cũng như tạo cấu trúc REST API bạn có thể tự tạo cho mình một framework riêng hoặc sử dụng một framework đã có và đang được phát triển và ưa dùng là grape. tham khảo tại đâyGr...", "content": "Trong việc module hóa cũng như tạo cấu trúc REST API bạn có thể tự tạo cho mình một framework riêng hoặc sử dụng một framework đã có và đang được phát triển và ưa dùng là grape. tham khảo tại đâyGrape api là gì ?Grape is a REST-like API framework for Ruby. It's designed to run on Rack or complement existing web application frameworks such as Rails and Sinatra by providing a simple DSL to easily develop RESTful APIs. It has built-in support for common conventions, including multiple formats, subdomain/prefix restriction, content negotiation, versioning and much more. api frame work Restful Apis Rack, Sinatra support built-in supportNói gọn lại là Grape là một framework tích hợp và ăn nhập với các server như rack, sinsatra để tạo ra hệ thống Restful APIs. Ưu điểm là nó rất nhanh hơn các rails-api thông thường.Trong bài này tôi sẽ giới thiệu và demo một ứng dụng sử dụng grape api.Với mong muốn đi sâu hơn vào grape và ứng dụng của grape với rails nên tôi sẽ có thể viết nối tiếp một số bài nữa.Cài đặtTrong gemfilegem 'grape'Run bundle : bundle updateController examplemodule Twitter class API &lt; Grape::API version 'v1', using: :header, vendor: 'twitter' format :json prefix :api helpers do def current_user @current_user ||= User.authorize!(env) end def authenticate! error!('401 Unauthorized', 401) unless current_user end end resource :statuses do desc 'Return a public timeline.' get :public_timeline do Status.limit(20) end desc 'Return a personal timeline.' get :home_timeline do authenticate! current_user.statuses.limit(20) end desc 'Return a status.' params do requires :id, type: Integer, desc: 'Status ID.' end route_param :id do get do Status.find(params[:id]) end end desc 'Create a status.' params do requires :status, type: String, desc: 'Your status.' end post do authenticate! Status.create!({ user: current_user, text: params[:status] }) end desc 'Update a status.' params do requires :id, type: String, desc: 'Status ID.' requires :status, type: String, desc: 'Your status.' end put ':id' do authenticate! current_user.statuses.find(params[:id]).update({ user: current_user, text: params[:status] }) end desc 'Delete a status.' params do requires :id, type: String, desc: 'Status ID.' end delete ':id' do authenticate! current_user.statuses.find(params[:id]).destroy end end endendCấu trúc thư mục sơ cấp app |––controllers |––api mount một file base.rb module API class Base &lt; Grape::API mount API::V1::Base endend cấu trúc thư mục app |––controllers |––api |––base.rb Khái niệm cơ bảnmountmount: là cơ chế nói cho rails biết là grape đang tạo ra số lượng api tương ứng với fileTwitter::API.compile!Đối với rails để mount một file và tạo ra routes ta cần làm như sau: rails &lt; 5.2sửa lại file application.rb config.paths.add File.join('app', 'api'), glob: File.join('**', '*.rb')config.autoload_paths += Dir[Rails.root.join('app', 'api', '*')] rails thêm vào file config/routes.rb mount Twitter::API =&gt; '/' rails 6.0 sửa file config/initializers/inflections.rb ActiveSupport::Inflector.inflections(:en) do |inflect|inflect.acronym 'API'end Trong việc mount là một cơ chế kỳ ảo của grape đảm bảo việc nested và mở rộng module hóa cũng quản lý version. versionTừ version 1 bạn có thể phát triển lên version 2, 3. Grape dễ dàng quản lý điều đó dựa trên các câu lệnh cùng thư mụcapp |––controllers |––api |––base.rb |––v1 |––base.rbCác khái niệm cơ bản:There are four strategies in which clients can reach your API’s endpoints: :path, :header, :accept_version_header and :param. The default strategy is :path. pathversion 'v1', using: :path use with curl: curl http://localhost:9292/v1/statuses/public_timeline header version 'v1', using: :header, vendor: 'twitter' use with curl: curl -H Accept:application/vnd.twitter-v1+json http://localhost:9292/statuses/public_timeline accept_version_header version 'v1', using: :accept_version_header example: curl -H \"Accept-Version:v1\" http://localhost:9292/statuses/public_timeline param version 'v1', using: :param, parameter: 'v' ex: curl http://localhost:9292/statuses/public_timeline?v=v1 version, mount, nestedGiả sử bạn cần mở rộng module v1 bằng việc khải báo thêm các api.bước 1: Tạo folder v1app |––controllers |––api |––base.rb |––v1bước 2: với mỗi version tạo một base.rb riêng:app |––controllers |––api |––base.rb |––v1 |––base.rbbước 3: Trong file base khai báo mount thư mục đồng cấpmodule API module V1 class Base &lt; Grape::API mount V1::Users # mount API::V1::AnotherResource end endendbước 4: khai báo api tương ứng user.rbapp |––controllers |––api |––base.rb |––v1 |––base.rb |––users.rbapi end pointNhư ở trên khi bạn khai báo 1 api đồng cấp, đó cũng là api end point.Đối với một api end point:module API module V1 class Users &lt; Grape::API include API::V1::Defaults resource :users do desc \"Return all users\" get \"\", root: :users do User.all end desc \"Return a user\" params do requires :id, type: String, desc: \"ID of the user\" end get \":id\", root: \"user\" do User.where(id: params[:user_id]).first end end end endend resource: khai báo router desc: mô tả get \"\", root: :users do khai báo url api lấy toàn bộ userex: http://localhost:3000/api/v1/userstool: POSTMANAction, HelperMixin, format dữ liệu, các hàm dùng chungđược khai báo tại file defaults.rb, ex: app/controllers/api/v1/defaults.rb khai báo định dạng output như json các hàm như authencation. module API module V1 module Defaults extend ActiveSupport::Concern included do prefix \"api\" version \"v1\", using: :path default_format :json format :json formatter :json, Grape::Formatter::ActiveModelSerializers helpers do def permitted_params @permitted_params ||= declared(params, include_missing: false) end def logger Rails.logger end end # check authentice_user def authenticate_user! uid = request.headers[\"Uid\"] token = request.headers[\"Access-Token\"] @current_user = User.find_by(uid: uid) unless @current_user &amp;&amp; @current_user.valid_token?(token) api_error!(\"You need to log in to use the app.\", \"failure\", 401, {}) end end\t\t # Hàm hiển thị errors message khi lỗi def api_error!(message, error_code, status, header) error!({message: message, code: error_code}, status, header) end # # Hàm raise errors message khi lỗi def api_error_log(message) @logger ||= Logger.new(ProjectLogger.log_path(\"project_api\")) @logger.info(\"=============#{Time.zone.now.to_s}==================\\n\") @logger.info(\"#{message}\\n\") end rescue_from ActiveRecord::RecordNotFound do |e| error_response(message: e.message, status: 404) end rescue_from ActiveRecord::RecordInvalid do |e| error_response(message: e.message, status: 422) end rescue_from Grape::Exceptions::ValidationErrors do |e| error_response(message: e.message, status: 400) end end endendend Action callbackGrape vấn support đầy đủ các hàm call back như:\t1. before\t2. before_validation\t3. validations\t4. after_validation\t5. the API call\t6. after\t7. finallyExample:class MyAPI &lt; Grape::API get '/' do \"root - #{@blah}\" end namespace :foo do before do @blah = 'blah' end get '/' do \"root - foo - #{@blah}\" end namespace :bar do get '/' do \"root - foo - bar - #{@blah}\" end end endendkết quảGET / # 'root - 'GET /foo # 'root - foo - blah'GET /foo/bar # 'root - foo - bar - blah'example versionclass Test &lt; Grape::API resource :foo do version 'v1', :using =&gt; :path do before do @output ||= 'v1-' end get '/' do @output += 'hello' end end version 'v2', :using =&gt; :path do before do @output ||= 'v2-' end get '/' do @output += 'hello' end end endendkết quảGET /foo/v1 # 'v1-hello'GET /foo/v2 # 'v2-hello'RspecViết rồi kiểm thử làm sao ?-&gt; viết rspec cho api ta có thể dùng gem airborne.install:gem install airbornecấu trúc thư mụcspec |––api |––v1 |––users_specnội dung file test:require \"rails_helper\"require \"airborne\"describe \"API::V1::Users\" do after(:all){I18n.locale = :ja} describe \"POST api/v1/users\" do let!(:user) do FactoryGirl.create :user, id: 1, email: \"test@gmail.com\", first_name: \"James\", last_name: \"Bond\", provider: \"email\" end context \"when user update successfully\" do let(:api_response){FactoryGirl.build(:api_update_user_success_response).deep_symbolize_keys} before do post(\"/api/v1/users\", {first_name: \"hitorri\"}, {\"Accept-Language\": \"en\", \"App-Version\": \"knt/1.0\", \"Uid\": user.uid, \"Access-Token\": user.access_token}) end it{expect_json(api_response)} end end endCORSusing gem rack-cors, cấu hình tại file config.rurequire 'rack/cors'use Rack::Cors do allow do origins '*' resource '*', headers: :any, methods: :get endendrun Twitter::APISerializingCông việc của chúng ta là convert mảng sang JSON. Để làm được việc này chúng ta cài thêm gem grape-active_model_serializersmodule API module V1 module Defaults extend ActiveSupport::Concern included do prefix \"api\" version \"v1\", using: :path default_format :json format :json formatter :json, Grape::Formatter::ActiveModelSerializers ...Viết file serializersCreate a directory, serializers, in the top level of your app. Create a graduate_serializer.rb file in that directory. Here is where our serializer will live.app/serializers/graduate_serializer.rbclass GraduateSerializer &lt; ActiveModel::Serializer attributes :id, :first_name, :last_name, :cohort, :current_job, :bio, :news, :website, :picture, :created_at, :updated_atendGrape Gem additionGrape Swagger Add gem ‘grape-swagger’ to your Gemfile and bundle install. Add grape-swagger documentation to the root or base class of your API.tại file app/controllers/api/v1/base.rb thêm đoạn sau: require \"grape-swagger\" ... add_swagger_documentation( api_version: \"v1\", hide_documentation_path: true, mount_path: \"/api/v1/swagger_doc\", hide_format: true ) end Add the documentation endpoint to your routes.tại file config/routes.rb: mount GrapeSwaggerRails::Engine, at: \"/documentation\" vào url: http://localhost/documentation bạn sẽ redirect tới: http://localhost:3000/api/v1/swagger_doc Grape EntityCó thể tham khảo thêm tại đâyGrape LoggerCó thể tham khảo tại đâyGrape Swagger representableCó thể tham khảo thêm tại đâyTài liệu tham khảo https://viblo.asia/p/xay-dung-api-voi-grape-bWrZne1vKxw https://www.thegreatcodeadventure.com/making-a-rails-api-with-grap/ https://github.com/ruby-grape/grape#what-is-grape" }, { "title": "Tạo Blog đơn giản với Github và Jekyll", "url": "/posts/tao-blog-don-gian-voi-jekyll/", "categories": "Blogging, Tutorial, Github", "tags": "github, jekyll", "date": "2021-03-20 00:00:00 +0700", "snippet": "Xây dựng Blog cá nhân với Github và JekyllĐầu tiên nếu đối với mỗi ai trong chúng ta có niềm yêu thích muốn xây dựng cho mình một blog, blog đó là nơi để chia sẻ những kinh nghiệm đời sống và công ...", "content": "Xây dựng Blog cá nhân với Github và JekyllĐầu tiên nếu đối với mỗi ai trong chúng ta có niềm yêu thích muốn xây dựng cho mình một blog, blog đó là nơi để chia sẻ những kinh nghiệm đời sống và công việc cũng có thể là một kênh thông tin để truyền tải thông điệp và lưu giữ nhưng bài học của bạn trong hành trình cuộc sống không đơn giản này.Tôi 30 tuổi và đang là một lập trình viên, tôi chưa gia đình, trong cuộc sống bộn bề và những suy nghĩ còn chưa đến đâu khi nhìn về tương lai và quá khứ đã qua một thứ luôn thúc đẩy tôi đó là tạo 1 blog để chia sẻ và viết lại nhưng dòng code, những dòng tâm sự dài dòng mà phải cô đọng.Chuẩn bị về kiến thức blog đã nàoĐiều bạn cần chuẩn bị cho một blog là một chiếc bàn phím với tối thiểu 44 phím cần thiết cho việc viết lách, một cái đầu vui vẻ sau những phút thảnh thơi giữa cuộc sống bộn bề của bạn.Về thứ hai, do bây giờ để không dài dòng tôi có thể giới thiệu bạn tới một số trang blog miễn phí trên mạng như Wordpress một trang mã nguồn mở cung cấp rất nhiều công cụ quản lý và giao diện đồ sộ, điều tôi không thích là nó quá rườm rà và khá chậm. Tumblr một trang mạng xã hội nhưng vẫn cho phép bạn tạo ra nhưng bài blog có chủ đề và đi kèm với nó là mạng lưới chia sẻ của Tumblr, chắc bạn sẽ phải ngặc nhiên khi có không ít nhưng cây viết Việt trên nền tảng mạng xã hội Tumblr. Bloger Một trang bloger được cung cấp với công cụ quản lý cùng với kho theme rộng rãi, một domain tuy không nhanh nhưng băng thông không giới hạn đủ để bạn có thể thoải mái sáng tạo. Facebook là một trang mạng xã hội, chúng ta có thể chia sẻ rất nhiều chủ đề bài viết, các bạn đừng nghĩ tôi đang nói đến một trang cá nhân vì chuyện đó quá đơn giản mà ở đây đẻ tạo một blog bạn cần tạo một tài khoản facebook sau đó tạo pages facebook, hiện tại có rất nhiều trang facebook theo từng chủ đề như chính trị, giải trí, truyền thông và có thể blog đơn giản của những tay viết nghiệp dư cho đến chuyên nghiệp. WIX Một trang web đang có lượng user đăng ký khá lớn cung cấp domain miễn phí. Quora Một mạng chia sẻ xã hội và những câu hỏi cùng giải đáp rất ý nghĩa Medium Medium là một sân chơi lớn hơn và cũng là một sân chơi đáng giá nơi bạn có thể chia sẻ các bài viết và ý tướng của mình, ở đây bạn có thể nhận được một khoản nhuận bút nếu bài viết của bạn hay và được rating cao. Ghost là một trang tạo blog content chuyên nghiệp và hấp dẫn từ kho giao diện cũng như là cung cấp nhưng công cụ quản lý điều duy nhất tôi không thích là bạn sẽ phải trả phí cho muộn public domain lên trên trang web Đây là cách truyền thống, bạn tạo ra website rồi update lên một hosting bất kỳ, có rất nhiều framework hỗ trợ bạn như wordpress, drupal, cakePHP, Yii, Laravel. Hosting bạn có thể tìm đến rất nhiều nhà cung cấp domain tại việt nam cũng như ở singapore với giá rẻ, bạn có thể phải trả phí cho 1 - 2 năm, giá khoảng 2.7 ~ 3 triệu vnđ. Github Github là công cụ quản lý source code mã nguồn mở đang được ưa dùng nhiều nhất hiện nay, chắc bạn cũng từng vài lần qua download nhưng mã nguồn mở hoặc upload các project của bản thân qua đây cũng là lời giới thiệu với mọi người biết bạn là một lập trình viên như nào về tư tưởng cũng như sự đam mê với lập trình. Tôi cũng trong số các bạn thích github và vẫn thường dạo quanh các repository để tìm hiểu có thể chỉ là một plugin js, css hoặc một dự án về docker cho đến các trang web lớn với các mục địch khác nhau đều được phổ biến tại trang web quản lý này. Đối với mỗi người dùng github bạn chỉ có thể tạo ra một website duy nhất để chia sẻ các dự án và kinh nghiệm của mình, sẽ hợp lý hơn là bạn viết nhưng bài giới thiệu và chủ đề bạn quan tâm đối với mỗi repositories hoặc giả sử bạn là một lập trình viên yêu thích lập trình cần có một trang web hiện đại nhanh chóng được tự cấu hình và lập trình riêng cho nó thi bạn nên chọn lựa github pages. Ở bài viết này tôi sẽ giới thiệu bạn tạo github pages với jekyll một trang sinh mã nguồn statics.Đăng ký Github pagesBước 1: Tạo repository dưới dạng usernameweb.github.io repository 1 tài khoản chỉ tạo được 1 blog.Bước 2: Trong tab setting chọn branchBước 3: Chọn theme nếu bạn dùng kho theme miễn phí của githubBước 4: Clone source code về và init projectgit clone git-ssh-linkcd git-ssh-linkTrong thư mục này bạn có thể tạo ra một trang giao diện tùy ý, github pages support js và javascript nhưng không support php hoặc các ngôn ngữ script khác như nodejs, rails, python.Bước 5: Khởi tạo jekyll như trang blog quản lýTừ bước này bạn sẽ có thể hình dung về trang blog của mình sẽ như thế nào rồi đấy.có thể vào bài này để làm tiếp bước này nhé.Bước 6: Trang quản lý jekyll Tham khảo github của nhà phát triển. Add the following to your site’s Gemfile: ruby gem 'jekyll-admin', group: :jekyll_plugins Run bundle instalBước 7: Cài đặt theme cho jekyll(tùy bạn nhé!)các bạn vào Đây để tham khảo kho theme của jekyll.Bước tiếp theo ? Làm sao để viết blog !Để viết một blog hay và có nội dung không phải khó nhưng cũng là một chuyện liên quan đến lối văn chương và kiến thức của từng người. Tôi không biết phải viết thế nào :) , những bài viết của tôi vẫn sẽ cung cấp cho các bạn những điều mà tôi muốn ghi lại tại đây và chia sẻ. Geting started Writing a new post Text and typography Tutorial" }, { "title": "Enable Google Page Views", "url": "/posts/enable-google-pv/", "categories": "Blogging, Tutorial", "tags": "google analytics, pageviews", "date": "2021-01-04 06:32:00 +0700", "snippet": "This post is to enable Page Views on the Chirpy theme based blog that you just built. This requires technical knowledge and it’s recommended to keep the google_analytics.pv.* empty unless you have ...", "content": "This post is to enable Page Views on the Chirpy theme based blog that you just built. This requires technical knowledge and it’s recommended to keep the google_analytics.pv.* empty unless you have a good reason. If your website has low traffic, the page views count would discourage you to write more blogs. With that said, let’s start with the setup.Set up Google AnalyticsCreate GA account and propertyFirst, you need to set up your account on Google analytics. While you create your account, you must create your first Property as well. Head to https://analytics.google.com/ and click on Start Measuring Enter your desired Account Name and choose the desired checkboxes Enter your desired Property Name. This is the name of the tracker project that appears on your Google Analytics dashboard Enter the required information About your business Hit Create and accept any license popup to set up your Google Analytics account and create your propertyCreate Data StreamWith your property created, you now need to set up Data Stream to track your blog traffic. After you signup, the prompt should automatically take you to create your first Data Stream. If not, follow these steps: Go to Admin on the left column Select the desired property from the drop-down on the second column Click on Data Streams Add a stream and click on Web Enter your blog’s URLIt should look like this:Now, click on the new data stream and grab the Measurement ID. It should look something like G-V6XXXXXXXX. Copy this to your _config.yml file:google_analytics: id: 'G-V6XXXXXXX' # fill in your Google Analytics ID # Google Analytics pageviews report settings pv: proxy_endpoint: # fill in the Google Analytics superProxy endpoint of Google App Engine cache_path: # the local PV cache data, friendly to visitors from GFW regionWhen you push these changes to your blog, you should start seeing the traffic on your Google Analytics. Play around with the Google Analytics dashboard to get familiar with the options available as it takes like 5 mins to pick up your changes. You should now be able to monitor your traffic in real time.Setup Page ViewsThere is a detailed tutorial available to set up Google Analytics superProxy. But, if you are interested to just quickly get your Chirpy-based blog display page views, follow along. These steps were tested on a Linux machine. If you are running Windows, you can use the Git bash terminal to run Unix-like commands.Setup Google App Engine Visit https://console.cloud.google.com/appengine Click on Create Application Click on Create Project Enter the name and choose the data center close to you Select Python language and Standard environment Enable billing account. Yeah, you have to link your credit card. But, you won’t be billed unless you exceed your free quota. For a simple blog, the free quota is more than sufficient. Go to your App Engine dashboard on your browser and select API &amp; Services from the left navigation menu Click on Enable APIs and Services button on the top Enable the following APIs: Google Analytics API On the left, Click on OAuth Consent Screen and accept Configure Consent Screen. Select External since your blog is probably hosted for the public. Click on Publish under Publishing Status Click on Credentials on the left and create a new OAuth Client IDs credential. Make sure to add an entry under Authorized redirect URIs that matches: https://&lt;project-id&gt;.&lt;region&gt;.r.appspot.com/admin/auth Note down the Your Client ID and Your Client Secret. You’ll need this in the next section. Download and install the cloud SDK for your platform: https://cloud.google.com/sdk/docs/quickstart Run the following commands: [root@bc96abf71ef8 /]# gcloud init~snip~Go to the following link in your browser: https://accounts.google.com/o/oauth2/auth?response_type=code&amp;client_id=XYZ.apps.googleusercontent.com&amp;redirect_uri=ABCDEFGEnter verification code: &lt;VERIFICATION CODE THAT YOU GET AFTER YOU VISIT AND AUTHENTICATE FROM THE ABOVE LINK&gt;You are logged in as: [blah_blah@gmail.com].Pick cloud project to use:[1] chirpy-test-300716[2] Create a new projectPlease enter numeric choice or text value (must exactly match listitem): 1[root@bc96abf71ef8 /]# gcloud info# Your selected project info should be displayed here Setup Google Analytics superProxy Clone the Google Analytics superProxy project on Github: https://github.com/googleanalytics/google-analytics-super-proxy to your local. Remove the first 2 lines in the src/app.yaml file: - application: your-project-id- version: 1 In src/config.py, add the OAUTH_CLIENT_ID and OAUTH_CLIENT_SECRET that you gathered from your App Engine Dashboard. Enter any random key for XSRF_KEY, your config.py should look similar to this #!/usr/bin/python2.7__author__ = 'pete.frisella@gmail.com (Pete Frisella)'# OAuth 2.0 Client SettingsAUTH_CONFIG = { 'OAUTH_CLIENT_ID': 'YOUR_CLIENT_ID', 'OAUTH_CLIENT_SECRET': 'YOUR_CLIENT_SECRET', 'OAUTH_REDIRECT_URI': '%s%s' % ( 'https://chirpy-test-XXXXXX.ue.r.appspot.com', '/admin/auth' )}# XSRF SettingsXSRF_KEY = 'OnceUponATimeThereLivedALegend' You can configure a custom domain instead of https://PROJECT_ID.REGION_ID.r.appspot.com.But, for the sake of keeping it simple, we will be using the Google provided default URL. From inside the src/ directory, deploy the app [root@bc96abf71ef8 src]# gcloud app deployServices to deploy:descriptor: [/tmp/google-analytics-super-proxy/src/app.yaml]source: [/tmp/google-analytics-super-proxy/src]target project: [chirpy-test-XXXX]target service: [default]target version: [VESRION_NUM]target url: [https://chirpy-test-XXXX.ue.r.appspot.com]Do you want to continue (Y/n)? YBeginning deployment of service [default]...╔════════════════════════════════════════════════════════════╗╠═ Uploading 1 file to Google Cloud Storage ═╣╚════════════════════════════════════════════════════════════╝File upload done.Updating service [default]...done.Setting traffic split for service [default]...done.Deployed service [default] to [https://chirpy-test-XXXX.ue.r.appspot.com]You can stream logs from the command line by running:$ gcloud app logs tail -s defaultTo view your application in the web browser run:$ gcloud app browse Visit the deployed service. Add a /admin to the end of the URL. Click on Authorize Users and make sure to add yourself as a managed user. If you get any errors, please Google it. The errors are self-explanatory and should be easy to fix. If everything went good, you’ll get this screen:Create Google Analytics QueryHead to https://PROJECT_ID.REGION_ID.r.appspot.com/admin and create a query after verifying the account. GA Core Reporting API query request can be created in Query Explorer.The query parameters are as follows: start-date: fill in the first day of blog posting end-date: fill in today (this is a parameter supported by GA Report, which means that it will always end according to the current query date) metrics: select ga:pageviews dimensions: select ga:pagePathIn order to reduce the returned results and reduce the network bandwidth, we add custom filtering rules 1: filters: fill in ga:pagePath=~^/posts/.*/$;ga:pagePath!@=. Among them, ; means using logical AND to concatenate two rules. If the site.baseurl is specified, change the first filtering rule to ga:pagePath=~^/BASE_URL/posts/.*/$, where BASE_URL is the value of site.baseurl. After Run Query, copy the generated contents of API Query URI at the bottom of the page and fill in the Encoded URI for the query of SuperProxy on GAE.After the query is saved on GAE, a Public Endpoint (public access address) will be generated, and we will get the query result in JSON format when accessing it. Finally, click Enable Endpoint in Public Request Endpoint to make the query effective, and click Start Scheduling in Scheduling to start the scheduled task.Configure Chirpy to Display Page ViewOnce all the hard part is done, it is very easy to enable the Page View on Chirpy theme. Your superProxy dashboard should look something like below and you can grab the required values.Update the _config.yml file of Chirpy project with the values from your dashboard, to look similar to the following:google_analytics: id: 'G-V6XXXXXXX' # fill in your Google Analytics ID pv: proxy_endpoint: 'https://PROJECT_ID.REGION_ID.r.appspot.com/query?id=&lt;ID FROM SUPER PROXY&gt;' cache_path: # the local PV cache data, friendly to visitors from GFW regionNow, you should see the Page View enabled on your blog.Reference Google Analytics Core Reporting API: Filters &#8617; " }, { "title": "Customize the Favicon", "url": "/posts/customize-the-favicon/", "categories": "Blogging, Tutorial", "tags": "favicon", "date": "2019-08-10 23:34:00 +0700", "snippet": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons...", "content": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons.Generate the faviconPrepare a square image (PNG, JPG, or SVG) with a size of 512x512 or more, and then go to the online tool Real Favicon Generator and click the button Select your Favicon image to upload your image file.In the next step, the webpage will show all usage scenarios. You can keep the default options, scroll to the bottom of the page, and click the button Generate your Favicons and HTML code to generate the favicon.Download &amp; ReplaceDownload the generated package, unzip and delete the following two from the extracted files: browserconfig.xml site.webmanifestAnd then copy the remaining image files (.PNG and .ICO) to cover the original files in the directory assets/img/favicons/ of your Jekyll site. If your Jekyll site doesn’t have this directory yet, just create one.The following table will help you understand the changes to the favicon files: File(s) From Online Tool From Chirpy *.PNG ✓ ✗ *.ICO ✓ ✗ ✓ means keep, ✗ means delete.The next time you build the site, the favicon will be replaced with a customized edition." }, { "title": "Getting Started", "url": "/posts/getting-started/", "categories": "Blogging, Tutorial", "tags": "getting started", "date": "2019-08-09 19:55:00 +0700", "snippet": "PrerequisitesFollow the instructions in the Jekyll Docs to complete the installation of the basic environment. Git also needs to be installed.InstallationCreating a New SiteThere are two ways to cr...", "content": "PrerequisitesFollow the instructions in the Jekyll Docs to complete the installation of the basic environment. Git also needs to be installed.InstallationCreating a New SiteThere are two ways to create a new repository for this theme: Using the Chirpy Starter - Easy to upgrade, isolates irrelevant project files so you can focus on writing. GitHub Fork - Convenient for custom development, but difficult to upgrade. Unless you are familiar with Jekyll and are determined to tweak or contribute to this project, this approach is not recommended.Option 1. Using the Chirpy StarterSign in to GitHub and browse to Chirpy Starter, click the button Use this template &gt; Create a new repository, and name the new repository USERNAME.github.io, where USERNAME represents your GitHub username.Option 2. GitHub ForkSign in to GitHub to fork Chirpy, and then rename it to USERNAME.github.io (USERNAME means your username).Next, clone your site to local machine. In order to build JavaScript files later, we need to install Node.js, and then run the tool:$ bash tools/init If you don’t want to deploy your site on GitHub Pages, append option --no-gh at the end of the above command.The above command will: Check out the code to the latest tag (to ensure the stability of your site: as the code for the default branch is under development). Remove non-essential sample files and take care of GitHub-related files. Build JavaScript files and export to assets/js/dist/, then make them tracked by Git. Automatically create a new commit to save the changes above.Installing DependenciesBefore running local server for the first time, go to the root directory of your site and run:$ bundleUsageConfigurationUpdate the variables of _config.yml as needed. Some of them are typical options: url avatar timezone langSocial Contact OptionsSocial contact options are displayed at the bottom of the sidebar. You can turn on/off the specified contacts in file _data/contact.yml.Customizing StylesheetIf you need to customize the stylesheet, copy the theme’s assets/css/jekyll-theme-chirpy.scss to the same path on your Jekyll site, and then add the custom style at the end of it.Starting with version 6.2.0, if you want to overwrite the SASS variables defined in _sass/addon/variables.scss, copy the main sass file _sass/main.scss into the _sass directory in your site’s source, then create a new file _sass/variables-hook.scss and assign new value.Customing Static AssetsStatic assets configuration was introduced in version 5.1.0. The CDN of the static assets is defined by file _data/origin/cors.yml, and you can replace some of them according to the network conditions in the region where your website is published.Also, if you’d like to self-host the static assets, please refer to the chirpy-static-assets.Running Local ServerYou may want to preview the site contents before publishing, so just run it by:$ bundle exec jekyll sAfter a few seconds, the local service will be published at 127.0.0.1:4000.DeploymentBefore the deployment begins, check out the file _config.yml and make sure the url is configured correctly. Furthermore, if you prefer the project site and don’t use a custom domain, or you want to visit your website with a base URL on a web server other than GitHub Pages, remember to change the baseurl to your project name that starts with a slash, e.g, /project-name.Now you can choose ONE of the following methods to deploy your Jekyll site.Deploy by Using GitHub ActionsThere are a few things to get ready for. If you’re on the GitHub Free plan, keep your site repository public. If you have committed Gemfile.lock to the repository, and your local machine is not running Linux, go to the root of your site and update the platform list of the lock-file: $ bundle lock --add-platform x86_64-linux Next, configure the Pages service. Browse to your repository on GitHub. Select the tab Settings, then click Pages in the left navigation bar. Then, in the Source section (under Build and deployment), select GitHub Actions from the dropdown menu. Push any commits to GitHub to trigger the Actions workflow. In the Actions tab of your repository, you should see the workflow Build and Deploy running. Once the build is complete and successful, the site will be deployed automatically. At this point, you can go to the URL indicated by GitHub to access your site.Manually Build and DeployOn self-hosted servers, you cannot enjoy the convenience of GitHub Actions. Therefore, you should build the site on your local machine and then upload the site files to the server.Go to the root of the source project, and build your site as follows:$ JEKYLL_ENV=production bundle exec jekyll bUnless you specified the output path, the generated site files will be placed in folder _site of the project’s root directory. Now you should upload those files to the target server. jekyll-theme-chirpy/master " }, { "title": "Writing a New Post", "url": "/posts/write-a-new-post/", "categories": "Blogging, Tutorial", "tags": "writing", "date": "2019-08-08 13:10:00 +0700", "snippet": "This tutorial will guide you how to write a post in the Chirpy template, and it’s worth reading even if you’ve used Jekyll before, as many features require specific variables to be set.Naming and P...", "content": "This tutorial will guide you how to write a post in the Chirpy template, and it’s worth reading even if you’ve used Jekyll before, as many features require specific variables to be set.Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts of the root directory. Please note that the EXTENSION must be one of md and markdown. If you want to save time of creating files, please consider using the plugin Jekyll-Compose to accomplish this.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG] # TAG names should always be lowercase--- The posts’ layout has been set to post by default, so there is no need to add the variable layout in the Front Matter block.Timezone of DateIn order to accurately record the release date of a post, you should not only set up the timezone of _config.yml but also provide the post’s timezone in variable date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post are designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:---categories: [Animal, Insect]tags: [bee]---Author InformationThe author information of the post usually does not need to be filled in the Front Matter , they will be obtained from variables social.name and the first entry of social.links of the configuration file by default. But you can also override it as follows:Adding author information in _data/authors.yml (If your website doesn’t have this file, don’t hesitate to create one).&lt;author_id&gt;: name: &lt;full name&gt; twitter: &lt;twitter_of_author&gt; url: &lt;homepage_of_author&gt;And then use author to specify a single entry or authors to specify multiple entries:---author: &lt;author_id&gt; # for single entry# orauthors: [&lt;author1_id&gt;, &lt;author2_id&gt;] # for multiple entries---Having said that, the key author can also identify multiple entries. The benefit of reading the author information from the file _data/authors.yml is that the page will have the meta tag twitter:creator, which enriches the Twitter Cards and is good for SEO.Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for a specific post, add the following to the post’s Front Matter:---toc: false---CommentsThe global switch of comments is defined by variable comments.active in the file _config.yml. After selecting a comment system for this variable, comments will be turned on for all posts.If you want to close the comment for a specific post, add the following to the Front Matter of the post:---comments: false---MathematicsFor website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:---math: true---After enabling the mathematical feature, you can add math equations with the following syntax: Block math should be added with $$ math $$ with mandatory blank lines before and after $$ Inserting equation numbering should be added with $$\\begin{equation} math \\end{equation}$$ Referencing equation numbering should be done with \\label{eq:label_name} in the equation block and \\eqref{eq:label_name} inline with text (see example below) Inline math (in lines) should be added with $$ math $$ without any blank line before or after $$ Inline math (in lists) should be added with \\$$ math $$&lt;!-- Block math, keep all blank lines --&gt;$$LaTeX_math_expression$$&lt;!-- Equation numbering, keep all blank lines --&gt;$$\\begin{equation} LaTeX_math_expression \\label{eq:label_name}\\end{equation}$$Can be referenced as \\eqref{eq:label_name}.&lt;!-- Inline math in lines, NO blank lines --&gt;\"Lorem ipsum dolor sit amet, $$ LaTeX_math_expression $$ consectetur adipiscing elit.\"&lt;!-- Inline math in lists, escape the first `$` --&gt;1. \\$$ LaTeX_math_expression $$2. \\$$ LaTeX_math_expression $$3. \\$$ LaTeX_math_expression $$MermaidMermaid is a great diagram generation tool. To enable it on your post, add the following to the YAML block:---mermaid: true---Then you can use it like other markdown languages: surround the graph code with ```mermaid and ```.ImagesCaptionAdd italics to the next line of an image, then it will become the caption and appear at the bottom of the image:![img-description](/path/to/image)_Image Caption_SizeIn order to prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image.![Desktop View](/assets/img/sample/mockup.png){: width=\"700\" height=\"400\" } For an SVG, you have to at least specify its width, otherwise it won’t be rendered.Starting from Chirpy v5.0.0, height and width support abbreviations (height → h, width → w). The following example has the same effect as the above:![Desktop View](/assets/img/sample/mockup.png){: w=\"700\" h=\"400\" }PositionBy default, the image is centered, but you can specify the position by using one of the classes normal, left, and right. Once the position is specified, the image caption should not be added. Normal position Image will be left aligned in below sample: ![Desktop View](/assets/img/sample/mockup.png){: .normal } Float to the left ![Desktop View](/assets/img/sample/mockup.png){: .left } Float to the right ![Desktop View](/assets/img/sample/mockup.png){: .right } Dark/Light modeYou can make images follow theme preferences in dark/light mode. This requires you to prepare two images, one for dark mode and one for light mode, and then assign them a specific class (dark or light):![Light mode only](/path/to/light-mode.png){: .light }![Dark mode only](/path/to/dark-mode.png){: .dark }ShadowThe screenshots of the program window can be considered to show the shadow effect:![Desktop View](/assets/img/sample/mockup.png){: .shadow }CDN URLIf you host the images on the CDN, you can save the time of repeatedly writing the CDN URL by assigning the variable img_cdn of _config.yml file:img_cdn: https://cdn.comOnce img_cdn is assigned, the CDN URL will be added to the path of all images (images of site avatar and posts) starting with /.For instance, when using images:![The flower](/path/to/flower.png)The parsing result will automatically add the CDN prefix https://cdn.com before the image path:&lt;img src=\"https://cdn.com/path/to/flower.png\" alt=\"The flower\" /&gt;Image PathWhen a post contains many images, it will be a time-consuming task to repeatedly define the path of the images. To solve this, we can define this path in the YAML block of the post:---img_path: /img/path/---And then, the image source of Markdown can write the file name directly:![The flower](flower.png)The output will be:&lt;img src=\"/img/path/flower.png\" alt=\"The flower\" /&gt;Preview ImageIf you want to add an image at the top of the post, please provide an image with a resolution of 1200 x 630. Please note that if the image aspect ratio does not meet 1.91 : 1, the image will be scaled and cropped.Knowing these prerequisites, you can start setting the image’s attribute:---image: path: /path/to/image alt: image alternative text---Note that the img_path can also be passed to the preview image, that is, when it has been set, the attribute path only needs the image file name.For simple use, you can also just use image to define the path.---image: /path/to/image---LQIPFor preview images:---image: lqip: /path/to/lqip-file # or base64 URI--- You can observe LQIP in the preview image of post Text and Typography.For normal images:![Image description](/path/to/image){: lqip=\"/path/to/lqip-file\" }Pinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:---pin: true---PromptsThere are several types of prompts: tip, info, warning, and danger. They can be generated by adding the class prompt-{type} to the blockquote. For example, define a prompt of type info as follows:&gt; Example line for prompt.{: .prompt-info }SyntaxInline Code`inline code part`Filepath Hightlight`/path/to/a/file.extend`{: .filepath}Code BlockMarkdown symbols ``` can easily create a code block as follows:```This is a plaintext code snippet.```Specifying LanguageUsing ```{language} you will get a code block with syntax highlight:```yamlkey: value``` The Jekyll tag {% highlight %} is not compatible with this theme.Line NumberBy default, all languages except plaintext, console, and terminal will display line numbers. When you want to hide the line number of a code block, add the class nolineno to it:```shellecho 'No more line numbers!'```{: .nolineno }Specifying the FilenameYou may have noticed that the code language will be displayed at the top of the code block. If you want to replace it with the file name, you can add the attribute file to achieve this:```shell# content```{: file=\"path/to/file\" }Liquid CodesIf you want to display the Liquid snippet, surround the liquid code with {% raw %} and {% endraw %}:{% raw %}```liquid{% if product.title contains 'Pack' %} This product's title contains the word Pack.{% endif %}```{% endraw %}Or adding render_with_liquid: false (Requires Jekyll 4.0 or higher) to the post’s YAML block.VideosYou can embed a video with the following syntax:{% include embed/{Platform}.html id='{ID}' %}Where Platform is the lowercase of the platform name, and ID is the video ID.The following table shows how to get the two parameters we need in a given video URL, and you can also know the currently supported video platforms. Video URL Platform ID https://www.youtube.com/watch?v=H-B46URT4mg youtube H-B46URT4mg https://www.twitch.tv/videos/1634779211 twitch 1634779211 https://www.bilibili.com/video/BV1Q44y1B7Wf bilibili BV1Q44y1B7Wf Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Text and Typography", "url": "/posts/text-and-typography/", "categories": "Blogging, Demo", "tags": "typography", "date": "2019-08-08 10:33:00 +0700", "snippet": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.HeadingsH1 - headingH2 - headingH3 - headingH...", "content": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.HeadingsH1 - headingH2 - headingH3 - headingH4 - headingParagraphQuisque egestas convallis ipsum, ut sollicitudin risus tincidunt a. Maecenas interdum malesuada egestas. Duis consectetur porta risus, sit amet vulputate urna facilisis ac. Phasellus semper dui non purus ultrices sodales. Aliquam ante lorem, ornare a feugiat ac, finibus nec mauris. Vivamus ut tristique nisi. Sed vel leo vulputate, efficitur risus non, posuere mi. Nullam tincidunt bibendum rutrum. Proin commodo ornare sapien. Vivamus interdum diam sed sapien blandit, sit amet aliquam risus mattis. Nullam arcu turpis, mollis quis laoreet at, placerat id nibh. Suspendisse venenatis eros eros.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph ToDo list Job Step 1 Step 2 Step 3 Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line shows the block quote.Prompts An example showing the tip type prompt. An example showing the info type prompt. An example showing the warning type prompt. An example showing the danger type prompt.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Links127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2.Inline codeThis is an example of Inline Code.FilepathHere is the /path/to/the/file.extend.Code blocksCommonThis is a common code snippet, without syntax highlight and line number.Specific Languageif [ $? -ne 0 ]; then echo \"The command was not successful.\"; #do the needful / exitfi;Specific filename@import \"colors/light-typography\", \"colors/dark-typography\";MathematicsThe mathematics powered by MathJax:\\[\\begin{equation} \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6} \\label{eq:series}\\end{equation}\\]We can reference the equation as \\eqref{eq:series}.When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dImagesDefault (with caption)Full screen width and center alignmentLeft alignedFloat to leftPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Float to rightPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.Dark/Light mode &amp; ShadowThe image below will toggle dark/light mode based on theme preference, notice it has shadows.VideoReverse Footnote The footnote source &#8617; The 2nd footnote source &#8617; " } ]
